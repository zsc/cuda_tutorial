<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第9章：张量核心与混合精度计算</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">CUDA 高性能编程实战教程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：CUDA硬件架构深度剖析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：CUDA编程模型与执行模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：全局内存优化策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：共享内存与Bank Conflict</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：寄存器优化与常量内存</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：Warp级编程与协作组</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：原子操作与同步原语</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：PTX内联与底层优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：张量核心与混合精度计算</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：CUTLASS深度解析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：激光雷达点云处理加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：多传感器融合的并行化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：实时语义分割与实例分割</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：路径规划与轨迹优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：视觉SLAM的GPU加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：机械臂运动规划</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第17章：强化学习推理加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第18章：大规模点云重建与网格化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第19章：多GPU编程与扩展</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第20章：CUDA Graph与内核融合</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第21章：嵌入式GPU开发（Jetson）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第22章：稀疏计算与动态稀疏</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第23章：量化与低精度计算</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第24章：新一代GPU特性展望</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第25章：性能分析与调优方法论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第26章：CUDA调试技术与错误处理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter27.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第27章：开发环境与工具链配置</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="9">第9章：张量核心与混合精度计算</h1>
<h2 id="_1">本章概览</h2>
<p>张量核心（Tensor Core）是NVIDIA从Volta架构开始引入的专用硬件单元，为深度学习工作负载提供了革命性的性能提升。通过支持混合精度计算，张量核心能够在保持模型精度的同时，实现4-8倍的吞吐量提升和显著的能效改进。在自动驾驶的感知模型和具身智能的决策网络中，张量核心已成为实现实时推理的关键技术。</p>
<p>本章将深入探讨张量核心的硬件架构、编程接口和优化策略。你将学习如何使用WMMA API编写高效的张量核心代码，掌握混合精度训练的核心技术，并通过Transformer模型的实际案例，体验张量核心带来的性能飞跃。更重要的是，我们将详细讨论数值稳定性问题，确保你能在追求极致性能的同时保持计算的正确性。</p>
<h3 id="_2">学习目标</h3>
<p>完成本章学习后，你将能够：</p>
<ul>
<li>理解张量核心的硬件架构和计算原理</li>
<li>熟练使用WMMA API进行张量核心编程</li>
<li>设计和实现混合精度训练策略</li>
<li>处理数值精度相关的各种挑战</li>
<li>将张量核心应用于实际的深度学习模型加速</li>
</ul>
<h2 id="91-tensor-core">9.1 Tensor Core架构与编程模型</h2>
<h3 id="911">9.1.1 张量核心硬件架构</h3>
<p>张量核心是专门为执行矩阵乘累加（Matrix Multiply-Accumulate, MMA）操作而设计的硬件单元。每个SM包含多个张量核心，它们能够在单个时钟周期内完成大规模的矩阵运算。</p>
<div class="codehilite"><pre><span></span><code>张量核心计算模型：
D = A × B + C

其中：

- A: M×K 矩阵
- B: K×N 矩阵  
- C: M×N 矩阵（累加器）
- D: M×N 矩阵（结果）
</code></pre></div>

<p>硬件架构特点：</p>
<ol>
<li>
<p><strong>并行计算单元</strong>：每个张量核心包含大量的乘累加单元（例如V100的张量核心包含64个FMA单元）</p>
</li>
<li>
<p><strong>分层架构</strong>：</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>SM Level:
┌─────────────────────────────────┐
│         Streaming              │
│       Multiprocessor           │
│  ┌──────────┬──────────┐       │
│  │  Tensor  │  Tensor  │       │
│  │  Core 0  │  Core 1  │ ...   │
│  └──────────┴──────────┘       │
└─────────────────────────────────┘

Tensor Core Level:
┌─────────────────────────────────┐
│        Tensor Core              │
│  ┌────────────────────────┐     │
│  │   64 FMA Units         │     │
│  │  (Fused Multiply-Add)  │     │
│  └────────────────────────┘     │
│  ┌────────────────────────┐     │
│  │  Register File         │     │
│  │  (Fragment Storage)    │     │
│  └────────────────────────┘     │
└─────────────────────────────────┘
</code></pre></div>

<ol start="3">
<li><strong>支持的数据类型演进</strong>：
- Volta (V100): FP16输入，FP16/FP32累加
- Turing (T4): 增加INT8/INT4支持
- Ampere (A100): 增加BF16、TF32支持
- Hopper (H100): 增加FP8支持，引入Transformer Engine</li>
</ol>
<h3 id="912-matrix-fragments">9.1.2 矩阵片段（Matrix Fragments）</h3>
<p>张量核心编程的核心概念是矩阵片段（Matrix Fragment），它是存储在寄存器中的矩阵子块：</p>
<div class="codehilite"><pre><span></span><code>Warp级协作模型：
┌─────────────────────────────────────┐
│            Warp (32 threads)        │
├─────────────────────────────────────┤
│  Thread 0: Fragment A[0], B[0]...   │
│  Thread 1: Fragment A[1], B[1]...   │
│  ...                                 │
│  Thread 31: Fragment A[31], B[31]...│
└─────────────────────────────────────┘

每个线程持有完整矩阵的一部分数据
所有线程协作完成矩阵运算
</code></pre></div>

<p>片段的特性：</p>
<ul>
<li><strong>分布式存储</strong>：矩阵数据分散在warp的32个线程中</li>
<li><strong>不透明格式</strong>：片段的内部布局对程序员不可见</li>
<li><strong>硬件优化</strong>：布局由硬件决定以最大化性能</li>
</ul>
<h3 id="913">9.1.3 编程模型层次</h3>
<p>张量核心提供多个编程抽象层次：</p>
<ol>
<li>
<p><strong>WMMA API（Warp Matrix Multiply-Accumulate）</strong>：
   - CUDA C++原生接口
   - 直接映射到PTX指令
   - 最大的控制灵活性</p>
</li>
<li>
<p><strong>CUTLASS</strong>：
   - 模板化的高性能库
   - 提供优化的GEMM实现
   - 支持自定义epilogue</p>
</li>
<li>
<p><strong>cuBLAS/cuDNN</strong>：
   - 高层库函数
   - 自动选择最优实现
   - 易于使用但灵活性较低</p>
</li>
<li>
<p><strong>框架集成（TensorFlow/PyTorch）</strong>：
   - 自动混合精度（AMP）
   - 透明的张量核心加速
   - 最少的代码修改</p>
</li>
</ol>
<h2 id="92-wmma-api">9.2 WMMA API详解</h2>
<h3 id="921-wmma">9.2.1 WMMA基础操作</h3>
<p>WMMA API提供了三个核心操作来完成张量核心计算：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 1. 加载操作 - 从内存加载矩阵片段</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">fragment</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span>

<span class="c1">// 2. 计算操作 - 执行矩阵乘累加</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">d_frag</span><span class="p">,</span><span class="w"> </span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="n">b_frag</span><span class="p">,</span><span class="w"> </span><span class="n">c_frag</span><span class="p">);</span>

<span class="c1">// 3. 存储操作 - 将结果写回内存</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">fragment</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span>
</code></pre></div>

<p>关键概念：</p>
<ul>
<li><strong>sync后缀</strong>：表示warp级同步操作，所有线程必须参与</li>
<li><strong>stride参数</strong>：矩阵的leading dimension，用于正确索引</li>
<li><strong>fragment</strong>：分布在warp所有线程中的矩阵数据</li>
</ul>
<h3 id="922-fragment">9.2.2 Fragment声明与管理</h3>
<p>Fragment是WMMA编程的核心数据结构：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// Fragment模板参数说明</span>
<span class="k">template</span><span class="o">&lt;</span>
<span class="w">    </span><span class="k">typename</span><span class="w"> </span><span class="nc">Use</span><span class="p">,</span><span class="w">        </span><span class="c1">// matrix_a, matrix_b, accumulator</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="c1">// 矩阵维度</span>
<span class="w">    </span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w">          </span><span class="c1">// 数据类型</span>
<span class="w">    </span><span class="k">typename</span><span class="w"> </span><span class="nc">Layout</span><span class="w">      </span><span class="c1">// row_major, col_major (可选)</span>
<span class="o">&gt;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">fragment</span><span class="p">;</span>

<span class="c1">// 实际使用示例</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">wmma</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// A矩阵片段 (M x K)</span>
<span class="w">    </span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">matrix_a</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">row_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">a_frag</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// B矩阵片段 (K x N)</span>
<span class="w">    </span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">matrix_b</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">col_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">b_frag</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 累加器片段 (M x N)</span>
<span class="w">    </span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">c_frag</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>支持的矩阵尺寸组合：</p>
<div class="codehilite"><pre><span></span><code>架构<span class="w">        </span>支持的<span class="ss">(</span><span class="nv">M</span>,<span class="w"> </span><span class="nv">N</span>,<span class="w"> </span><span class="nv">K</span><span class="ss">)</span>组合
<span class="o">--------------------------------------</span>
<span class="nv">Volta</span>:<span class="w">      </span><span class="ss">(</span><span class="mi">16</span>,<span class="w"> </span><span class="mi">16</span>,<span class="w"> </span><span class="mi">16</span><span class="ss">)</span>,<span class="w"> </span><span class="ss">(</span><span class="mi">32</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">16</span><span class="ss">)</span>,<span class="w"> </span><span class="ss">(</span><span class="mi">8</span>,<span class="w"> </span><span class="mi">32</span>,<span class="w"> </span><span class="mi">16</span><span class="ss">)</span>
<span class="nv">Turing</span>:<span class="w">     </span>上述<span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="ss">(</span><span class="mi">8</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">16</span><span class="ss">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">INT8</span>
<span class="nv">Ampere</span>:<span class="w">     </span>上述<span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="ss">(</span><span class="mi">16</span>,<span class="w"> </span><span class="mi">8</span>,<span class="w"> </span><span class="mi">8</span><span class="ss">)</span>,<span class="w"> </span><span class="ss">(</span><span class="mi">8</span>,<span class="w"> </span><span class="mi">16</span>,<span class="w"> </span><span class="mi">8</span><span class="ss">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">TF32</span>
<span class="nv">Hopper</span>:<span class="w">     </span>上述<span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="ss">(</span><span class="mi">16</span>,<span class="w"> </span><span class="mi">16</span>,<span class="w"> </span><span class="mi">8</span><span class="ss">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">FP8</span>
</code></pre></div>

<h3 id="923-gemm">9.2.3 完整GEMM实现</h3>
<p>下面是一个优化的GEMM内核实现，展示了WMMA的实际应用：</p>
<div class="codehilite"><pre><span></span><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_K</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">wmma_gemm_optimized</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">A</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">B</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">__restrict__</span><span class="w"> </span><span class="n">C</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">beta</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 常量定义</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">WMMA_M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">WMMA_N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">WMMA_K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 共享内存声明</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">smem_a</span><span class="p">[</span><span class="n">TILE_M</span><span class="p">][</span><span class="n">TILE_K</span><span class="p">];</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">smem_b</span><span class="p">[</span><span class="n">TILE_K</span><span class="p">][</span><span class="n">TILE_N</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// 计算warp和线程的位置</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">warpId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">warpSize</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">laneId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">warpSize</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">warpsPerBlock</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">warpSize</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 计算输出块的位置</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">blockRow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">TILE_M</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">blockCol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Fragment声明</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">a_frag</span><span class="p">;</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">b_frag</span><span class="p">;</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">acc_frag</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 初始化累加器</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">acc_frag</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 主循环：沿K维度分块</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 协作加载数据到共享内存</span>
<span class="w">        </span><span class="c1">// 每个线程负责加载一部分数据</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">tidx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">tidy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 加载A矩阵块</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">blockRow</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidy</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">smem_a</span><span class="p">[</span><span class="n">tidy</span><span class="p">][</span><span class="n">tidx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[(</span><span class="n">blockRow</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidy</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidx</span><span class="p">)];</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">smem_a</span><span class="p">[</span><span class="n">tidy</span><span class="p">][</span><span class="n">tidx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 加载B矩阵块</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidy</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">blockCol</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">smem_b</span><span class="p">[</span><span class="n">tidy</span><span class="p">][</span><span class="n">tidx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[(</span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidy</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">blockCol</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tidx</span><span class="p">)];</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">smem_b</span><span class="p">[</span><span class="n">tidy</span><span class="p">][</span><span class="n">tidx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 使用张量核心计算</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 计算warp负责的子块位置</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">warpRow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">warpId</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">TILE_N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">;</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">warpCol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">warpId</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="n">TILE_N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 加载fragments</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">smem_a</span><span class="p">[</span><span class="n">warpRow</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">);</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">b_frag</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">smem_b</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">warpCol</span><span class="p">],</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 执行矩阵乘累加</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">acc_frag</span><span class="p">,</span><span class="w"> </span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="n">b_frag</span><span class="p">,</span><span class="w"> </span><span class="n">acc_frag</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 写回结果</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">warpRow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">warpId</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">TILE_N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">warpCol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">warpId</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="n">TILE_N</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">blockRow</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">warpRow</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">M</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">blockCol</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">warpCol</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 应用alpha和beta缩放</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">c_frag</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 加载原始C矩阵</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">c_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">blockRow</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">warpRow</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">blockCol</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">warpCol</span><span class="p">);</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span><span class="w"> </span><span class="n">c_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 缩放并累加</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">c_frag</span><span class="p">.</span><span class="n">num_elements</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">c_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">acc_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 存储最终结果</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">c_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">c_frag</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="924">9.2.4 性能优化技巧</h3>
<ol>
<li><strong>数据重用最大化</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>每个A fragment被N/WMMA_N个B fragments重用
每个B fragment被M/WMMA_M个A fragments重用
优化目标：最大化fragment在寄存器中的驻留时间
</code></pre></div>

<ol start="2">
<li><strong>共享内存bank conflict避免</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用padding避免bank conflict</span>
<span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">smem_a</span><span class="p">[</span><span class="n">TILE_M</span><span class="p">][</span><span class="n">TILE_K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">];</span><span class="w">  </span><span class="c1">// +1 padding</span>
<span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">smem_b</span><span class="p">[</span><span class="n">TILE_K</span><span class="p">][</span><span class="n">TILE_N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">];</span>
</code></pre></div>

<ol start="3">
<li><strong>异步数据传输</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用异步拷贝指令（Ampere+）</span>
<span class="n">__pipeline_memcpy_async</span><span class="p">(</span><span class="o">&amp;</span><span class="n">smem_a</span><span class="p">[</span><span class="n">row</span><span class="p">][</span><span class="n">col</span><span class="p">],</span><span class="w"> </span>
<span class="w">                        </span><span class="o">&amp;</span><span class="n">global_a</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">],</span><span class="w"> </span>
<span class="w">                        </span><span class="k">sizeof</span><span class="p">(</span><span class="n">half</span><span class="p">));</span>
<span class="n">__pipeline_commit</span><span class="p">();</span>
<span class="n">__pipeline_wait_prior</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</code></pre></div>

<h2 id="93">9.3 混合精度训练策略</h2>
<h3 id="931">9.3.1 混合精度训练原理</h3>
<p>混合精度训练通过在不同计算阶段使用不同数值精度来平衡性能和精度：</p>
<div class="codehilite"><pre><span></span><code>混合精度训练流程：
┌──────────────────┐
│   FP32 主权重     │ ←─── 保持高精度副本
└────────┬─────────┘
         │ 转换
    ┌────▼─────┐
    │ FP16权重  │
    └────┬─────┘
         │
┌────────▼─────────┐
│   前向传播        │ ←─── FP16计算
│  (Tensor Core)   │
└────────┬─────────┘
         │
    ┌────▼─────┐
    │ FP16损失  │
    └────┬─────┘
         │ 损失缩放
    ┌────▼─────┐
    │缩放后损失 │
    └────┬─────┘
         │
┌────────▼─────────┐
│   反向传播        │ ←─── FP16计算
│  (Tensor Core)   │
└────────┬─────────┘
         │
    ┌────▼─────┐
    │FP16梯度  │
    └────┬─────┘
         │ 反缩放
    ┌────▼─────┐
    │FP32梯度  │
    └────┬─────┘
         │
┌────────▼─────────┐
│  权重更新(FP32)   │ ←─── 高精度更新
└──────────────────┘
</code></pre></div>

<p>关键优势：</p>
<ol>
<li><strong>性能提升</strong>：张量核心加速，内存带宽减半</li>
<li><strong>内存节省</strong>：激活值和梯度使用FP16存储</li>
<li><strong>精度保持</strong>：FP32主权重确保收敛质量</li>
</ol>
<h3 id="932">9.3.2 损失缩放机制</h3>
<p>损失缩放是混合精度训练的核心技术，用于防止梯度下溢：</p>
<div class="codehilite"><pre><span></span><code>FP16动态范围问题：
┌────────────────────────────────┐
│     FP32: ±3.4×10^38           │
│     FP16: ±65,504              │
│                                 │
│  梯度分布:                      │
│  ┌──────────────────────┐      │
│  │ 大部分梯度 &lt; 2^-24    │      │
│  │ FP16下溢为0!          │      │
│  └──────────────────────┘      │
└────────────────────────────────┘
</code></pre></div>

<p><strong>静态损失缩放</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">loss_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1024.0f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 固定缩放因子</span>

<span class="c1">// 前向传播</span>
<span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_forward</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">);</span>

<span class="c1">// 应用损失缩放</span>
<span class="n">scaled_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">loss_scale</span><span class="p">;</span>

<span class="c1">// 反向传播</span>
<span class="n">scaled_gradients</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="n">scaled_loss</span><span class="p">);</span>

<span class="c1">// 梯度反缩放</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">scaled_gradients</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">loss_scale</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// 权重更新</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">gradients</span><span class="p">);</span>
</code></pre></div>

<p><strong>动态损失缩放算法</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DynamicLossScaler</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">65536.0f</span><span class="p">;</span><span class="w">     </span><span class="c1">// 初始缩放因子</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">growth_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w">       </span><span class="c1">// 增长因子</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">backoff_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w">      </span><span class="c1">// 回退因子</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">growth_interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2000</span><span class="p">;</span><span class="w">  </span><span class="c1">// 增长间隔</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">_growth_tracker</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">     </span><span class="c1">// 增长计数器</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">scale_and_check</span><span class="p">(</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">loss</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">gradients</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 应用损失缩放</span>
<span class="w">        </span><span class="n">loss</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">scale</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 反向传播</span>
<span class="w">        </span><span class="n">gradients</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 检查是否有inf/nan</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">has_inf_or_nan</span><span class="p">(</span><span class="n">gradients</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 减小缩放因子</span>
<span class="w">            </span><span class="n">scale</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">backoff_factor</span><span class="p">;</span>
<span class="w">            </span><span class="n">_growth_tracker</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w">  </span><span class="c1">// 跳过本次更新</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 梯度反缩放</span>
<span class="w">        </span><span class="n">gradients</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">scale</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 更新增长计数器</span>
<span class="w">        </span><span class="n">_growth_tracker</span><span class="o">++</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 定期增大缩放因子</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">_growth_tracker</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">growth_interval</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">scale</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">growth_factor</span><span class="p">;</span>
<span class="w">            </span><span class="n">_growth_tracker</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// 可以更新权重</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="933">9.3.3 自动混合精度实现</h3>
<p>现代深度学习框架提供了自动混合精度（AMP）支持：</p>
<p><strong>PyTorch AMP示例（伪代码）</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 初始化</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>

<span class="c1"># 训练循环</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 自动混合精度前向传播</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># 损失缩放和反向传播</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># 梯度裁剪（可选）</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

        <span class="c1"># 优化器步骤</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>

<p><strong>关键组件</strong>：</p>
<ol>
<li><strong>autocast上下文</strong>：自动选择合适的精度</li>
<li><strong>GradScaler</strong>：处理损失缩放和梯度反缩放</li>
<li><strong>黑白名单机制</strong>：特定层强制使用FP32</li>
</ol>
<h3 id="934">9.3.4 精度敏感层处理</h3>
<p>某些操作对数值精度特别敏感，需要特殊处理：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 自定义混合精度策略</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MixedPrecisionPolicy</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// FP32白名单（始终使用FP32）</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">set</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">fp32_ops</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;BatchNorm&quot;</span><span class="p">,</span><span class="w">      </span><span class="c1">// 批归一化</span>
<span class="w">        </span><span class="s">&quot;LayerNorm&quot;</span><span class="p">,</span><span class="w">      </span><span class="c1">// 层归一化</span>
<span class="w">        </span><span class="s">&quot;Softmax&quot;</span><span class="p">,</span><span class="w">        </span><span class="c1">// Softmax激活</span>
<span class="w">        </span><span class="s">&quot;CrossEntropy&quot;</span><span class="p">,</span><span class="w">   </span><span class="c1">// 交叉熵损失</span>
<span class="w">        </span><span class="s">&quot;L2Loss&quot;</span><span class="w">          </span><span class="c1">// L2损失</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="c1">// FP16黑名单（避免使用FP16）</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">set</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">fp16_blacklist</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s">&quot;Exp&quot;</span><span class="p">,</span><span class="w">            </span><span class="c1">// 指数运算</span>
<span class="w">        </span><span class="s">&quot;Log&quot;</span><span class="p">,</span><span class="w">            </span><span class="c1">// 对数运算</span>
<span class="w">        </span><span class="s">&quot;Pow&quot;</span><span class="p">,</span><span class="w">            </span><span class="c1">// 幂运算</span>
<span class="w">        </span><span class="s">&quot;Sum&quot;</span><span class="p">,</span><span class="w">            </span><span class="c1">// 大规模求和</span>
<span class="w">        </span><span class="s">&quot;Prod&quot;</span><span class="w">            </span><span class="c1">// 大规模乘积</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="c1">// 自适应精度选择</span>
<span class="w">    </span><span class="n">DataType</span><span class="w"> </span><span class="nf">select_precision</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op_type</span><span class="p">,</span><span class="w"> </span>
<span class="w">                              </span><span class="k">const</span><span class="w"> </span><span class="n">TensorShape</span><span class="o">&amp;</span><span class="w"> </span><span class="n">shape</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 强制FP32</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">fp32_ops</span><span class="p">.</span><span class="n">count</span><span class="p">(</span><span class="n">op_type</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">FP32</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 避免FP16</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">fp16_blacklist</span><span class="p">.</span><span class="n">count</span><span class="p">(</span><span class="n">op_type</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">FP32</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 小张量使用FP32（避免开销）</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">num_elements</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1024</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">FP32</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 默认使用FP16</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">FP16</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="935">9.3.5 梯度累积与混合精度</h3>
<p>在小批量训练时，梯度累积需要特别注意：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 混合精度梯度累积</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MixedPrecisionGradientAccumulator</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">fp32_grad_buffers</span><span class="p">;</span><span class="w">  </span><span class="c1">// FP32梯度缓冲</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">accumulation_steps</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">current_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">accumulate</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">fp16_grads</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="kt">float</span><span class="w"> </span><span class="n">loss_scale</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">fp16_grads</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 转换为FP32并反缩放</span>
<span class="w">            </span><span class="n">Tensor</span><span class="w"> </span><span class="n">fp32_grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fp16_grads</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">DataType</span><span class="o">::</span><span class="n">FP32</span><span class="p">);</span>
<span class="w">            </span><span class="n">fp32_grad</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">loss_scale</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 累积梯度</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">current_step</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">fp32_grad_buffers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fp32_grad</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">fp32_grad_buffers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">fp32_grad</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">current_step</span><span class="o">++</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 达到累积步数，执行更新</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">current_step</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">accumulation_steps</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 平均梯度</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">fp32_grad_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">grad</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">accumulation_steps</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="c1">// 应用梯度更新</span>
<span class="w">            </span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">fp32_grad_buffers</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 重置</span>
<span class="w">            </span><span class="n">current_step</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">            </span><span class="n">clear_buffers</span><span class="p">();</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h2 id="94">9.4 数值稳定性保证</h2>
<h3 id="941">9.4.1 数值精度问题的根源</h3>
<p>混合精度计算中的数值问题主要源于以下几个方面：</p>
<div class="codehilite"><pre><span></span><code>精度对比：
┌─────────────────────────────────────────┐
│ 类型    符号  指数  尾数   范围           │
├─────────────────────────────────────────┤
│ FP32:   1    8     23    ±3.4×10^38     │
│ FP16:   1    5     10    ±65,504        │
│ BF16:   1    8     7     ±3.4×10^38     │
│ TF32:   1    8     10    ±3.4×10^38     │
│ FP8:    1    4/5   3/2   ±240/57,344    │
└─────────────────────────────────────────┘
</code></pre></div>

<p>关键挑战：</p>
<ol>
<li><strong>梯度消失</strong>：小于2^-24的梯度在FP16中变为0</li>
<li><strong>梯度爆炸</strong>：超出FP16范围导致inf</li>
<li><strong>舍入误差累积</strong>：迭代计算中误差不断放大</li>
<li><strong>更新丢失</strong>：权重更新量太小无法表示</li>
</ol>
<h3 id="942">9.4.2 溢出检测与处理</h3>
<p>实时监控数值溢出是保证训练稳定的关键：</p>
<div class="codehilite"><pre><span></span><code><span class="n">__device__</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">check_tensor_validity</span><span class="p">(</span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_nan</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_inf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 并行检查每个元素</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">half</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

<span class="w">        </span><span class="c1">// 使用CUDA内建函数检查</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">__hisnan</span><span class="p">(</span><span class="n">val</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">has_nan</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">__hisinf</span><span class="p">(</span><span class="n">val</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">has_inf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Warp级归约</span>
<span class="w">    </span><span class="n">has_nan</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__any_sync</span><span class="p">(</span><span class="mh">0xFFFFFFFF</span><span class="p">,</span><span class="w"> </span><span class="n">has_nan</span><span class="p">);</span>
<span class="w">    </span><span class="n">has_inf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__any_sync</span><span class="p">(</span><span class="mh">0xFFFFFFFF</span><span class="p">,</span><span class="w"> </span><span class="n">has_inf</span><span class="p">);</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">!</span><span class="p">(</span><span class="n">has_nan</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">has_inf</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// 全局溢出检测内核</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">global_overflow_check</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">**</span><span class="w"> </span><span class="n">tensors</span><span class="p">,</span><span class="w">      </span><span class="c1">// 张量指针数组</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">tensor_sizes</span><span class="p">,</span><span class="w">   </span><span class="c1">// 每个张量的大小</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">,</span><span class="w">     </span><span class="c1">// 张量数量</span>
<span class="w">    </span><span class="kt">bool</span><span class="o">*</span><span class="w"> </span><span class="n">overflow_flag</span><span class="w">  </span><span class="c1">// 输出标志</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">block_overflow</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">block_overflow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 每个block检查一个张量</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">bool</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">check_tensor_validity</span><span class="p">(</span>
<span class="w">            </span><span class="n">tensors</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">],</span><span class="w"> </span>
<span class="w">            </span><span class="n">tensor_sizes</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span>
<span class="w">        </span><span class="p">);</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">valid</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">atomicOr</span><span class="p">((</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">overflow_flag</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="943-kahan">9.4.3 Kahan求和与补偿算法</h3>
<p>对于长序列的累加操作，使用补偿求和算法提高精度：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// Kahan求和在张量核心中的应用</span>
<span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">kahan_wmma_accumulate</span><span class="p">(</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">sum_frag</span><span class="p">,</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">c_frag</span><span class="p">,</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">new_frag</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 对fragment的每个元素应用Kahan算法</span>
<span class="w">    </span><span class="cp">#pragma unroll</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">sum_frag</span><span class="p">.</span><span class="n">num_elements</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">T</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">c_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w">  </span><span class="c1">// 补偿</span>
<span class="w">        </span><span class="n">T</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">;</span><span class="w">            </span><span class="c1">// 新和</span>
<span class="w">        </span><span class="n">c_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">sum_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="p">;</span><span class="w">  </span><span class="c1">// 新补偿项</span>
<span class="w">        </span><span class="n">sum_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 分块Kahan求和</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">blocked_kahan_reduction</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">block_size</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared_data</span><span class="p">[];</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">s_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shared_data</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">s_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">shared_data</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">bid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 初始化</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 分块Kahan求和</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bid</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">block_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span>
<span class="w">         </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">min</span><span class="p">((</span><span class="n">bid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">block_size</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span><span class="w"> </span>
<span class="w">         </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">;</span>
<span class="w">        </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">sum</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="p">;</span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">s_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
<span class="w">    </span><span class="n">s_c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 树形归约（保持补偿）</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">&gt;&gt;=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s_sum</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">s</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s_c</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">;</span>
<span class="w">            </span><span class="n">s_c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">])</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s_c</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">s</span><span class="p">];</span>
<span class="w">            </span><span class="n">s_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">bid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">s_sum</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="944">9.4.4 数值稳定的激活函数</h3>
<p>针对混合精度优化的激活函数实现：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 稳定的Softmax（避免溢出）</span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">stable_softmax_mixed_precision</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 步骤1：找最大值（FP32精度）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">max_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">INFINITY</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="n">max_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span><span class="w"> </span><span class="n">val</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 步骤2：计算exp(x - max)并求和</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_val</span><span class="p">;</span>
<span class="w">        </span><span class="c1">// 防止极小值下溢</span>
<span class="w">        </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="w"> </span><span class="mf">-80.0f</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">exp_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">exp_val</span><span class="p">);</span>
<span class="w">        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">exp_val</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 步骤3：归一化</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">inv_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">sum</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1e-10f</span><span class="p">);</span><span class="w">  </span><span class="c1">// 避免除零</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inv_sum</span><span class="p">;</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">normalized</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 稳定的LogSoftmax</span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">stable_log_softmax_mixed</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 找最大值</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">max_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">INFINITY</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">max_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">max_val</span><span class="p">,</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]));</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 计算log-sum-exp</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum_exp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">sum_exp</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_val</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">log_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_val</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">logf</span><span class="p">(</span><span class="n">sum_exp</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 计算log_softmax</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">log_sum</span><span class="p">;</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 稳定的LayerNorm（Welford算法）</span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">stable_layer_norm_welford</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">gamma</span><span class="p">,</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">eps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1e-5f</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Welford在线算法计算均值和方差</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">M2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">;</span>
<span class="w">        </span><span class="n">mean</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">delta2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">;</span>
<span class="w">        </span><span class="n">M2</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">delta2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">inv_std</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rsqrtf</span><span class="p">(</span><span class="n">variance</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eps</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 应用归一化和仿射变换</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inv_std</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">scaled</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">gamma</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span>

<span class="w">                      </span><span class="o">+</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">scaled</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="945">9.4.5 梯度裁剪与正则化</h3>
<p>防止梯度爆炸的多种策略：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 全局梯度范数裁剪</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">gradient_clip_by_global_norm</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">**</span><span class="w"> </span><span class="n">gradients</span><span class="p">,</span><span class="w">      </span><span class="c1">// 梯度张量数组</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">grad_sizes</span><span class="p">,</span><span class="w">       </span><span class="c1">// 每个梯度的大小</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">num_grads</span><span class="p">,</span><span class="w">         </span><span class="c1">// 梯度数量</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">max_norm</span><span class="p">,</span><span class="w">        </span><span class="c1">// 最大范数</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">global_norm</span><span class="w">     </span><span class="c1">// 输出的全局范数</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">[];</span>

<span class="w">    </span><span class="c1">// 步骤1：计算局部平方和</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">local_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">gid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_grads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gradients</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_sizes</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">            </span><span class="n">local_sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">val</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 块内归约</span>
<span class="w">    </span><span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">local_sum</span><span class="p">;</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">&gt;&gt;=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">s</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 步骤2：原子加到全局范数</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">gid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_grads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">atomicAdd</span><span class="p">(</span><span class="n">global_norm</span><span class="p">,</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 等待所有块完成</span>
<span class="w">    </span><span class="n">__threadfence</span><span class="p">();</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 步骤3：应用裁剪</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">gid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="o">*</span><span class="n">global_norm</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">norm</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">max_norm</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="o">*</span><span class="n">global_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_norm</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">norm</span><span class="p">;</span><span class="w">  </span><span class="c1">// 缩放因子</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="o">*</span><span class="n">global_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">__threadfence</span><span class="p">();</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 步骤4：缩放梯度</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_grads</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gradients</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_sizes</span><span class="p">[</span><span class="n">gid</span><span class="p">];</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">*</span><span class="n">global_norm</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">scale</span><span class="p">;</span>
<span class="w">            </span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 自适应梯度裁剪（AGC）</span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">compute_adaptive_clip_factor</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">clip_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.01f</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">grad_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">weight_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 计算范数</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">gradient</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">weight</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="n">grad_norm</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">g</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">g</span><span class="p">;</span>
<span class="w">        </span><span class="n">weight_norm</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">w</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">grad_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">);</span>
<span class="w">    </span><span class="n">weight_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">weight_norm</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 计算裁剪因子</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">max_norm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clip_factor</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">weight_norm</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">grad_norm</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">max_norm</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">max_norm</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">grad_norm</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="95-transformer">9.5 案例：Transformer模型的张量核心加速</h2>
<h3 id="951-transformer">9.5.1 Transformer架构与计算特征</h3>
<p>Transformer模型是自动驾驶感知和具身智能决策的核心架构，其计算密集型特征使其成为张量核心加速的理想目标：</p>
<div class="codehilite"><pre><span></span><code>Transformer计算分布：
┌────────────────────────────────────┐
│ 操作类型         计算占比  内存占比  │
├────────────────────────────────────┤
│ 自注意力矩阵乘法    45%      25%    │
│ FFN层矩阵乘法       35%      20%    │
│ LayerNorm/Softmax   10%      15%    │
│ 残差连接与激活       5%      20%    │
│ 位置编码等其他       5%      20%    │
└────────────────────────────────────┘
</code></pre></div>

<p>关键优化机会：</p>
<ol>
<li><strong>矩阵乘法密集</strong>：80%的计算是GEMM操作</li>
<li><strong>批处理友好</strong>：多个序列可以并行处理</li>
<li><strong>精度容忍</strong>：大部分操作可用FP16/BF16</li>
<li><strong>内存带宽受限</strong>：张量核心可缓解带宽压力</li>
</ol>
<h3 id="952">9.5.2 多头注意力的张量核心实现</h3>
<p>多头注意力是Transformer的核心组件，其实现充分利用张量核心：</p>
<div class="codehilite"><pre><span></span><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">NUM_HEADS</span><span class="o">&gt;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TensorCoreMultiHeadAttention</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 投影权重（FP16存储）</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">W_q</span><span class="p">;</span><span class="w">  </span><span class="c1">// [hidden_dim, hidden_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">W_k</span><span class="p">;</span><span class="w">  </span><span class="c1">// [hidden_dim, hidden_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">W_v</span><span class="p">;</span><span class="w">  </span><span class="c1">// [hidden_dim, hidden_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">W_o</span><span class="p">;</span><span class="w">  </span><span class="c1">// [hidden_dim, hidden_dim]</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">compute_attention</span><span class="p">(</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w">      </span><span class="c1">// [batch, seq_len, hidden_dim]</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w">     </span><span class="c1">// [batch, seq_len, hidden_dim]</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">mask</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span>
<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 分配共享内存</span>
<span class="w">        </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">[];</span>

<span class="w">        </span><span class="c1">// 计算Q, K, V投影</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">;</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">V</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 步骤1：使用张量核心计算Q = input * W_q</span>
<span class="w">        </span><span class="n">tensor_core_gemm_batched</span><span class="p">(</span>
<span class="w">            </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">W_q</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span>
<span class="w">        </span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 步骤2：计算K和V</span>
<span class="w">        </span><span class="n">tensor_core_gemm_batched</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">W_k</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="cm">/*dims*/</span><span class="p">);</span>
<span class="w">        </span><span class="n">tensor_core_gemm_batched</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">W_v</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="cm">/*dims*/</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 步骤3：计算注意力分数</span>
<span class="w">        </span><span class="n">compute_attention_scores</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">compute_attention_scores</span><span class="p">(</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">mask</span>
<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">head_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">NUM_HEADS</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rsqrtf</span><span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">head_size</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 每个warp处理一个注意力头</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">warp_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">lane_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">warp_id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">NUM_HEADS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 计算QK^T使用张量核心</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">q_frag</span><span class="p">;</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">k_frag</span><span class="p">;</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">scores_frag</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 分块计算注意力矩阵</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="c1">// 初始化累加器</span>
<span class="w">                    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">scores_frag</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>

<span class="w">                    </span><span class="c1">// 计算QK^T的一个块</span>
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">head_size</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="c1">// 加载Q和K的片段</span>
<span class="w">                        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">q_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">warp_id</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">head_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">k</span><span class="p">;</span>
<span class="w">                        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">k_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">warp_id</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">head_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">k</span><span class="p">;</span>

<span class="w">                        </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">q_frag</span><span class="p">,</span><span class="w"> </span><span class="n">q_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">                        </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">k_frag</span><span class="p">,</span><span class="w"> </span><span class="n">k_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">                        </span><span class="c1">// 执行矩阵乘法</span>
<span class="w">                        </span><span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">scores_frag</span><span class="p">,</span><span class="w"> </span><span class="n">q_frag</span><span class="p">,</span><span class="w"> </span><span class="n">k_frag</span><span class="p">,</span><span class="w"> </span><span class="n">scores_frag</span><span class="p">);</span>
<span class="w">                    </span><span class="p">}</span>

<span class="w">                    </span><span class="c1">// 应用缩放和mask</span>
<span class="w">                    </span><span class="n">apply_scale_and_mask</span><span class="p">(</span><span class="n">scores_frag</span><span class="p">,</span><span class="w"> </span><span class="n">scale</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">);</span>

<span class="w">                    </span><span class="c1">// 存储分数</span>
<span class="w">                    </span><span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span>
<span class="w">                        </span><span class="n">attention_scores</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="p">,</span>
<span class="w">                        </span><span class="n">scores_frag</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span>
<span class="w">                    </span><span class="p">);</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="c1">// 应用softmax（使用FP32以保证稳定性）</span>
<span class="w">            </span><span class="n">apply_softmax_mixed_precision</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 计算attention * V</span>
<span class="w">            </span><span class="n">compute_attention_output</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="953-ffn">9.5.3 前馈网络（FFN）优化</h3>
<p>Transformer的FFN层包含两个大型矩阵乘法，是张量核心优化的重点：</p>
<div class="codehilite"><pre><span></span><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">HIDDEN_DIM</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">FFN_DIM</span><span class="o">&gt;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TensorCoreFFN</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="c1">// 权重矩阵</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">W1</span><span class="p">;</span><span class="w">  </span><span class="c1">// [hidden_dim, ffn_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">W2</span><span class="p">;</span><span class="w">  </span><span class="c1">// [ffn_dim, hidden_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">bias1</span><span class="p">;</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">bias2</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 激活函数类型</span>
<span class="w">    </span><span class="k">enum</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">Activation</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">RELU</span><span class="p">,</span><span class="w"> </span><span class="n">GELU</span><span class="p">,</span><span class="w"> </span><span class="n">SWISH</span><span class="w"> </span><span class="p">};</span>
<span class="w">    </span><span class="n">Activation</span><span class="w"> </span><span class="n">activation_type</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w">   </span><span class="c1">// [batch * seq_len, hidden_dim]</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w">  </span><span class="c1">// [batch * seq_len, hidden_dim]</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_seq_len</span>
<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 分配中间激活内存</span>
<span class="w">        </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">[];</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 第一个线性层：hidden_dim -&gt; ffn_dim</span>
<span class="w">        </span><span class="n">tensor_core_gemm_with_bias</span><span class="p">(</span>
<span class="w">            </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">W1</span><span class="p">,</span><span class="w"> </span><span class="n">bias1</span><span class="p">,</span><span class="w"> </span><span class="n">intermediate</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch_seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">HIDDEN_DIM</span><span class="p">,</span><span class="w"> </span><span class="n">FFN_DIM</span>
<span class="w">        </span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 应用激活函数（混合精度）</span>
<span class="w">        </span><span class="n">apply_activation_mixed</span><span class="p">(</span><span class="n">intermediate</span><span class="p">,</span><span class="w"> </span><span class="n">batch_seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FFN_DIM</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 第二个线性层：ffn_dim -&gt; hidden_dim</span>
<span class="w">        </span><span class="n">tensor_core_gemm_with_bias</span><span class="p">(</span>
<span class="w">            </span><span class="n">intermediate</span><span class="p">,</span><span class="w"> </span><span class="n">W2</span><span class="p">,</span><span class="w"> </span><span class="n">bias2</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch_seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">FFN_DIM</span><span class="p">,</span><span class="w"> </span><span class="n">HIDDEN_DIM</span>
<span class="w">        </span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">tensor_core_gemm_with_bias</span><span class="p">(</span>
<span class="w">        </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">,</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span>
<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 使用更大的tile size以提高张量核心利用率</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 每个block处理一个输出tile</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">block_row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">TILE_M</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">block_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 共享内存用于数据复用</span>
<span class="w">        </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">As</span><span class="p">[</span><span class="n">TILE_M</span><span class="p">][</span><span class="n">TILE_K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="p">];</span><span class="w">  </span><span class="c1">// padding避免bank conflict</span>
<span class="w">        </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">Bs</span><span class="p">[</span><span class="n">TILE_K</span><span class="p">][</span><span class="n">TILE_N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="p">];</span>

<span class="w">        </span><span class="c1">// WMMA fragments</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">a_frag</span><span class="p">;</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">b_frag</span><span class="p">;</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">c_frag</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 初始化累加器</span>
<span class="w">        </span><span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 主循环：K维度分块</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">K</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 协作加载A和B到共享内存</span>
<span class="w">            </span><span class="n">load_tile_to_shared</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">As</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">block_row</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">);</span>
<span class="w">            </span><span class="n">load_tile_to_shared</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">Bs</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">block_col</span><span class="p">);</span>
<span class="w">            </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">            </span><span class="c1">// 使用张量核心计算</span>
<span class="w">            </span><span class="cp">#pragma unroll</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">wk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">wk</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">;</span><span class="w"> </span><span class="n">wk</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">warp_row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">warp_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>

<span class="w">                </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">As</span><span class="p">[</span><span class="n">warp_row</span><span class="p">][</span><span class="n">wk</span><span class="p">],</span><span class="w"> </span><span class="n">TILE_K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>
<span class="w">                </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">b_frag</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">Bs</span><span class="p">[</span><span class="n">wk</span><span class="p">][</span><span class="n">warp_col</span><span class="p">],</span><span class="w"> </span><span class="n">TILE_N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>
<span class="w">                </span><span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span><span class="w"> </span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="n">b_frag</span><span class="p">,</span><span class="w"> </span><span class="n">c_frag</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">            </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 加上bias并存储结果</span>
<span class="w">        </span><span class="n">add_bias_and_store</span><span class="p">(</span><span class="n">c_frag</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">block_row</span><span class="p">,</span><span class="w"> </span><span class="n">block_col</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">apply_activation_mixed</span><span class="p">(</span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">tid</span><span class="p">]);</span>

<span class="w">            </span><span class="k">switch</span><span class="p">(</span><span class="n">activation_type</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">case</span><span class="w"> </span><span class="no">Activation</span><span class="o">::</span><span class="no">RELU</span><span class="p">:</span>
<span class="w">                    </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">val</span><span class="p">);</span>
<span class="w">                    </span><span class="k">break</span><span class="p">;</span>

<span class="w">                </span><span class="k">case</span><span class="w"> </span><span class="no">Activation</span><span class="o">::</span><span class="no">GELU</span><span class="p">:</span>
<span class="w">                    </span><span class="c1">// GELU = x * Φ(x)，使用近似公式</span>
<span class="w">                    </span><span class="kt">float</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">val</span><span class="p">;</span>
<span class="w">                    </span><span class="kt">float</span><span class="w"> </span><span class="n">cdf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5f</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tanhf</span><span class="p">(</span>
<span class="w">                        </span><span class="mf">0.7978845608f</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.044715f</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">)</span>
<span class="w">                    </span><span class="p">));</span>
<span class="w">                    </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">cdf</span><span class="p">;</span>
<span class="w">                    </span><span class="k">break</span><span class="p">;</span>

<span class="w">                </span><span class="k">case</span><span class="w"> </span><span class="no">Activation</span><span class="o">::</span><span class="no">SWISH</span><span class="p">:</span>
<span class="w">                    </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="o">-</span><span class="n">val</span><span class="p">));</span>
<span class="w">                    </span><span class="k">break</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="n">data</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">val</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="954-flash-attention">9.5.4 Flash Attention与张量核心结合</h3>
<p>Flash Attention通过分块计算减少内存访问，与张量核心结合可进一步提升性能：</p>
<div class="codehilite"><pre><span></span><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="o">&gt;</span>
<span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">flash_attention_tensor_core</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Q</span><span class="p">,</span><span class="w">      </span><span class="c1">// [seq_len, head_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w">      </span><span class="c1">// [seq_len, head_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">V</span><span class="p">,</span><span class="w">      </span><span class="c1">// [seq_len, head_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">O</span><span class="p">,</span><span class="w">      </span><span class="c1">// [seq_len, head_dim]</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 分块大小（适配张量核心）</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">Br</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">;</span><span class="w">  </span><span class="c1">// 行块大小</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">Bc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">;</span><span class="w">  </span><span class="c1">// 列块大小</span>

<span class="w">    </span><span class="c1">// 共享内存分配</span>
<span class="w">    </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="n">half</span><span class="w"> </span><span class="n">smem</span><span class="p">[];</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Qi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">smem</span><span class="p">;</span><span class="w">                          </span><span class="c1">// [Br, head_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Kj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Qi</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Br</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w">           </span><span class="c1">// [Bc, head_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Vj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Kj</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Bc</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w">           </span><span class="c1">// [Bc, head_dim]</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Vj</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Bc</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w">            </span><span class="c1">// [Br, Bc]</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)(</span><span class="n">S</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Br</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Bc</span><span class="p">);</span><span class="w">        </span><span class="c1">// [Br] row max</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Br</span><span class="p">;</span><span class="w">                        </span><span class="c1">// [Br] row sum</span>

<span class="w">    </span><span class="c1">// 初始化输出和统计量</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">Br</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">O</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">Br</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">INFINITY</span><span class="p">;</span>
<span class="w">        </span><span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 外循环：遍历K/V的块</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">Bc</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 加载Kj和Vj块</span>
<span class="w">        </span><span class="n">load_block</span><span class="p">(</span><span class="n">K</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">,</span><span class="w"> </span><span class="n">Kj</span><span class="p">,</span><span class="w"> </span><span class="n">Bc</span><span class="p">,</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">);</span>
<span class="w">        </span><span class="n">load_block</span><span class="p">(</span><span class="n">V</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">,</span><span class="w"> </span><span class="n">Vj</span><span class="p">,</span><span class="w"> </span><span class="n">Bc</span><span class="p">,</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">);</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 使用张量核心计算S = Qi @ Kj^T</span>
<span class="w">        </span><span class="n">compute_attention_block_tc</span><span class="p">(</span><span class="n">Qi</span><span class="p">,</span><span class="w"> </span><span class="n">Kj</span><span class="p">,</span><span class="w"> </span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">Br</span><span class="p">,</span><span class="w"> </span><span class="n">Bc</span><span class="p">,</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">);</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 在线softmax更新</span>
<span class="w">        </span><span class="n">update_online_softmax</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">Br</span><span class="p">,</span><span class="w"> </span><span class="n">Bc</span><span class="p">);</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 使用张量核心计算O += S @ Vj</span>
<span class="w">        </span><span class="n">accumulate_output_tc</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="n">Vj</span><span class="p">,</span><span class="w"> </span><span class="n">O</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">Br</span><span class="p">,</span><span class="w"> </span><span class="n">Bc</span><span class="p">,</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">);</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 最终归一化</span>
<span class="w">    </span><span class="n">finalize_output</span><span class="p">(</span><span class="n">O</span><span class="p">,</span><span class="w"> </span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">Br</span><span class="p">,</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">compute_attention_block_tc</span><span class="p">(</span>
<span class="w">    </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Qi</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">Kj</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">S</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">Br</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">Bc</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">d</span>
<span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 使用WMMA计算小块</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">WMMA_M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>

<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_a</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">row_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">q_frag</span><span class="p">;</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">matrix_b</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">col_major</span><span class="o">&gt;</span><span class="w"> </span><span class="n">k_frag</span><span class="p">;</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">,</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">s_frag</span><span class="p">;</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">warp_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">num_warps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 每个warp处理S的一个子块</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">warp_id</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">Br</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">num_warps</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">WMMA_M</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">Bc</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">WMMA_N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">fill_fragment</span><span class="p">(</span><span class="n">s_frag</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 累积K维度</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">d</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">WMMA_K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">q_frag</span><span class="p">,</span><span class="w"> </span><span class="n">Qi</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">);</span>
<span class="w">                </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">k_frag</span><span class="p">,</span><span class="w"> </span><span class="n">Kj</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="p">);</span>
<span class="w">                </span><span class="n">wmma</span><span class="o">::</span><span class="n">mma_sync</span><span class="p">(</span><span class="n">s_frag</span><span class="p">,</span><span class="w"> </span><span class="n">q_frag</span><span class="p">,</span><span class="w"> </span><span class="n">k_frag</span><span class="p">,</span><span class="w"> </span><span class="n">s_frag</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="c1">// 缩放并存储</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rsqrtf</span><span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">d</span><span class="p">);</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">s_frag</span><span class="p">.</span><span class="n">num_elements</span><span class="p">;</span><span class="w"> </span><span class="n">idx</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">s_frag</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">scale</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="n">wmma</span><span class="o">::</span><span class="n">store_matrix_sync</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Bc</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="p">,</span><span class="w"> </span><span class="n">s_frag</span><span class="p">,</span><span class="w"> </span><span class="n">Bc</span><span class="p">,</span><span class="w"> </span><span class="n">wmma</span><span class="o">::</span><span class="n">mem_row_major</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="955">9.5.5 性能分析与优化结果</h3>
<p>通过张量核心优化后的Transformer性能提升显著：</p>
<div class="codehilite"><pre><span></span><code>性能对比（BERT-Large, 序列长度512, 批大小32）：
┌─────────────────────────────────────────────┐
│ 实现方式          吞吐量    延迟    内存占用  │
├─────────────────────────────────────────────┤
│ FP32 cuBLAS       1.0x     100ms    24GB    │
│ FP16 cuBLAS       2.8x     36ms     12GB    │
│ WMMA手工优化      4.2x     24ms     10GB    │
│ Flash Attention   5.6x     18ms     8GB     │
│ + Tensor Core     7.8x     13ms     8GB     │
└─────────────────────────────────────────────┘
</code></pre></div>

<p>关键优化点总结：</p>
<ol>
<li><strong>矩阵乘法加速</strong>：张量核心提供8倍FP16吞吐量</li>
<li><strong>内存带宽优化</strong>：FP16减少50%内存传输</li>
<li><strong>融合操作</strong>：减少kernel启动开销</li>
<li><strong>数值稳定性</strong>：关键操作保持FP32精度</li>
<li><strong>自适应精度</strong>：根据层类型选择最优精度</li>
</ol>
<h2 id="_3">本章小结</h2>
<p>本章深入探讨了张量核心与混合精度计算技术，这是现代GPU加速深度学习的核心。通过学习本章内容，你已经掌握了：</p>
<h3 id="_4">核心知识点</h3>
<ol>
<li>
<p><strong>张量核心架构</strong>：
   - 专用矩阵乘累加硬件单元，每周期执行大量FMA操作
   - Warp级协作计算模型，32个线程共同完成矩阵运算
   - 从Volta到Hopper的架构演进，支持越来越多的数值精度</p>
</li>
<li>
<p><strong>WMMA编程接口</strong>：
   - Fragment作为核心抽象，分布式存储矩阵数据
   - load_matrix_sync、mma_sync、store_matrix_sync三大操作
   - 支持多种矩阵尺寸组合（16×16×16、8×32×16等）</p>
</li>
<li>
<p><strong>混合精度训练</strong>：
   - FP32主权重 + FP16计算的混合策略
   - 损失缩放机制解决梯度下溢问题
   - 动态损失缩放自适应调整缩放因子</p>
</li>
<li>
<p><strong>数值稳定性保证</strong>：
   - Kahan求和算法提高累加精度
   - 稳定的激活函数实现（Softmax、LayerNorm）
   - 梯度裁剪防止数值爆炸</p>
</li>
<li>
<p><strong>实际应用优化</strong>：
   - Transformer模型的全面张量核心加速
   - Flash Attention与张量核心的结合
   - 7.8倍的端到端性能提升</p>
</li>
</ol>
<h3 id="_5">关键公式</h3>
<ol>
<li><strong>张量核心基本操作</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>D = A × B + C
其中 A: M×K, B: K×N, C/D: M×N
</code></pre></div>

<ol start="2">
<li><strong>损失缩放</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>scaled_loss = loss × scale_factor
grad = ∂(scaled_loss)/∂w / scale_factor
</code></pre></div>

<ol start="3">
<li><strong>Kahan求和</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>y = x - c        // 补偿
t = sum + y      // 新和
c = (t - sum) - y  // 新补偿项
sum = t
</code></pre></div>

<ol start="4">
<li><strong>稳定Softmax</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>softmax(x)_i = exp(x_i - max(x)) / Σ_j exp(x_j - max(x))
</code></pre></div>

<h2 id="_6">练习题</h2>
<h3 id="_7">基础题</h3>
<p><strong>练习9.1</strong>：实现一个简单的WMMA GEMM内核
编写一个使用WMMA API的矩阵乘法内核，处理256×256×256的矩阵乘法。</p>
<p><em>提示</em>：使用16×16×16的fragment配置，注意处理边界条件。</p>
<details>
<summary>参考答案</summary>
<p>关键实现要点：</p>
<ul>
<li>使用共享内存进行数据分块和重用</li>
<li>每个warp处理一个16×16的输出块</li>
<li>循环遍历K维度进行累加</li>
<li>处理非16倍数的矩阵维度需要padding或条件检查</li>
</ul>
</details>
<p><strong>练习9.2</strong>：损失缩放实验
在一个简单的神经网络训练中，比较静态和动态损失缩放的效果。测量梯度下溢的频率。</p>
<p><em>提示</em>：监控梯度的最小值和零梯度的比例。</p>
<details>
<summary>参考答案</summary>
<p>实验设计：</p>
<ul>
<li>使用FP16训练一个3层全连接网络</li>
<li>静态缩放使用固定factor=1024</li>
<li>动态缩放初始factor=65536，根据溢出情况调整</li>
<li>记录每个epoch的梯度统计信息</li>
<li>动态缩放通常能使用更大的缩放因子，减少下溢</li>
</ul>
</details>
<p><strong>练习9.3</strong>：数值精度比较
实现同一个LayerNorm操作的三个版本：纯FP16、混合精度、纯FP32。比较它们的数值误差。</p>
<p><em>提示</em>：使用Welford算法计算均值和方差，注意中间变量的精度。</p>
<details>
<summary>参考答案</summary>
<p>误差分析要点：</p>
<ul>
<li>FP16版本在大序列长度时误差显著</li>
<li>混合精度版本（统计量用FP32）接近FP32精度</li>
<li>相对误差通常在1e-3到1e-4之间</li>
<li>Welford算法比两遍扫描算法更稳定</li>
</ul>
</details>
<p><strong>练习9.4</strong>：Fragment数据布局探索
编写代码探索WMMA fragment在不同线程中的数据分布模式。</p>
<p><em>提示</em>：使用fragment.x数组访问元素，打印每个线程持有的数据索引。</p>
<details>
<summary>参考答案</summary>
<p>发现规律：</p>
<ul>
<li>每个线程持有fragment.num_elements个元素</li>
<li>数据分布是硬件特定的，不同架构可能不同</li>
<li>通常按照2×2或4×4的小块分配给线程</li>
<li>理解分布模式有助于优化数据加载</li>
</ul>
</details>
<h3 id="_8">挑战题</h3>
<p><strong>练习9.5</strong>：实现Flash Attention的张量核心版本
基于本章的示例，实现一个完整的Flash Attention算法，支持任意序列长度。</p>
<p><em>提示</em>：关键是正确实现在线softmax和分块计算的边界处理。</p>
<details>
<summary>参考答案</summary>
<p>实现要点：</p>
<ul>
<li>使用两级分块：外层适配共享内存，内层适配张量核心</li>
<li>在线softmax需要维护row-wise的最大值和求和</li>
<li>边界块需要mask处理，避免越界访问</li>
<li>性能优化：异步内存传输、双缓冲、软件流水线</li>
<li>预期性能：比标准attention快3-5倍，内存使用O(N)而非O(N²)</li>
</ul>
</details>
<p><strong>练习9.6</strong>：自适应精度选择系统
设计一个系统，根据层类型、张量大小和数值范围自动选择最优精度。</p>
<p><em>提示</em>：收集不同配置下的性能和精度数据，建立决策模型。</p>
<details>
<summary>参考答案</summary>
<p>系统设计：</p>
<ul>
<li>性能模型：基于矩阵大小预测不同精度的执行时间</li>
<li>精度模型：基于层类型和历史统计预测数值误差</li>
<li>决策逻辑：在满足精度约束下最大化性能</li>
<li>运行时适配：根据实际overflow情况动态调整</li>
<li>可以使用强化学习或贝叶斯优化自动调参</li>
</ul>
</details>
<p><strong>练习9.7</strong>：混合精度的分布式训练
实现一个支持混合精度的数据并行训练系统，优化梯度通信。</p>
<p><em>提示</em>：考虑梯度压缩、量化通信、异步all-reduce等技术。</p>
<details>
<summary>参考答案</summary>
<p>关键技术：</p>
<ul>
<li>FP16梯度通信减少50%带宽</li>
<li>梯度量化到INT8进一步压缩</li>
<li>Error feedback机制补偿量化误差</li>
<li>Gradient bucketing减少通信次数</li>
<li>重叠计算与通信隐藏延迟</li>
<li>预期：通信开销减少60-75%</li>
</ul>
</details>
<p><strong>练习9.8</strong>：张量核心的新应用探索
将张量核心应用到非传统的计算任务，如图像处理、物理仿真或密码学。</p>
<p><em>提示</em>：许多算法可以重构为矩阵运算形式。</p>
<details>
<summary>参考答案</summary>
<p>创新应用示例：</p>
<ul>
<li>图像卷积：im2col转换为GEMM</li>
<li>FFT：蝶形运算矩阵化</li>
<li>稀疏矩阵：2:4结构化稀疏</li>
<li>图神经网络：邻接矩阵运算</li>
<li>关键：算法重构的开销vs张量核心加速的收益权衡</li>
</ul>
</details>
<h2 id="_9">常见陷阱与错误</h2>
<h3 id="1-fragment">1. Fragment使用错误</h3>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：在条件分支中使用WMMA</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span><span class="w">  </span><span class="c1">// 死锁！</span>
<span class="p">}</span>

<span class="c1">// 正确：所有线程必须参与</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span>
</code></pre></div>

<h3 id="2">2. 精度混淆</h3>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：混淆累加器精度</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="n">half</span><span class="o">&gt;</span><span class="w"> </span><span class="n">acc</span><span class="p">;</span><span class="w">  </span><span class="c1">// 应该用float</span>

<span class="c1">// 正确：累加器通常使用更高精度</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="n">wmma</span><span class="o">::</span><span class="n">accumulator</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">acc</span><span class="p">;</span>
</code></pre></div>

<h3 id="3">3. 损失缩放不当</h3>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：缩放因子太小</span>
<span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">8.0f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 可能无法防止下溢</span>

<span class="c1">// 正确：使用足够大的初始缩放因子</span>
<span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">65536.0f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 2^16，留有余地</span>
</code></pre></div>

<h3 id="4">4. 数值稳定性忽视</h3>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：直接计算softmax</span>
<span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sum_exp</span><span class="p">;</span><span class="w">  </span><span class="c1">// 可能溢出</span>

<span class="c1">// 正确：减去最大值</span>
<span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_val</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sum_exp</span><span class="p">;</span>
</code></pre></div>

<h3 id="5">5. 内存对齐问题</h3>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：未对齐的地址</span>
<span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">half</span><span class="o">*</span><span class="p">)((</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">base</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w">  </span><span class="c1">// 奇数字节偏移</span>
<span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">frag</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span><span class="w">  </span><span class="c1">// 可能出错</span>

<span class="c1">// 正确：确保16字节对齐</span>
<span class="n">half</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">half</span><span class="o">*</span><span class="p">)((</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">base</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="o">~</span><span class="mh">0xF</span><span class="p">);</span>
</code></pre></div>

<h3 id="6">6. 性能陷阱</h3>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：频繁的精度转换</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="c1">// 单个元素处理</span>
<span class="w">    </span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half</span><span class="p">(</span><span class="n">val</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// 正确：向量化处理</span>
<span class="n">half2</span><span class="o">*</span><span class="w"> </span><span class="n">h2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">half2</span><span class="o">*</span><span class="p">)</span><span class="n">h</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">h2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__hmul2</span><span class="p">(</span><span class="n">h2</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">__float2half2_rn</span><span class="p">(</span><span class="mf">2.0f</span><span class="p">));</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="_10">最佳实践检查清单</h2>
<h3 id="_11">设计阶段</h3>
<ul>
<li>[ ] 识别计算密集型的矩阵运算</li>
<li>[ ] 评估精度要求，确定可以使用混合精度的部分</li>
<li>[ ] 设计数据布局以适配张量核心要求</li>
<li>[ ] 规划内存层次结构和数据重用策略</li>
</ul>
<h3 id="_12">实现阶段</h3>
<ul>
<li>[ ] 使用适当的矩阵尺寸（16的倍数）</li>
<li>[ ] 确保内存对齐（16字节或32字节）</li>
<li>[ ] 实现损失缩放机制</li>
<li>[ ] 为精度敏感操作保留FP32路径</li>
<li>[ ] 添加溢出检测和处理逻辑</li>
</ul>
<h3 id="_13">优化阶段</h3>
<ul>
<li>[ ] 最大化张量核心利用率（&gt;75%）</li>
<li>[ ] 减少精度转换开销</li>
<li>[ ] 使用共享内存减少全局内存访问</li>
<li>[ ] 实现双缓冲和软件流水线</li>
<li>[ ] 融合相邻的操作减少kernel启动</li>
</ul>
<h3 id="_14">验证阶段</h3>
<ul>
<li>[ ] 比较混合精度与FP32的数值误差</li>
<li>[ ] 测试极端输入（很大/很小的值）</li>
<li>[ ] 验证梯度流的正确性</li>
<li>[ ] 监控训练稳定性和收敛速度</li>
<li>[ ] 进行长时间训练的稳定性测试</li>
</ul>
<h3 id="_15">部署阶段</h3>
<ul>
<li>[ ] 根据硬件能力选择最优配置</li>
<li>[ ] 实现自适应精度策略</li>
<li>[ ] 提供精度与性能的权衡选项</li>
<li>[ ] 添加性能监控和诊断工具</li>
<li>[ ] 准备不同GPU架构的后备方案</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter8.html" class="nav-link prev">← 第8章：PTX内联与底层优化</a><a href="chapter10.html" class="nav-link next">第10章：CUTLASS深度解析 →</a></nav>
        </main>
    </div>
</body>
</html>