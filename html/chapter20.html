<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第20章：CUDA Graph与内核融合</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">CUDA 高性能编程实战教程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：CUDA硬件架构深度剖析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：CUDA编程模型与执行模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：全局内存优化策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：共享内存与Bank Conflict</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：寄存器优化与常量内存</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：Warp级编程与协作组</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：原子操作与同步原语</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：PTX内联与底层优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：张量核心与混合精度计算</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：CUTLASS深度解析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：激光雷达点云处理加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：多传感器融合的并行化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：实时语义分割与实例分割</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：路径规划与轨迹优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：视觉SLAM的GPU加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：机械臂运动规划</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第17章：强化学习推理加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第18章：大规模点云重建与网格化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第19章：多GPU编程与扩展</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第20章：CUDA Graph与内核融合</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第21章：嵌入式GPU开发（Jetson）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第22章：稀疏计算与动态稀疏</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第23章：量化与低精度计算</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第24章：新一代GPU特性展望</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第25章：性能分析与调优方法论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第26章：CUDA调试技术与错误处理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter27.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第27章：开发环境与工具链配置</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="20cuda-graph">第20章：CUDA Graph与内核融合</h1>
<p>在现代AI推理系统中，单个内核的优化已经接近硬件极限，而系统级的优化成为了提升性能的关键。CUDA Graph通过消除CPU-GPU同步开销实现了执行流的极致优化，而内核融合则通过减少内存访问次数大幅提升了算子效率。本章将深入探讨这两项关键技术，并结合JIT编译和自动调优框架，帮助你构建端到端的高性能推理系统。通过学习本章，你将掌握将离散的CUDA内核组织成高效执行图的方法，理解不同层次的融合策略，并能够针对特定硬件和工作负载进行自动优化。</p>
<h2 id="201-cuda-graph">20.1 CUDA Graph构建与优化</h2>
<h3 id="2011-graph">20.1.1 Graph基本概念与执行模型</h3>
<p>CUDA Graph是CUDA 10引入的革命性特性，它将一系列CUDA操作（内核启动、内存拷贝、同步等）封装成一个可重复执行的图结构。与传统的流式执行相比，Graph模式具有以下优势：</p>
<ol>
<li><strong>极低的启动开销</strong>：Graph实例化后，整个执行序列通过单次API调用完成，避免了反复的内核启动开销</li>
<li><strong>优化的调度</strong>：驱动程序可以预先分析整个执行图，进行全局优化</li>
<li><strong>确定性执行</strong>：固定的执行模式有利于性能预测和调试</li>
</ol>
<p>Graph的核心组件包括：</p>
<ul>
<li><strong>节点（Node）</strong>：代表单个操作，如内核启动、memcpy、event记录等</li>
<li><strong>边（Edge）</strong>：定义节点间的依赖关系</li>
<li><strong>图模板（Graph Template）</strong>：定义执行拓扑的蓝图</li>
<li><strong>图实例（Executable Graph）</strong>：可以实际执行的图对象</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="n">Graph执行流程</span><span class="err">：</span>
<span class="w">     </span><span class="err">创建</span><span class="w">           </span><span class="err">实例化</span><span class="w">            </span><span class="err">执行</span>
<span class="n">Template</span><span class="w"> </span><span class="o">-----&gt;</span><span class="w"> </span><span class="n">Executable</span><span class="w"> </span><span class="o">-----&gt;</span><span class="w"> </span><span class="n">Launch</span>
<span class="w">  </span><span class="nf">Graph</span><span class="w">           </span><span class="nf">Graph</span><span class="w">           </span><span class="p">(</span><span class="err">重复</span><span class="p">)</span>
<span class="w">    </span><span class="o">|</span><span class="w">               </span><span class="o">|</span><span class="w">                </span><span class="o">|</span>
<span class="w">  </span><span class="err">定义拓扑</span><span class="w">        </span><span class="err">资源分配</span><span class="w">         </span><span class="err">高效执行</span>
</code></pre></div>

<h3 id="2012">20.1.2 流捕获机制详解</h3>
<p>流捕获（Stream Capture）是构建CUDA Graph最便捷的方式，它能够自动记录流上的操作序列：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 基本流捕获模式</span>
<span class="n">cudaGraph_t</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>

<span class="c1">// 开始捕获</span>
<span class="n">cudaStreamBeginCapture</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStreamCaptureModeGlobal</span><span class="p">);</span>

<span class="c1">// 记录操作序列</span>
<span class="n">kernel1</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid1</span><span class="p">,</span><span class="w"> </span><span class="n">block1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>
<span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToDevice</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="n">kernel2</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid2</span><span class="p">,</span><span class="w"> </span><span class="n">block2</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>

<span class="c1">// 结束捕获</span>
<span class="n">cudaStreamEndCapture</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>

<span class="c1">// 实例化图</span>
<span class="n">cudaGraphExec_t</span><span class="w"> </span><span class="n">graphExec</span><span class="p">;</span>
<span class="n">cudaGraphInstantiate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="c1">// 执行图（可重复执行）</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">iterations</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaGraphLaunch</span><span class="p">(</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>流捕获支持三种模式：</p>
<ul>
<li><strong>Global模式</strong>：捕获所有相关流的操作</li>
<li><strong>Local模式</strong>：仅捕获指定流的操作</li>
<li><strong>Relaxed模式</strong>：允许某些操作逃逸捕获</li>
</ul>
<p>对于复杂的多流场景，需要careful处理流间同步：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 多流捕获与同步</span>
<span class="n">cudaStreamBeginCapture</span><span class="p">(</span><span class="n">stream1</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStreamCaptureModeGlobal</span><span class="p">);</span>

<span class="n">kernel_a</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>
<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event1</span><span class="p">,</span><span class="w"> </span><span class="n">stream1</span><span class="p">);</span>

<span class="c1">// stream2等待stream1</span>
<span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream2</span><span class="p">,</span><span class="w"> </span><span class="n">event1</span><span class="p">);</span>
<span class="n">kernel_b</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream2</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>

<span class="c1">// 合并回主流</span>
<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event2</span><span class="p">,</span><span class="w"> </span><span class="n">stream2</span><span class="p">);</span>
<span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream1</span><span class="p">,</span><span class="w"> </span><span class="n">event2</span><span class="p">);</span>
<span class="n">kernel_c</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>

<span class="n">cudaStreamEndCapture</span><span class="p">(</span><span class="n">stream1</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>
</code></pre></div>

<h3 id="2013-graph">20.1.3 手动Graph构建与节点管理</h3>
<p>对于需要精细控制的场景，手动构建Graph提供了最大的灵活性：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 手动创建Graph节点</span>
<span class="n">cudaGraph_t</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="n">cudaGraphCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="c1">// 添加内核节点</span>
<span class="n">cudaGraphNode_t</span><span class="w"> </span><span class="n">kernelNode1</span><span class="p">,</span><span class="w"> </span><span class="n">kernelNode2</span><span class="p">;</span>
<span class="n">cudaKernelNodeParams</span><span class="w"> </span><span class="n">kernelParams</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">kernelParams</span><span class="p">.</span><span class="n">func</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">myKernel</span><span class="p">;</span>
<span class="n">kernelParams</span><span class="p">.</span><span class="n">gridDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gridDim</span><span class="p">;</span>
<span class="n">kernelParams</span><span class="p">.</span><span class="n">blockDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockDim</span><span class="p">;</span>
<span class="n">kernelParams</span><span class="p">.</span><span class="n">sharedMemBytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="n">kernelParams</span><span class="p">.</span><span class="n">kernelParams</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kernelArgs</span><span class="p">;</span>

<span class="n">cudaGraphAddKernelNode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernelNode1</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">kernelParams</span><span class="p">);</span>

<span class="c1">// 添加内存拷贝节点</span>
<span class="n">cudaGraphNode_t</span><span class="w"> </span><span class="n">memcpyNode</span><span class="p">;</span>
<span class="n">cudaMemcpy3DParms</span><span class="w"> </span><span class="n">memcpyParams</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="c1">// 配置memcpyParams...</span>
<span class="n">cudaGraphAddMemcpyNode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">memcpyNode</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">kernelNode1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">memcpyParams</span><span class="p">);</span>

<span class="c1">// 设置依赖关系</span>
<span class="n">cudaGraphNode_t</span><span class="w"> </span><span class="n">dependencies</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">kernelNode1</span><span class="p">,</span><span class="w"> </span><span class="n">memcpyNode</span><span class="p">};</span>
<span class="n">cudaGraphAddKernelNode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernelNode2</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="n">dependencies</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">kernelParams2</span><span class="p">);</span>
</code></pre></div>

<p>Graph支持的节点类型包括：</p>
<ul>
<li>Kernel节点：GPU内核执行</li>
<li>Memcpy节点：内存传输操作</li>
<li>Memset节点：内存初始化</li>
<li>Host节点：CPU回调函数</li>
<li>Child Graph节点：嵌套子图</li>
<li>Event节点：事件记录与等待</li>
<li>Conditional节点：条件执行（CUDA 12.3+）</li>
</ul>
<h3 id="2014-graph">20.1.4 Graph更新与条件执行</h3>
<p>动态更新Graph参数是实现灵活执行的关键：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 更新内核参数</span>
<span class="n">cudaKernelNodeParams</span><span class="w"> </span><span class="n">updatedParams</span><span class="p">;</span>
<span class="n">cudaGraphKernelNodeGetParams</span><span class="p">(</span><span class="n">kernelNode</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">updatedParams</span><span class="p">);</span>
<span class="n">updatedParams</span><span class="p">.</span><span class="n">kernelParams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">newValue</span><span class="p">;</span>
<span class="n">cudaGraphExecKernelNodeSetParams</span><span class="p">(</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">kernelNode</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">updatedParams</span><span class="p">);</span>

<span class="c1">// 批量更新多个节点</span>
<span class="n">cudaGraphExecUpdate</span><span class="p">(</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">updateResult</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">updateResult</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">cudaGraphExecUpdateSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 需要重新实例化</span>
<span class="w">    </span><span class="n">cudaGraphExecDestroy</span><span class="p">(</span><span class="n">graphExec</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaGraphInstantiate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>CUDA 12.3引入的条件节点支持动态控制流：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 创建条件节点</span>
<span class="n">cudaGraphNode_t</span><span class="w"> </span><span class="n">conditionalNode</span><span class="p">;</span>
<span class="n">cudaGraphConditionalHandle</span><span class="w"> </span><span class="n">handle</span><span class="p">;</span>
<span class="n">cudaGraphAddConditionalNode</span><span class="p">(</span><span class="o">&amp;</span><span class="n">conditionalNode</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="n">deps</span><span class="p">,</span><span class="w"> </span><span class="n">numDeps</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">handle</span><span class="p">);</span>

<span class="c1">// 设置条件分支</span>
<span class="n">cudaGraph_t</span><span class="w"> </span><span class="n">graphIf</span><span class="p">,</span><span class="w"> </span><span class="n">graphElse</span><span class="p">;</span>
<span class="c1">// 构建分支图...</span>
<span class="n">cudaGraphConditionalNodeSetParams</span><span class="p">(</span><span class="n">conditionalNode</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">graphIf</span><span class="p">,</span><span class="w"> </span><span class="n">graphElse</span><span class="p">);</span>
</code></pre></div>

<h3 id="2015">20.1.5 性能优化技巧</h3>
<p>Graph优化的关键在于最大化并行度和最小化同步点：</p>
<ol>
<li><strong>合并小内核</strong>：将多个小内核合并成一个节点，减少调度开销</li>
<li><strong>异步操作链</strong>：尽可能使用异步操作，避免隐式同步</li>
<li><strong>预分配资源</strong>：Graph实例化时预分配所有资源，避免运行时分配</li>
<li><strong>图分区</strong>：将大图分解为多个子图，便于并行执行和更新</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 优化示例：使用持久化内核减少启动开销</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">persistentKernel</span><span class="p">(</span><span class="k">volatile</span><span class="w"> </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">flag</span><span class="p">,</span><span class="w"> </span><span class="p">...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">flag</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 执行工作</span>
<span class="w">        </span><span class="n">processWork</span><span class="p">();</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>性能分析要点：</p>
<ul>
<li>使用Nsight Systems查看Graph执行时间线</li>
<li>监控Graph实例化开销</li>
<li>分析节点间的依赖关系是否合理</li>
<li>评估Graph更新频率对性能的影响</li>
</ul>
<h2 id="202">20.2 内核融合策略</h2>
<h3 id="2021">20.2.1 融合的动机与收益分析</h3>
<p>内核融合是优化GPU程序的核心技术之一，其主要动机包括：</p>
<ol>
<li><strong>减少内存访问</strong>：中间结果保持在寄存器或共享内存中，避免全局内存往返</li>
<li><strong>降低启动开销</strong>：减少内核启动次数，特别是对小规模问题</li>
<li><strong>提高数据局部性</strong>：融合后的内核能更好地利用缓存</li>
<li><strong>增加并行机会</strong>：跨操作的并行优化</li>
</ol>
<p>融合收益的量化分析：</p>
<div class="codehilite"><pre><span></span><code>未融合：Kernel1(读A,写B) + Kernel2(读B,写C) + Kernel3(读C,写D)
内存访问：2N + 2N + 2N = 6N

融合后：FusedKernel(读A,写D)  
内存访问：2N

理论加速比：3x（仅考虑内存带宽）
</code></pre></div>

<p>实际收益受多个因素影响：</p>
<ul>
<li>算术强度（compute/memory ratio）</li>
<li>寄存器压力</li>
<li>共享内存使用量</li>
<li>占用率变化</li>
</ul>
<h3 id="2022">20.2.2 垂直融合技术</h3>
<p>垂直融合（Vertical Fusion）将producer-consumer关系的内核合并：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 未融合版本</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">relu</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">add_bias</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">bias</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">BIAS_SIZE</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 垂直融合版本</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_relu_bias</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]);</span><span class="w">  </span><span class="c1">// ReLU</span>
<span class="w">        </span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">bias</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">BIAS_SIZE</span><span class="p">];</span><span class="w">  </span><span class="c1">// Add bias</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>高级垂直融合模式：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 多层融合with共享内存优化</span>
<span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_SIZE</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_conv_bn_relu</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">bn_params</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">C</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">tile</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">][</span><span class="n">TILE_SIZE</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// 1. 卷积计算</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">conv_result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="c1">// 卷积逻辑...</span>

<span class="w">    </span><span class="c1">// 2. BatchNorm（融合在寄存器级）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bn_params</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bn_params</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bn_params</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bn_params</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">conv_result</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">var</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1e-5f</span><span class="p">);</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">bn_result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">beta</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 3. ReLU激活</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">bn_result</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="2023">20.2.3 水平融合技术</h3>
<p>水平融合（Horizontal Fusion）将独立的操作合并以提高硬件利用率：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 水平融合多个独立的GEMV操作</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_multi_gemv</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y1</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A2</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">y2</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">row</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">M</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">sum1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">sum2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 交错执行两个GEMV，提高指令级并行</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">sum1</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A1</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x1</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
<span class="w">            </span><span class="n">sum2</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A2</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x2</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">y1</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum1</span><span class="p">;</span>
<span class="w">        </span><span class="n">y2</span><span class="p">[</span><span class="n">row</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sum2</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>动态批处理融合：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 将多个小批次请求融合执行</span>
<span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">MAX_BATCH</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_batch_inference</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">**</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">**</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="o">*</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_count</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">work_assignment</span><span class="p">[</span><span class="n">MAX_BATCH</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// 动态工作分配</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batch_count</span><span class="p">;</span><span class="w"> </span><span class="n">b</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">work_assignment</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">offset</span><span class="p">;</span>
<span class="w">            </span><span class="n">offset</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">sizes</span><span class="p">[</span><span class="n">b</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 并行处理多个批次</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">global_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">find_batch</span><span class="p">(</span><span class="n">global_idx</span><span class="p">,</span><span class="w"> </span><span class="n">work_assignment</span><span class="p">);</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">local_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global_idx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">work_assignment</span><span class="p">[</span><span class="n">batch_id</span><span class="p">];</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch_id</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batch_count</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">local_idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">sizes</span><span class="p">[</span><span class="n">batch_id</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">process_item</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">batch_id</span><span class="p">],</span><span class="w"> </span><span class="n">outputs</span><span class="p">[</span><span class="n">batch_id</span><span class="p">],</span><span class="w"> </span><span class="n">local_idx</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="2024-element-wise">20.2.4 Element-wise操作融合</h3>
<p>Element-wise操作是融合的理想目标，因为它们通常是memory-bound的：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 融合复杂的element-wise表达式</span>
<span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_gelu_dropout_scale</span><span class="p">(</span>
<span class="w">    </span><span class="n">T</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">dropout_mask</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">scale</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">dropout_prob</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">    </span><span class="n">T</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// GELU: x * Φ(x)</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="n">cdf_const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5f</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="n">sqrt_2_over_pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.7978845608028654f</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="n">fitting_const</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.044715f</span><span class="p">;</span>

<span class="w">    </span><span class="n">T</span><span class="w"> </span><span class="n">x_cubed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="n">T</span><span class="w"> </span><span class="n">tanh_arg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrt_2_over_pi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fitting_const</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x_cubed</span><span class="p">);</span>
<span class="w">    </span><span class="n">T</span><span class="w"> </span><span class="n">tanh_result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanhf</span><span class="p">(</span><span class="n">tanh_arg</span><span class="p">);</span>
<span class="w">    </span><span class="n">T</span><span class="w"> </span><span class="n">gelu_result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cdf_const</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tanh_result</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Dropout</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">keep_prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">dropout_prob</span><span class="p">;</span>
<span class="w">    </span><span class="n">T</span><span class="w"> </span><span class="n">dropout_result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dropout_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">dropout_prob</span><span class="w"> </span><span class="o">?</span><span class="w"> </span>
<span class="w">                       </span><span class="n">gelu_result</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">keep_prob</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Scale</span>
<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dropout_result</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">scale</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>向量化融合优化：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用向量化加载/存储提高带宽利用率</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_elementwise_vec4</span><span class="p">(</span>
<span class="w">    </span><span class="n">float4</span><span class="o">*</span><span class="w"> </span><span class="n">input1</span><span class="p">,</span><span class="w"> </span><span class="n">float4</span><span class="o">*</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="n">float4</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">    </span><span class="n">float4</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input1</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">    </span><span class="n">float4</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input2</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// 融合多个操作</span>
<span class="w">    </span><span class="n">float4</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaf</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">);</span><span class="w">  </span><span class="c1">// a*b+1</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaf</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">);</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaf</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">);</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaf</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// ReLU</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>
<span class="w">    </span><span class="n">result</span><span class="p">.</span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>

<span class="w">    </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="2025-reduction">20.2.5 Reduction融合技术</h3>
<p>Reduction操作的融合需要特殊考虑，因为它们改变了数据维度：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 融合reduction和后续操作</span>
<span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fused_reduction_softmax</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared</span><span class="p">[];</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">row_max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shared</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">row_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">shared</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Phase 1: 找到行最大值（融合）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">thread_max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">thread_max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">thread_max</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">row_max</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">thread_max</span><span class="p">;</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 树形归约找最大值</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">&gt;&gt;=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">row_max</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">row_max</span><span class="p">[</span><span class="n">tid</span><span class="p">],</span><span class="w"> </span><span class="n">row_max</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">stride</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">max_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">row_max</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Phase 2: 计算exp和sum（融合）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">thread_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">exp_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_val</span><span class="p">);</span>
<span class="w">        </span><span class="n">thread_sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">exp_val</span><span class="p">;</span>
<span class="w">        </span><span class="c1">// 临时存储exp值以复用</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exp_val</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">row_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">thread_sum</span><span class="p">;</span>
<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// 树形归约求和</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">&gt;&gt;=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tid</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">row_sum</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">row_sum</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">stride</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">__syncthreads</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">sum_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">row_sum</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// Phase 3: 归一化（融合）</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">row</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">col</span><span class="p">]</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">sum_val</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="203">20.3 自动调优框架</h2>
<h3 id="2031-profile-guided-optimization">20.3.1 Profile-Guided Optimization</h3>
<p>基于性能剖析的优化是自动调优的基础：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">AutoTuner</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">KernelConfig</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">dim3</span><span class="w"> </span><span class="n">gridDim</span><span class="p">;</span>
<span class="w">        </span><span class="n">dim3</span><span class="w"> </span><span class="n">blockDim</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">sharedMem</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">params</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">executionTime</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">KernelConfig</span><span class="o">&gt;</span><span class="w"> </span><span class="n">searchSpace</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">profileKernel</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">KernelConfig</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">cudaEvent_t</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">stop</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">start</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stop</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Warmup</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">gridDim</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">,</span>
<span class="w">                           </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">sharedMem</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// Profile</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">start</span><span class="p">);</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">gridDim</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">,</span>
<span class="w">                           </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">sharedMem</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>

<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">milliseconds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaEventElapsedTime</span><span class="p">(</span><span class="o">&amp;</span><span class="n">milliseconds</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">stop</span><span class="p">);</span>
<span class="w">        </span><span class="n">config</span><span class="p">.</span><span class="n">executionTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">milliseconds</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">100.0f</span><span class="p">;</span>

<span class="w">        </span><span class="n">cudaEventDestroy</span><span class="p">(</span><span class="n">start</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventDestroy</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">findBestConfig</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">bestConfig</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">bestTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FLT_MAX</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">searchSpace</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">profileKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">);</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">executionTime</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">bestTime</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">bestTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">executionTime</span><span class="p">;</span>
<span class="w">                </span><span class="n">bestConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">config</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">bestConfig</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2032">20.3.2 搜索空间定义与剪枝</h3>
<p>有效的搜索空间定义是快速收敛的关键：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SearchSpaceGenerator</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">Constraints</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">maxThreadsPerBlock</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">maxSharedMemory</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">warpSize</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">smCount</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">KernelConfig</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generateSearchSpace</span><span class="p">(</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">problemSize</span><span class="p">,</span><span class="w"> </span><span class="n">Constraints</span><span class="o">&amp;</span><span class="w"> </span><span class="n">hw</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">KernelConfig</span><span class="o">&gt;</span><span class="w"> </span><span class="n">configs</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 生成block size候选</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">blockSizes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span><span class="w"> </span><span class="mi">1024</span><span class="p">};</span>

<span class="w">        </span><span class="c1">// 生成tile size候选</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tileSizes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">};</span>

<span class="w">        </span><span class="c1">// 生成unroll factor候选</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unrollFactors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">};</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">bs</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">blockSizes</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">bs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">hw</span><span class="p">.</span><span class="n">maxThreadsPerBlock</span><span class="p">)</span><span class="w"> </span><span class="k">continue</span><span class="p">;</span>

<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ts</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tileSizes</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">sharedMem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ts</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ts</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">sharedMem</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">hw</span><span class="p">.</span><span class="n">maxSharedMemory</span><span class="p">)</span><span class="w"> </span><span class="k">continue</span><span class="p">;</span>

<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">uf</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">unrollFactors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">config</span><span class="p">;</span>
<span class="w">                    </span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dim3</span><span class="p">(</span><span class="n">bs</span><span class="p">);</span>
<span class="w">                    </span><span class="n">config</span><span class="p">.</span><span class="n">gridDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dim3</span><span class="p">((</span><span class="n">problemSize</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">bs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">bs</span><span class="p">);</span>
<span class="w">                    </span><span class="n">config</span><span class="p">.</span><span class="n">sharedMem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sharedMem</span><span class="p">;</span>
<span class="w">                    </span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">&quot;TILE_SIZE&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ts</span><span class="p">;</span>
<span class="w">                    </span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">&quot;UNROLL_FACTOR&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">uf</span><span class="p">;</span>

<span class="w">                    </span><span class="c1">// 启发式剪枝</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">isValidConfig</span><span class="p">(</span><span class="n">config</span><span class="p">,</span><span class="w"> </span><span class="n">hw</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="n">configs</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">configs</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">isValidConfig</span><span class="p">(</span><span class="n">KernelConfig</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="p">,</span><span class="w"> </span><span class="n">Constraints</span><span class="o">&amp;</span><span class="w"> </span><span class="n">hw</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 占用率检查</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">blocksPerSM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hw</span><span class="p">.</span><span class="n">maxThreadsPerBlock</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">blocksPerSM</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 寄存器压力估计</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">estimatedRegs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">&quot;TILE_SIZE&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">estimatedRegs</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">255</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2033">20.3.3 贝叶斯优化策略</h3>
<p>贝叶斯优化通过建立性能模型来指导搜索：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">BayesianOptimizer</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="c1">// 高斯过程模型</span>
<span class="w">    </span><span class="k">class</span><span class="w"> </span><span class="nc">GaussianProcess</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">public</span><span class="o">:</span>
<span class="w">        </span><span class="kt">void</span><span class="w"> </span><span class="n">fit</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">KernelConfig</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">configs</span><span class="p">,</span>
<span class="w">                </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">performances</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 构建协方差矩阵</span>
<span class="w">            </span><span class="c1">// 使用RBF核函数</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">KernelConfig</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 返回均值和方差</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">variance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">            </span><span class="c1">// 计算预测...</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">variance</span><span class="p">};</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">GaussianProcess</span><span class="w"> </span><span class="n">gp</span><span class="p">;</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="nf">acquisitionFunction</span><span class="p">(</span><span class="n">KernelConfig</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="p">[</span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">variance</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gp</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Expected Improvement (EI)</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">best_so_far</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getBestPerformance</span><span class="p">();</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">improvement</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_so_far</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">variance</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">1e-6f</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">Z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">improvement</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">variance</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">ei</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">improvement</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">normcdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">normpdf</span><span class="p">(</span><span class="n">Z</span><span class="p">);</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ei</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">optimize</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">iterations</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">KernelConfig</span><span class="o">&gt;</span><span class="w"> </span><span class="n">evaluated</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">performances</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 初始随机采样</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">randomSample</span><span class="p">();</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">perf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">evaluateKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="p">);</span>
<span class="w">            </span><span class="n">evaluated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>
<span class="w">            </span><span class="n">performances</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">perf</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 贝叶斯优化迭代</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">iterations</span><span class="p">;</span><span class="w"> </span><span class="n">iter</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">gp</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">evaluated</span><span class="p">,</span><span class="w"> </span><span class="n">performances</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 选择下一个采样点</span>
<span class="w">            </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">nextConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">argmax</span><span class="p">(</span><span class="n">acquisitionFunction</span><span class="p">);</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">perf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">evaluateKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">nextConfig</span><span class="p">);</span>

<span class="w">            </span><span class="n">evaluated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">nextConfig</span><span class="p">);</span>
<span class="w">            </span><span class="n">performances</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">perf</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">getBestConfig</span><span class="p">(</span><span class="n">evaluated</span><span class="p">,</span><span class="w"> </span><span class="n">performances</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2034">20.3.4 遗传算法优化</h3>
<p>遗传算法适合离散的大规模搜索空间：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">GeneticOptimizer</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">Individual</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">config</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">fitness</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Individual</span><span class="o">&gt;</span><span class="w"> </span><span class="n">population</span><span class="p">;</span>

<span class="w">    </span><span class="n">Individual</span><span class="w"> </span><span class="nf">crossover</span><span class="p">(</span><span class="n">Individual</span><span class="o">&amp;</span><span class="w"> </span><span class="n">parent1</span><span class="p">,</span><span class="w"> </span><span class="n">Individual</span><span class="o">&amp;</span><span class="w"> </span><span class="n">parent2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">Individual</span><span class="w"> </span><span class="n">child</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 均匀交叉</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">child</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">parent1</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">child</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">parent2</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 参数交叉</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="p">[</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">parent1</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">child</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">value</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">child</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">parent2</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">];</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">child</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">mutate</span><span class="p">(</span><span class="n">Individual</span><span class="o">&amp;</span><span class="w"> </span><span class="n">individual</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">mutationRate</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">randf</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">mutationRate</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 随机改变block size</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">blockSizes</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">};</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">()</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span>
<span class="w">            </span><span class="n">individual</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockSizes</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 参数突变</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="p">[</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">individual</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">randf</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">mutationRate</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mutateParameter</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">KernelConfig</span><span class="w"> </span><span class="n">evolve</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">generations</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">POP_SIZE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">MUTATION_RATE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1f</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">ELITE_RATIO</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.2f</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 初始化种群</span>
<span class="w">        </span><span class="n">population</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateRandomPopulation</span><span class="p">(</span><span class="n">POP_SIZE</span><span class="p">);</span>
<span class="w">        </span><span class="n">evaluatePopulation</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">);</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">gen</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">gen</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">generations</span><span class="p">;</span><span class="w"> </span><span class="n">gen</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="n">population</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">population</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
<span class="w">                     </span><span class="p">[](</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">fitness</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">fitness</span><span class="p">;</span><span class="w"> </span><span class="p">});</span>

<span class="w">            </span><span class="c1">// 精英保留</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">eliteSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">POP_SIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ELITE_RATIO</span><span class="p">;</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Individual</span><span class="o">&gt;</span><span class="w"> </span><span class="n">newPopulation</span><span class="p">(</span>
<span class="w">                </span><span class="n">population</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">population</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">eliteSize</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 交叉和突变</span>
<span class="w">            </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">newPopulation</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">POP_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">Individual</span><span class="w"> </span><span class="n">parent1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tournamentSelection</span><span class="p">();</span>
<span class="w">                </span><span class="n">Individual</span><span class="w"> </span><span class="n">parent2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tournamentSelection</span><span class="p">();</span>
<span class="w">                </span><span class="n">Individual</span><span class="w"> </span><span class="n">child</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">crossover</span><span class="p">(</span><span class="n">parent1</span><span class="p">,</span><span class="w"> </span><span class="n">parent2</span><span class="p">);</span>
<span class="w">                </span><span class="n">mutate</span><span class="p">(</span><span class="n">child</span><span class="p">,</span><span class="w"> </span><span class="n">MUTATION_RATE</span><span class="p">);</span>
<span class="w">                </span><span class="n">newPopulation</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">child</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="n">population</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">newPopulation</span><span class="p">;</span>
<span class="w">            </span><span class="n">evaluatePopulation</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">population</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">config</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2035">20.3.5 性能模型与预测</h3>
<p>构建准确的性能模型可以减少实际评估次数：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PerformanceModel</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">HardwareFeatures</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">peakGFLOPS</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">memoryBandwidth</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">smCount</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">l2CacheSize</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">HardwareFeatures</span><span class="w"> </span><span class="n">hw</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">predictKernelTime</span><span class="p">(</span><span class="n">KernelConfig</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="p">,</span><span class="w"> </span>
<span class="w">                           </span><span class="kt">int</span><span class="w"> </span><span class="n">computeOps</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">memoryOps</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Roofline模型预测</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">arithmeticIntensity</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">computeOps</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">memoryOps</span><span class="p">;</span>

<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">computeTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">computeOps</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">hw</span><span class="p">.</span><span class="n">peakGFLOPS</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e9</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">memoryTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memoryOps</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">hw</span><span class="p">.</span><span class="n">memoryBandwidth</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e9</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 考虑占用率</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">threadsPerSM</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">occupancy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">min</span><span class="p">(</span><span class="mf">1.0f</span><span class="p">,</span><span class="w"> </span><span class="n">threadsPerSM</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">2048.0f</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 考虑缓存效果</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">cacheHitRate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">estimateCacheHitRate</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">effectiveMemTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memoryTime</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">cacheHitRate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">0.8f</span><span class="p">);</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">max</span><span class="p">(</span><span class="n">computeTime</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">occupancy</span><span class="p">,</span><span class="w"> </span><span class="n">effectiveMemTime</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">estimateCacheHitRate</span><span class="p">(</span><span class="n">KernelConfig</span><span class="o">&amp;</span><span class="w"> </span><span class="n">config</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">workingSet</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">&quot;TILE_SIZE&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span>
<span class="w">                        </span><span class="n">config</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">&quot;TILE_SIZE&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">workingSet</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hw</span><span class="p">.</span><span class="n">l2CacheSize</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="mf">0.8f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 高缓存命中率</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="mf">0.2f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 低缓存命中率</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h2 id="204-jit">20.4 JIT编译优化</h2>
<h3 id="2041-nvrtc">20.4.1 NVRTC运行时编译</h3>
<p>NVRTC（NVIDIA Runtime Compilation）允许在运行时编译CUDA代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">JITCompiler</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="n">nvrtcProgram</span><span class="w"> </span><span class="n">prog</span><span class="p">;</span>
<span class="w">    </span><span class="n">CUmodule</span><span class="w"> </span><span class="k">module</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">CUfunction</span><span class="o">&gt;</span><span class="w"> </span><span class="n">functionCache</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">compileKernel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span>
<span class="w">                       </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">kernelName</span><span class="p">,</span>
<span class="w">                       </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 创建程序</span>
<span class="w">        </span><span class="n">nvrtcCreateProgram</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span>
<span class="w">                          </span><span class="p">(</span><span class="n">kernelName</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;.cu&quot;</span><span class="p">).</span><span class="n">c_str</span><span class="p">(),</span>
<span class="w">                          </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 编译选项</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">opts</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">opt</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">opts</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">opt</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 编译</span>
<span class="w">        </span><span class="n">nvrtcResult</span><span class="w"> </span><span class="n">compileResult</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvrtcCompileProgram</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                                        </span><span class="n">opts</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span>
<span class="w">                                                        </span><span class="n">opts</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">compileResult</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">NVRTC_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">logSize</span><span class="p">;</span>
<span class="w">            </span><span class="n">nvrtcGetProgramLogSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">logSize</span><span class="p">);</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="w"> </span><span class="n">log</span><span class="p">(</span><span class="n">logSize</span><span class="p">);</span>
<span class="w">            </span><span class="n">nvrtcGetProgramLog</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">log</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>
<span class="w">            </span><span class="k">throw</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">runtime_error</span><span class="p">(</span><span class="s">&quot;Compilation failed: &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">                                   </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="n">log</span><span class="p">.</span><span class="n">data</span><span class="p">()));</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 获取PTX</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">ptxSize</span><span class="p">;</span>
<span class="w">        </span><span class="n">nvrtcGetPTXSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">ptxSize</span><span class="p">);</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="w"> </span><span class="n">ptx</span><span class="p">(</span><span class="n">ptxSize</span><span class="p">);</span>
<span class="w">        </span><span class="n">nvrtcGetPTX</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">ptx</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>

<span class="w">        </span><span class="c1">// 加载模块</span>
<span class="w">        </span><span class="n">cuModuleLoadDataEx</span><span class="p">(</span><span class="o">&amp;</span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="n">ptx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 获取函数</span>
<span class="w">        </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">kernel</span><span class="p">;</span>
<span class="w">        </span><span class="n">cuModuleGetFunction</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="n">kernelName</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="w">        </span><span class="n">functionCache</span><span class="p">[</span><span class="n">kernelName</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kernel</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">launchKernel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">kernelName</span><span class="p">,</span>
<span class="w">                     </span><span class="n">dim3</span><span class="w"> </span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">dim3</span><span class="w"> </span><span class="n">block</span><span class="p">,</span>
<span class="w">                     </span><span class="n">Args</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functionCache</span><span class="p">[</span><span class="n">kernelName</span><span class="p">];</span>

<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">kernelArgs</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">&amp;</span><span class="n">args</span><span class="p">...</span><span class="w"> </span><span class="p">};</span>

<span class="w">        </span><span class="n">cuLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span>
<span class="w">                      </span><span class="n">grid</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">.</span><span class="n">z</span><span class="p">,</span>
<span class="w">                      </span><span class="n">block</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">z</span><span class="p">,</span>
<span class="w">                      </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">                      </span><span class="n">kernelArgs</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2042">20.4.2 模板元编程与代码生成</h3>
<p>使用模板生成特化的内核代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">KernelGenerator</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">generateGEMMKernel</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                   </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">stringstream</span><span class="w"> </span><span class="n">ss</span><span class="p">;</span>

<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;#define TILE_M &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">TILE_M</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;#define TILE_N &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">TILE_N</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;#define TILE_K &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">TILE_K</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;__global__ void gemm_kernel(</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    const float* __restrict__ A,</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    const float* __restrict__ B,</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    float* __restrict__ C,</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    const int M, const int N, const int K) {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    __shared__ float As[TILE_M][TILE_K];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    __shared__ float Bs[TILE_K][TILE_N];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    int bx = blockIdx.x, by = blockIdx.y;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    int tx = threadIdx.x, ty = threadIdx.y;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    int row = by * TILE_M + ty;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    int col = bx * TILE_N + tx;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    float sum = 0.0f;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 主循环</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    for (int tile = 0; tile &lt; (K + TILE_K - 1) / TILE_K; tile++) {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        // Load tiles</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        if (row &lt; M &amp;&amp; tile * TILE_K + tx &lt; K) {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;            As[ty][tx] = A[row * K + tile * TILE_K + tx];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        } else {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;            As[ty][tx] = 0.0f;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        }</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        if (col &lt; N &amp;&amp; tile * TILE_K + ty &lt; K) {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;            Bs[ty][tx] = B[(tile * TILE_K + ty) * N + col];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        } else {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;            Bs[ty][tx] = 0.0f;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        }</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        __syncthreads();</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 计算</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        #pragma unroll</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        for (int k = 0; k &lt; TILE_K; k++) {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;            sum += As[ty][k] * Bs[k][tx];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        }</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        __syncthreads();</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    }</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 存储结果</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    if (row &lt; M &amp;&amp; col &lt; N) {</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;        C[row * N + col] = sum;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    }</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ss</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;}</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ss</span><span class="p">.</span><span class="n">str</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">generateOptimizedGEMM</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 根据问题规模选择tile size</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">TILE_M</span><span class="p">,</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">,</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">M</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">TILE_M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TILE_N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">            </span><span class="n">TILE_K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">TILE_M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TILE_N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>
<span class="w">            </span><span class="n">TILE_K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">8</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateGEMMKernel</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                               </span><span class="n">TILE_M</span><span class="p">,</span><span class="w"> </span><span class="n">TILE_N</span><span class="p">,</span><span class="w"> </span><span class="n">TILE_K</span><span class="p">);</span>

<span class="w">        </span><span class="n">JITCompiler</span><span class="w"> </span><span class="n">compiler</span><span class="p">;</span>
<span class="w">        </span><span class="n">compiler</span><span class="p">.</span><span class="n">compileKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;gemm_kernel&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                              </span><span class="p">{</span><span class="s">&quot;-arch=sm_80&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;-use_fast_math&quot;</span><span class="p">});</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">compiler</span><span class="p">.</span><span class="n">getFunction</span><span class="p">(</span><span class="s">&quot;gemm_kernel&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2043">20.4.3 缓存策略与持久化</h3>
<p>JIT编译结果的缓存对性能至关重要：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">KernelCache</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">CacheEntry</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">source</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">ptx</span><span class="p">;</span>
<span class="w">        </span><span class="n">CUmodule</span><span class="w"> </span><span class="k">module</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">CUfunction</span><span class="o">&gt;</span><span class="w"> </span><span class="n">functions</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">time_point</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">steady_clock</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lastAccess</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="p">,</span><span class="w"> </span><span class="n">CacheEntry</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cache</span><span class="p">;</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">maxCacheSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span>

<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="nf">computeHash</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span>
<span class="w">                      </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">hash</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hasher</span><span class="p">;</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">hash</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hasher</span><span class="p">(</span><span class="n">source</span><span class="p">);</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">opt</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">hash</span><span class="w"> </span><span class="o">^=</span><span class="w"> </span><span class="n">hasher</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mh">0x9e3779b9</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">hash</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="mi">6</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">hash</span><span class="w"> </span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">hash</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">evictLRU</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cache</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">maxCacheSize</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="n">oldest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cache</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cache</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">cache</span><span class="p">.</span><span class="n">end</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">it</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">it</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">lastAccess</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">oldest</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">lastAccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">oldest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">it</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">cuModuleUnload</span><span class="p">(</span><span class="n">oldest</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="k">module</span><span class="p">);</span>
<span class="w">        </span><span class="n">cache</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">oldest</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">getOrCompile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">kernelName</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">hash</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">computeHash</span><span class="p">(</span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">options</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 检查缓存</span>
<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cache</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">hash</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">it</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">cache</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">it</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">lastAccess</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">steady_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">it</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">.</span><span class="n">functions</span><span class="p">[</span><span class="n">kernelName</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 编译新内核</span>
<span class="w">        </span><span class="n">evictLRU</span><span class="p">();</span>

<span class="w">        </span><span class="n">CacheEntry</span><span class="w"> </span><span class="n">entry</span><span class="p">;</span>
<span class="w">        </span><span class="n">entry</span><span class="p">.</span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">source</span><span class="p">;</span>
<span class="w">        </span><span class="n">entry</span><span class="p">.</span><span class="n">lastAccess</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">steady_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 编译并缓存</span>
<span class="w">        </span><span class="n">compileAndCache</span><span class="p">(</span><span class="n">entry</span><span class="p">,</span><span class="w"> </span><span class="n">kernelName</span><span class="p">,</span><span class="w"> </span><span class="n">options</span><span class="p">);</span>
<span class="w">        </span><span class="n">cache</span><span class="p">[</span><span class="n">hash</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">entry</span><span class="p">;</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">entry</span><span class="p">.</span><span class="n">functions</span><span class="p">[</span><span class="n">kernelName</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">persistCache</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">filename</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">ofstream</span><span class="w"> </span><span class="nf">file</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>

<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">numEntries</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cache</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">        </span><span class="n">file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">numEntries</span><span class="p">),</span><span class="w"> </span>
<span class="w">                  </span><span class="k">sizeof</span><span class="p">(</span><span class="n">numEntries</span><span class="p">));</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="p">[</span><span class="n">hash</span><span class="p">,</span><span class="w"> </span><span class="n">entry</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">cache</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 保存hash</span>
<span class="w">            </span><span class="n">file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">hash</span><span class="p">),</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">hash</span><span class="p">));</span>

<span class="w">            </span><span class="c1">// 保存PTX</span>
<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">ptxSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">entry</span><span class="p">.</span><span class="n">ptx</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">            </span><span class="n">file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptxSize</span><span class="p">),</span><span class="w"> </span>
<span class="w">                      </span><span class="k">sizeof</span><span class="p">(</span><span class="n">ptxSize</span><span class="p">));</span>
<span class="w">            </span><span class="n">file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="n">ptx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">ptxSize</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">loadCache</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">filename</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="w"> </span><span class="nf">file</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">file</span><span class="p">.</span><span class="n">is_open</span><span class="p">())</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">numEntries</span><span class="p">;</span>
<span class="w">        </span><span class="n">file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">numEntries</span><span class="p">),</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">numEntries</span><span class="p">));</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">numEntries</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">hash</span><span class="p">;</span>
<span class="w">            </span><span class="n">file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">hash</span><span class="p">),</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">hash</span><span class="p">));</span>

<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">ptxSize</span><span class="p">;</span>
<span class="w">            </span><span class="n">file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptxSize</span><span class="p">),</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">ptxSize</span><span class="p">));</span>

<span class="w">            </span><span class="n">CacheEntry</span><span class="w"> </span><span class="n">entry</span><span class="p">;</span>
<span class="w">            </span><span class="n">entry</span><span class="p">.</span><span class="n">ptx</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">ptxSize</span><span class="p">);</span>
<span class="w">            </span><span class="n">file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">entry</span><span class="p">.</span><span class="n">ptx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">ptxSize</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 加载模块</span>
<span class="w">            </span><span class="n">cuModuleLoadData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">entry</span><span class="p">.</span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="n">entry</span><span class="p">.</span><span class="n">ptx</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>
<span class="w">            </span><span class="n">cache</span><span class="p">[</span><span class="n">hash</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">entry</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2044-kernel-specialization">20.4.4 动态Kernel Specialization</h3>
<p>根据运行时参数生成特化版本：</p>
<div class="codehilite"><pre><span></span><code><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DynamicKernelSpecializer</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">Specialization</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">name</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">code</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">constants</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">generateSpecializedCode</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Specialization</span><span class="o">&amp;</span><span class="w"> </span><span class="n">spec</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">spec</span><span class="p">.</span><span class="n">code</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 替换常量</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="p">[</span><span class="n">placeholder</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">pos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">            </span><span class="k">while</span><span class="w"> </span><span class="p">((</span><span class="n">pos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">code</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">placeholder</span><span class="p">,</span><span class="w"> </span><span class="n">pos</span><span class="p">))</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">::</span><span class="n">npos</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">code</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span><span class="w"> </span><span class="n">placeholder</span><span class="p">.</span><span class="n">length</span><span class="p">(),</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="w">                </span><span class="n">pos</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">value</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">code</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">specializeForProblem</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">useTensorCore</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">Specialization</span><span class="w"> </span><span class="n">spec</span><span class="p">;</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;specialized_gemm&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 基础模板</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">code</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sa">R</span><span class="s">&quot;</span><span class="dl">(</span>
<span class="s">            extern &quot;C&quot; __global__ void specialized_gemm(</span>
<span class="s">                const TYPE* A, const TYPE* B, TYPE* C) {</span>

<span class="s">                const int M = CONST_M;</span>
<span class="s">                const int N = CONST_N;</span>
<span class="s">                const int K = CONST_K;</span>

<span class="s">                // 特化的内循环</span>
<span class="s">                #if USE_TENSOR_CORE</span>
<span class="s">                    // Tensor Core路径</span>
<span class="s">                    wmma::fragment&lt;...&gt; a_frag, b_frag, c_frag;</span>
<span class="s">                    // ...</span>
<span class="s">                #else</span>
<span class="s">                    // 常规路径</span>
<span class="s">                    TYPE sum = 0;</span>
<span class="s">                    #pragma unroll UNROLL_FACTOR</span>
<span class="s">                    for (int k = 0; k &lt; K; k++) {</span>
<span class="s">                        sum += A[...] * B[...];</span>
<span class="s">                    }</span>
<span class="s">                #endif</span>

<span class="s">                C[...] = sum;</span>
<span class="s">            }</span>
<span class="s">        </span><span class="dl">)</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 设置常量</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">[</span><span class="s">&quot;TYPE&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="s">&quot;float&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="s">&quot;half&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">[</span><span class="s">&quot;CONST_M&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">M</span><span class="p">);</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">[</span><span class="s">&quot;CONST_N&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">[</span><span class="s">&quot;CONST_K&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">K</span><span class="p">);</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">[</span><span class="s">&quot;USE_TENSOR_CORE&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">useTensorCore</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="s">&quot;1&quot;</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="s">&quot;0&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 根据K的大小选择展开因子</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">unrollFactor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">K</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="n">unrollFactor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">;</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">K</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">128</span><span class="p">)</span><span class="w"> </span><span class="n">unrollFactor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">8</span><span class="p">;</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="n">unrollFactor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span>
<span class="w">        </span><span class="n">spec</span><span class="p">.</span><span class="n">constants</span><span class="p">[</span><span class="s">&quot;UNROLL_FACTOR&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">unrollFactor</span><span class="p">);</span>

<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">specializedCode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateSpecializedCode</span><span class="p">(</span><span class="n">spec</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// JIT编译</span>
<span class="w">        </span><span class="n">JITCompiler</span><span class="w"> </span><span class="n">compiler</span><span class="p">;</span>
<span class="w">        </span><span class="n">compiler</span><span class="p">.</span><span class="n">compileKernel</span><span class="p">(</span><span class="n">specializedCode</span><span class="p">,</span><span class="w"> </span><span class="n">spec</span><span class="p">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span>
<span class="w">                              </span><span class="p">{</span><span class="s">&quot;-arch=sm_80&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;-maxrregcount=128&quot;</span><span class="p">});</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">compiler</span><span class="p">.</span><span class="n">getFunction</span><span class="p">(</span><span class="n">spec</span><span class="p">.</span><span class="n">name</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2045-ptx">20.4.5 PTX级优化</h3>
<p>直接生成优化的PTX代码：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PTXGenerator</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">generateOptimizedPTX</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">operation</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">stringstream</span><span class="w"> </span><span class="n">ptx</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// PTX头部</span>
<span class="w">        </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;.version 7.5</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;.target sm_80</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;.address_size 64</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">operation</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;fast_exp&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 快速指数函数实现</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;.visible .entry fast_exp(</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .param .u64 .ptr.global.align 4 input,</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .param .u64 .ptr.global.align 4 output,</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .param .u32 n</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;{</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .reg .f32 %f&lt;4&gt;;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .reg .pred %p&lt;2&gt;;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .reg .b32 %r&lt;8&gt;;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    .reg .b64 %rd&lt;8&gt;;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 获取线程索引</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    mov.u32 %r1, %ctaid.x;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    mov.u32 %r2, %ntid.x;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    mov.u32 %r3, %tid.x;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    mad.lo.s32 %r4, %r1, %r2, %r3;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 边界检查</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    ld.param.u32 %r5, [n];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    setp.ge.u32 %p1, %r4, %r5;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    @%p1 bra EXIT;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 加载数据</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    ld.param.u64 %rd1, [input];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    cvta.to.global.u64 %rd2, %rd1;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    mul.wide.u32 %rd3, %r4, 4;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    add.s64 %rd4, %rd2, %rd3;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    ld.global.f32 %f1, [%rd4];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 快速exp近似（使用硬件特殊函数）</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    ex2.approx.f32 %f2, %f1;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 存储结果</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    ld.param.u64 %rd5, [output];</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    cvta.to.global.u64 %rd6, %rd5;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    add.s64 %rd7, %rd6, %rd3;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    st.global.f32 [%rd7], %f2;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>

<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;EXIT:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;    ret;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">            </span><span class="n">ptx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;}</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ptx</span><span class="p">.</span><span class="n">str</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">compilePTX</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">operation</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">ptxCode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateOptimizedPTX</span><span class="p">(</span><span class="n">operation</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 使用cuModuleLoadData加载PTX</span>
<span class="w">        </span><span class="n">CUmodule</span><span class="w"> </span><span class="k">module</span><span class="p">;</span>
<span class="w">        </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">kernel</span><span class="p">;</span>

<span class="w">        </span><span class="n">cuModuleLoadData</span><span class="p">(</span><span class="o">&amp;</span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="n">ptxCode</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="w">        </span><span class="n">cuModuleGetFunction</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="k">module</span><span class="p">,</span><span class="w"> </span><span class="n">operation</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">kernel</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>

<span class="cp">## 20.5 案例：端到端推理优化</span>

<span class="n">本节将以Transformer模型的推理优化为例</span><span class="err">，</span><span class="n">展示如何综合运用CUDA</span><span class="w"> </span><span class="n">Graph</span><span class="err">、</span><span class="n">内核融合</span><span class="err">、</span><span class="n">JIT编译等技术实现端到端的性能优化</span><span class="err">。</span><span class="n">我们将从一个未优化的baseline实现开始</span><span class="err">，</span><span class="n">逐步应用各种优化技术</span><span class="err">，</span><span class="n">最终实现10x以上的性能提升</span><span class="err">。</span>

<span class="cp">### 20.5.1 Baseline实现分析</span>

<span class="n">首先分析未优化的Transformer推理实现</span><span class="err">：</span>

<span class="err">```</span><span class="n">cpp</span>
<span class="c1">// Baseline实现：逐层执行，无融合</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TransformerBaseline</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="c1">// 各层权重</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">wq</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">wk</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">wv</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">wo</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">w1</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">w2</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">ln1_gamma</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">ln1_beta</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">ln2_gamma</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">ln2_beta</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">        </span><span class="c1">// 临时缓冲区</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">attn_scores</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">attn_output</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">ffn_hidden</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">residual</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// Layer Norm 1</span>
<span class="w">        </span><span class="n">layerNorm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">ln1_gamma</span><span class="p">,</span><span class="w"> </span><span class="n">ln1_beta</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                   </span><span class="n">residual</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Multi-Head Attention</span>
<span class="w">        </span><span class="c1">// QKV投影（3个独立内核）</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span><span class="w"> </span><span class="n">wq</span><span class="p">,</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span>
<span class="w">                             </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span><span class="w"> </span><span class="n">wk</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span>
<span class="w">                             </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span><span class="w"> </span><span class="n">wv</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span>
<span class="w">                             </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Attention计算</span>
<span class="w">        </span><span class="n">computeAttention</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">attn_scores</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                         </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Output投影</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span><span class="w"> </span><span class="n">wo</span><span class="p">,</span><span class="w"> </span><span class="n">attn_output</span><span class="p">,</span><span class="w"> </span>
<span class="w">                             </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Residual连接</span>
<span class="w">        </span><span class="n">elementwiseAdd</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">attn_output</span><span class="p">,</span><span class="w"> </span><span class="n">residual</span><span class="p">,</span>
<span class="w">                                       </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Layer Norm 2</span>
<span class="w">        </span><span class="n">layerNorm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span><span class="w"> </span><span class="n">ln2_gamma</span><span class="p">,</span><span class="w"> </span><span class="n">ln2_beta</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                   </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// FFN</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">w1</span><span class="p">,</span><span class="w"> </span><span class="n">ffn_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span>
<span class="w">                             </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// GELU激活</span>
<span class="w">        </span><span class="n">gelu</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">ffn_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">ffn_hidden</span><span class="p">,</span><span class="w"> </span>
<span class="w">                             </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// FFN输出</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">ffn_hidden</span><span class="p">,</span><span class="w"> </span><span class="n">w2</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span>
<span class="w">                             </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 最终residual</span>
<span class="w">        </span><span class="n">elementwiseAdd</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">residual</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">                                       </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">// 性能分析结果：</span>
<span class="c1">// - 12个独立的内核启动</span>
<span class="c1">// - 大量中间结果写入全局内存</span>
<span class="c1">// - 无内核间并行</span>
<span class="c1">// - 小batch时GPU利用率低</span>
</code></pre></div>

<h3 id="2052-graph">20.5.2 Graph优化实现</h3>
<p>使用CUDA Graph减少内核启动开销：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TransformerGraphOptimized</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="n">cudaGraph_t</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaGraphExec_t</span><span class="w"> </span><span class="n">graphExec</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 预分配的缓冲区</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">BufferPool</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">v</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">attn_scores</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">attn_output</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">ffn_hidden</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">residual</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span><span class="w">  </span><span class="c1">// 双缓冲</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">current_residual</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="kt">void</span><span class="w"> </span><span class="nf">swap_residual</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">current_residual</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">current_residual</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="n">buffers</span><span class="p">;</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">buildGraph</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 开始捕获</span>
<span class="w">        </span><span class="n">cudaStreamBeginCapture</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStreamCaptureModeGlobal</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 使用多流并行</span>
<span class="w">        </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
<span class="w">        </span><span class="n">cudaEvent_t</span><span class="w"> </span><span class="n">event_qkv</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream_qkv</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">            </span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">event_qkv</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// Layer Norm 1</span>
<span class="w">        </span><span class="n">layerNorm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">ln1_gamma</span><span class="p">,</span><span class="w"> </span><span class="n">ln1_beta</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">residual</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="w">            </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 并行QKV投影</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event_start</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>

<span class="w">        </span><span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">event_start</span><span class="p">);</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">buffers</span><span class="p">.</span><span class="n">residual</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">wq</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">q</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event_qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>

<span class="w">        </span><span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">event_start</span><span class="p">);</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">buffers</span><span class="p">.</span><span class="n">residual</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">wk</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">k</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event_qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>

<span class="w">        </span><span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="w"> </span><span class="n">event_start</span><span class="p">);</span>
<span class="w">        </span><span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">buffers</span><span class="p">.</span><span class="n">residual</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">wv</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">v</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event_qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="w"> </span><span class="n">stream_qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>

<span class="w">        </span><span class="c1">// 等待QKV完成</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">event_qkv</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// Attention计算</span>
<span class="w">        </span><span class="n">computeAttention</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">buffers</span><span class="p">.</span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">attn_scores</span><span class="p">,</span>
<span class="w">            </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 继续执行...</span>

<span class="w">        </span><span class="c1">// 结束捕获</span>
<span class="w">        </span><span class="n">cudaStreamEndCapture</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 实例化</span>
<span class="w">        </span><span class="n">cudaGraphInstantiate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 直接执行预构建的图</span>
<span class="w">        </span><span class="n">cudaGraphLaunch</span><span class="p">(</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">// 优化效果：</span>
<span class="c1">// - 单次API调用执行整个网络</span>
<span class="c1">// - QKV投影并行执行</span>
<span class="c1">// - 消除CPU-GPU同步开销</span>
<span class="c1">// - 约2x加速</span>
</code></pre></div>

<h3 id="2053">20.5.3 内核融合优化</h3>
<p>融合多个操作减少内存访问：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 融合的Attention内核</span>
<span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">NUM_HEADS</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fusedMultiHeadAttention</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w">      </span><span class="c1">// [batch, seq_len, hidden_dim]</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">wqkv</span><span class="p">,</span><span class="w">       </span><span class="c1">// [hidden_dim, 3 * hidden_dim]</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">wo</span><span class="p">,</span><span class="w">         </span><span class="c1">// [hidden_dim, hidden_dim]</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w">     </span><span class="c1">// [batch, seq_len, hidden_dim]</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">ln_gamma</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">ln_beta</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">NUM_HEADS</span><span class="p">;</span>
<span class="w">    </span><span class="k">extern</span><span class="w"> </span><span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">[];</span>

<span class="w">    </span><span class="c1">// 共享内存布局</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">q_smem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shared_mem</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">k_smem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">q_smem</span><span class="p">[</span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">v_smem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">k_smem</span><span class="p">[</span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">];</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">attn_smem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">v_smem</span><span class="p">[</span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="p">];</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">head_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">seq_idx</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">seq_len</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Step 1: LayerNorm + QKV投影（融合）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">norm_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">hidden_dim</span><span class="p">);</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 计算LayerNorm统计量</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">batch_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">                         </span><span class="n">seq_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="n">mean</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">val</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">mean</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">batch_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">                         </span><span class="n">seq_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="n">var</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="p">(</span><span class="n">val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">var</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1e-6f</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 融合LayerNorm和QKV投影</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">q_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">k_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">v_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">input</span><span class="p">[...]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">var</span><span class="p">;</span>
<span class="w">        </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ln_gamma</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ln_beta</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

<span class="w">        </span><span class="c1">// 直接投影到QKV</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">qkv_offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">head_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span>
<span class="w">        </span><span class="n">q_val</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">wqkv</span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">qkv_offset</span><span class="p">];</span>
<span class="w">        </span><span class="n">k_val</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">wqkv</span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">qkv_offset</span><span class="p">];</span>
<span class="w">        </span><span class="n">v_val</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">wqkv</span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">qkv_offset</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 存储到共享内存</span>
<span class="w">    </span><span class="n">q_smem</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">q_val</span><span class="p">;</span>
<span class="w">    </span><span class="n">k_smem</span><span class="p">[</span><span class="n">seq_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k_val</span><span class="p">;</span>
<span class="w">    </span><span class="n">v_smem</span><span class="p">[</span><span class="n">seq_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">v_val</span><span class="p">;</span>

<span class="w">    </span><span class="n">__syncthreads</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Step 2: Attention计算（融合softmax）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">attn_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">max_score</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Online softmax（单pass）</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">score</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">q_smem</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span>
<span class="w">                    </span><span class="n">k_smem</span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">score</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="n">norm_scale</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// Online softmax更新</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">old_max</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max_score</span><span class="p">;</span>
<span class="w">        </span><span class="n">max_score</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaxf</span><span class="p">(</span><span class="n">max_score</span><span class="p">,</span><span class="w"> </span><span class="n">score</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">exp_score</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">score</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_score</span><span class="p">);</span>

<span class="w">        </span><span class="n">attn_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">attn_sum</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">expf</span><span class="p">(</span><span class="n">old_max</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">max_score</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exp_score</span><span class="p">;</span>
<span class="w">        </span><span class="n">attn_smem</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exp_score</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Step 3: 加权求和V + 输出投影（融合）</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">out_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">attn_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">attn_smem</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">attn_sum</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">out_val</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">attn_weight</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">v_smem</span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 输出投影和residual连接（融合）</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">out_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">                 </span><span class="n">seq_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">head_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">HEAD_DIM</span><span class="p">;</span><span class="w"> </span><span class="n">d</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">proj_val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">NUM_HEADS</span><span class="p">;</span><span class="w"> </span><span class="n">h</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">proj_val</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">out_val</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">wo</span><span class="p">[...];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// Residual连接</span>
<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">out_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">out_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">proj_val</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 融合的FFN层</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">fusedFFN</span><span class="p">(</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">w1</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">w2</span><span class="p">,</span>
<span class="w">    </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">ln_gamma</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">ln_beta</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_seq_len</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">batch_seq_len</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 融合LayerNorm + FFN + GELU + 投影</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// LayerNorm</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">mean</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">mean</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="n">var</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="p">(</span><span class="n">val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">val</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sqrtf</span><span class="p">(</span><span class="n">var</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1e-6f</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// FFN with融合GELU</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">out_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">out_i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">out_i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">in_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">in_i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">in_i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">float</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">input</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">in_i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">var</span><span class="p">;</span>
<span class="w">            </span><span class="n">normalized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ln_gamma</span><span class="p">[</span><span class="n">in_i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ln_beta</span><span class="p">[</span><span class="n">in_i</span><span class="p">];</span>
<span class="w">            </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">normalized</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">w1</span><span class="p">[</span><span class="n">in_i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">out_i</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// GELU激活（融合）</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">gelu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5f</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tanhf</span><span class="p">(</span><span class="mf">0.7978845608f</span><span class="w"> </span><span class="o">*</span><span class="w"> </span>
<span class="w">                    </span><span class="p">(</span><span class="n">sum</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.044715f</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sum</span><span class="p">)));</span>

<span class="w">        </span><span class="c1">// 第二层投影（融合）</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">final_i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">final_i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">;</span><span class="w"> </span><span class="n">final_i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">final_i</span><span class="p">],</span>
<span class="w">                     </span><span class="n">gelu</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">w2</span><span class="p">[</span><span class="n">out_i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">final_i</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 优化效果：</span>
<span class="c1">// - 减少70%的内存访问</span>
<span class="c1">// - 消除中间结果存储</span>
<span class="c1">// - 约5x加速</span>
</code></pre></div>

<h3 id="2054">20.5.4 动态批处理优化</h3>
<p>处理可变长度输入的优化：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DynamicBatchingOptimizer</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">Request</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">request_id</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">queue</span><span class="o">&lt;</span><span class="n">Request</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pending_requests</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="w"> </span><span class="n">queue_mutex</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Padding策略</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="nf">getPaddedLength</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 向上取整到32的倍数（warp对齐）</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="p">((</span><span class="n">seq_len</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">31</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">processBatch</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Request</span><span class="o">&gt;</span><span class="w"> </span><span class="n">batch</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">total_tokens</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">max_seq_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 收集请求形成批次</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lock</span><span class="p">(</span><span class="n">queue_mutex</span><span class="p">);</span>

<span class="w">            </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">pending_requests</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">Request</span><span class="w"> </span><span class="n">req</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pending_requests</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>

<span class="w">                </span><span class="c1">// 检查是否超过token预算</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">total_tokens</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">req</span><span class="p">.</span><span class="n">seq_len</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">4096</span><span class="p">)</span><span class="w"> </span><span class="k">break</span><span class="p">;</span>

<span class="w">                </span><span class="n">pending_requests</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
<span class="w">                </span><span class="n">batch</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">req</span><span class="p">);</span>
<span class="w">                </span><span class="n">total_tokens</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">req</span><span class="p">.</span><span class="n">seq_len</span><span class="p">;</span>
<span class="w">                </span><span class="n">max_seq_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">req</span><span class="p">.</span><span class="n">seq_len</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 动态选择执行策略</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 单请求：使用专门优化的小batch内核</span>
<span class="w">            </span><span class="n">executeSingleRequest</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">max_seq_len</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">128</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 短序列：使用融合内核</span>
<span class="w">            </span><span class="n">executeShortSequenceBatch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">max_seq_len</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 长序列：使用分块处理</span>
<span class="w">            </span><span class="n">executeLongSequenceBatch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">max_seq_len</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">executeShortSequenceBatch</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Request</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                  </span><span class="kt">int</span><span class="w"> </span><span class="n">max_seq_len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">padded_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getPaddedLength</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">);</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 分配连续内存</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">batch_input</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">batch_output</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">batch_input</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">        </span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">batch_output</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

<span class="w">        </span><span class="c1">// 打包输入（with padding）</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span><span class="w"> </span><span class="n">b</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaMemcpy2D</span><span class="p">(</span><span class="n">batch_input</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span>
<span class="w">                        </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                        </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">input</span><span class="p">,</span>
<span class="w">                        </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                        </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                        </span><span class="n">hidden_dim</span><span class="p">,</span>
<span class="w">                        </span><span class="n">cudaMemcpyDeviceToDevice</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// Padding位置设置为0</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">padded_len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">cudaMemset</span><span class="p">(</span><span class="n">batch_input</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">                          </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span>
<span class="w">                          </span><span class="mi">0</span><span class="p">,</span>
<span class="w">                          </span><span class="p">(</span><span class="n">padded_len</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 执行融合内核</span>
<span class="w">        </span><span class="n">dim3</span><span class="w"> </span><span class="n">grid</span><span class="p">(</span><span class="n">padded_len</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">);</span>
<span class="w">        </span><span class="n">dim3</span><span class="w"> </span><span class="nf">block</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">);</span><span class="w">  </span><span class="c1">// warp per sequence position</span>

<span class="w">        </span><span class="n">fusedTransformerKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">batch_input</span><span class="p">,</span><span class="w"> </span><span class="n">batch_output</span><span class="p">,</span>
<span class="w">            </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="n">padded_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 解包输出</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span><span class="w"> </span><span class="n">b</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaMemcpy2D</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">output</span><span class="p">,</span>
<span class="w">                        </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                        </span><span class="n">batch_output</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span>
<span class="w">                        </span><span class="n">padded_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                        </span><span class="n">batch</span><span class="p">[</span><span class="n">b</span><span class="p">].</span><span class="n">seq_len</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                        </span><span class="n">hidden_dim</span><span class="p">,</span>
<span class="w">                        </span><span class="n">cudaMemcpyDeviceToDevice</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">batch_input</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaFree</span><span class="p">(</span><span class="n">batch_output</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">// 优化效果：</span>
<span class="c1">// - 提高GPU利用率</span>
<span class="c1">// - 减少内存碎片</span>
<span class="c1">// - 约3x加速（相比逐个处理）</span>
</code></pre></div>

<h3 id="2055">20.5.5 延迟隐藏技术</h3>
<p>通过流水线化隐藏延迟：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PipelinedTransformer</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="w">    </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">NUM_STAGES</span><span class="p">];</span>
<span class="w">    </span><span class="n">cudaEvent_t</span><span class="w"> </span><span class="n">events</span><span class="p">[</span><span class="n">NUM_STAGES</span><span class="p">];</span>

<span class="w">    </span><span class="c1">// 三级流水线缓冲</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">PipelineBuffer</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">NUM_STAGES</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">current_stage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="nf">get_current</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">current_stage</span><span class="p">];</span><span class="w"> </span><span class="p">}</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="nf">get_next</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">data</span><span class="p">[(</span><span class="n">current_stage</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">];</span><span class="w"> </span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="kt">void</span><span class="w"> </span><span class="nf">advance</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span>
<span class="w">            </span><span class="n">current_stage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">current_stage</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">;</span><span class="w"> </span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="n">buffers</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">pipelinedForward</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">                         </span><span class="kt">int</span><span class="w"> </span><span class="n">num_layers</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">        </span><span class="c1">// 初始化流水线</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">            </span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">events</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 预热流水线</span>
<span class="w">        </span><span class="c1">// Stage 0: 第1层的前半部分</span>
<span class="w">        </span><span class="n">layerForwardPart1</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">            </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">events</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>

<span class="w">        </span><span class="c1">// 主循环：流水线执行</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_layers</span><span class="p">;</span><span class="w"> </span><span class="n">layer</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">stage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">;</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">prev_stage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">stage</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">;</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">next_stage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">stage</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 等待前一阶段</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">layer</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">stage</span><span class="p">],</span><span class="w"> </span><span class="n">events</span><span class="p">[</span><span class="n">prev_stage</span><span class="p">]);</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="c1">// 执行当前层</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">layer</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num_layers</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// 并行执行：当前层后半部分 + 下一层前半部分</span>

<span class="w">                </span><span class="c1">// 当前层后半部分</span>
<span class="w">                </span><span class="n">layerForwardPart2</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">stage</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">                    </span><span class="n">buffers</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">prev_stage</span><span class="p">],</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">stage</span><span class="p">],</span><span class="w"> </span><span class="n">layer</span><span class="p">);</span>

<span class="w">                </span><span class="c1">// 下一层前半部分（在不同流上）</span>
<span class="w">                </span><span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">next_stage</span><span class="p">],</span><span class="w"> </span><span class="n">events</span><span class="p">[</span><span class="n">stage</span><span class="p">]);</span>
<span class="w">                </span><span class="n">layerForwardPart1</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">next_stage</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">                    </span><span class="n">buffers</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">stage</span><span class="p">],</span><span class="w"> </span><span class="n">buffers</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">next_stage</span><span class="p">],</span><span class="w"> </span><span class="n">layer</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">                </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">events</span><span class="p">[</span><span class="n">next_stage</span><span class="p">],</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">next_stage</span><span class="p">]);</span>
<span class="w">            </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// 最后一层</span>
<span class="w">                </span><span class="n">layerForwardPart2</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">stage</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">                    </span><span class="n">buffers</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">prev_stage</span><span class="p">],</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">layer</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">events</span><span class="p">[</span><span class="n">stage</span><span class="p">],</span><span class="w"> </span><span class="n">streams</span><span class="p">[</span><span class="n">stage</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 同步所有流</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">NUM_STAGES</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">// 优化效果：</span>
<span class="c1">// - 隐藏内存传输延迟</span>
<span class="c1">// - 提高计算/内存重叠</span>
<span class="c1">// - 约1.5x额外加速</span>
</code></pre></div>

<h3 id="2056">20.5.6 性能对比与分析</h3>
<p>综合所有优化技术的最终实现：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">OptimizedTransformerInference</span><span class="w"> </span><span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
<span class="w">    </span><span class="c1">// 组合所有优化技术</span>
<span class="w">    </span><span class="n">TransformerGraphOptimized</span><span class="w"> </span><span class="n">graph_optimizer</span><span class="p">;</span>
<span class="w">    </span><span class="n">DynamicBatchingOptimizer</span><span class="w"> </span><span class="n">batch_optimizer</span><span class="p">;</span>
<span class="w">    </span><span class="n">PipelinedTransformer</span><span class="w"> </span><span class="n">pipeline_optimizer</span><span class="p">;</span>
<span class="w">    </span><span class="n">JITCompiler</span><span class="w"> </span><span class="n">jit_compiler</span><span class="p">;</span>
<span class="w">    </span><span class="n">KernelCache</span><span class="w"> </span><span class="n">kernel_cache</span><span class="p">;</span>

<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">PerformanceMetrics</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">baseline_time</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">graph_time</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">fusion_time</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">batching_time</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">pipeline_time</span><span class="p">;</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">combined_time</span><span class="p">;</span>

<span class="w">        </span><span class="kt">void</span><span class="w"> </span><span class="nf">print</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Performance Analysis:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Baseline:        %.2f ms</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">baseline_time</span><span class="p">);</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;With Graph:      %.2f ms (%.2fx)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="n">graph_time</span><span class="p">,</span><span class="w"> </span><span class="n">baseline_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">graph_time</span><span class="p">);</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;With Fusion:     %.2f ms (%.2fx)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="n">fusion_time</span><span class="p">,</span><span class="w"> </span><span class="n">baseline_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">fusion_time</span><span class="p">);</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;With Batching:   %.2f ms (%.2fx)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="n">batching_time</span><span class="p">,</span><span class="w"> </span><span class="n">baseline_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">batching_time</span><span class="p">);</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;With Pipeline:   %.2f ms (%.2fx)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="n">pipeline_time</span><span class="p">,</span><span class="w"> </span><span class="n">baseline_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">pipeline_time</span><span class="p">);</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Combined:        %.2f ms (%.2fx)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="n">combined_time</span><span class="p">,</span><span class="w"> </span><span class="n">baseline_time</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">combined_time</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="n">metrics</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">num_layers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 测试各种优化组合</span>
<span class="w">        </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="p">;</span>
<span class="w">        </span><span class="n">allocateBuffers</span><span class="p">(</span><span class="o">&amp;</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// Baseline</span>
<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">baseline_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">timeKernel</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">baselineForward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">num_layers</span><span class="p">);</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="c1">// Graph优化</span>
<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">graph_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">timeKernel</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">graph_optimizer</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="c1">// 融合优化</span>
<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">fusion_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">timeKernel</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">fusedForward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">num_layers</span><span class="p">);</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="c1">// 批处理优化</span>
<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">batching_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">timeKernel</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">batch_optimizer</span><span class="p">.</span><span class="n">processBatch</span><span class="p">();</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="c1">// 流水线优化</span>
<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">pipeline_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">timeKernel</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">pipeline_optimizer</span><span class="p">.</span><span class="n">pipelinedForward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                               </span><span class="n">num_layers</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="c1">// 组合所有优化</span>
<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">combined_time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">timeKernel</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">combinedOptimizedForward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">num_layers</span><span class="p">);</span>
<span class="w">        </span><span class="p">});</span>

<span class="w">        </span><span class="n">metrics</span><span class="p">.</span><span class="n">print</span><span class="p">();</span>

<span class="w">        </span><span class="c1">// 详细性能分析</span>
<span class="w">        </span><span class="n">profileWithNsight</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">combinedOptimizedForward</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">                                 </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">num_layers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 根据输入特征选择最优策略</span>
<span class="w">        </span><span class="n">OptimizationStrategy</span><span class="w"> </span><span class="n">strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">selectStrategy</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strategy</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">SMALL_BATCH</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 小batch：最大化融合</span>
<span class="w">            </span><span class="n">CUfunction</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jit_compiler</span><span class="p">.</span><span class="n">specializeForProblem</span><span class="p">(</span>
<span class="w">                </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>

<span class="w">            </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">args</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="o">&amp;</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">weights</span><span class="p">};</span>
<span class="w">            </span><span class="n">cuLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">.</span><span class="n">z</span><span class="p">,</span>
<span class="w">                          </span><span class="n">block</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">z</span><span class="p">,</span>
<span class="w">                          </span><span class="n">shared_mem</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span>

<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strategy</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">LARGE_BATCH</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 大batch：使用Tensor Core + Graph</span>
<span class="w">            </span><span class="n">buildTensorCoreGraph</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>
<span class="w">            </span><span class="n">cudaGraphLaunch</span><span class="p">(</span><span class="n">tc_graph_exec</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">strategy</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">LONG_SEQUENCE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 长序列：分块处理 + 流水线</span>
<span class="w">            </span><span class="n">pipeline_optimizer</span><span class="p">.</span><span class="n">pipelinedForward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">                                               </span><span class="n">num_layers</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">seq_len</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">profileWithNsight</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 使用CUPTI API收集详细性能数据</span>
<span class="w">        </span><span class="n">CUpti_ProfilerRange</span><span class="w"> </span><span class="n">profiler</span><span class="p">(</span><span class="s">&quot;TransformerInference&quot;</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 内存带宽分析</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">achieved_bandwidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">profiler</span><span class="p">.</span><span class="n">getMetric</span><span class="p">(</span><span class="s">&quot;dram_throughput&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">peak_bandwidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">900.0f</span><span class="p">;</span><span class="w">  </span><span class="c1">// GB/s for A100</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Memory Bandwidth Utilization: %.1f%%</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">               </span><span class="n">achieved_bandwidth</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">peak_bandwidth</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 计算吞吐量分析</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">achieved_tflops</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">profiler</span><span class="p">.</span><span class="n">getMetric</span><span class="p">(</span><span class="s">&quot;flop_sp_efficiency&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">peak_tflops</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">19.5f</span><span class="p">;</span><span class="w">  </span><span class="c1">// TFLOPs for A100</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Compute Utilization: %.1f%%</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">               </span><span class="n">achieved_tflops</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">peak_tflops</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 占用率分析</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">occupancy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">profiler</span><span class="p">.</span><span class="n">getMetric</span><span class="p">(</span><span class="s">&quot;achieved_occupancy&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Achieved Occupancy: %.1f%%</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">occupancy</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 内核效率分析</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">sm_efficiency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">profiler</span><span class="p">.</span><span class="n">getMetric</span><span class="p">(</span><span class="s">&quot;sm_efficiency&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;SM Efficiency: %.1f%%</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">sm_efficiency</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>

<span class="c1">// 最终优化结果示例：</span>
<span class="c1">// Batch=1, Seq=512, Layers=12:</span>
<span class="c1">//   Baseline:      45.3 ms</span>
<span class="c1">//   Optimized:     3.8 ms (11.9x speedup)</span>
<span class="c1">//</span>
<span class="c1">// Batch=32, Seq=128, Layers=12:</span>
<span class="c1">//   Baseline:      285.6 ms</span>
<span class="c1">//   Optimized:     24.2 ms (11.8x speedup)</span>
</code></pre></div>

<h2 id="206">20.6 本章小结</h2>
<p>本章深入探讨了CUDA Graph和内核融合这两项关键的系统级优化技术，并结合JIT编译和自动调优框架，展示了如何构建高性能的端到端推理系统。</p>
<h3 id="_1">核心要点回顾</h3>
<ol>
<li>
<p><strong>CUDA Graph技术</strong>
   - Graph通过消除CPU-GPU同步开销，实现了执行流的极致优化
   - 流捕获机制提供了便捷的Graph构建方式，而手动构建提供了最大灵活性
   - Graph更新和条件执行支持动态工作负载
   - 性能提升关键在于最大化并行度和最小化同步点</p>
</li>
<li>
<p><strong>内核融合策略</strong>
   - 垂直融合将producer-consumer关系的内核合并，减少中间结果存储
   - 水平融合提高硬件利用率，特别适合批处理场景
   - Element-wise操作融合能显著减少内存带宽压力
   - Reduction融合需要特殊的算法设计，如online softmax</p>
</li>
<li>
<p><strong>自动调优框架</strong>
   - Profile-guided optimization基于实际性能数据进行优化
   - 搜索空间的有效定义和剪枝是快速收敛的关键
   - 贝叶斯优化通过建立性能模型指导搜索
   - 遗传算法适合处理离散的大规模搜索空间</p>
</li>
<li>
<p><strong>JIT编译优化</strong>
   - NVRTC运行时编译实现了kernel的动态特化
   - 模板元编程结合代码生成提供了灵活性
   - 缓存策略对JIT性能至关重要
   - PTX级优化能够实现极致性能</p>
</li>
<li>
<p><strong>端到端优化实践</strong>
   - 不同优化技术的组合使用能够产生叠加效应
   - 动态批处理提高了系统的吞吐量
   - 流水线技术有效隐藏了延迟
   - 综合优化可以实现10x以上的性能提升</p>
</li>
</ol>
<h3 id="_2">关键公式与度量</h3>
<ol>
<li><strong>Graph执行效率</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>Efficiency = (Kernel_Execution_Time) / (Total_Graph_Time)
Graph_Overhead = Total_Graph_Time - Kernel_Execution_Time
</code></pre></div>

<ol start="2">
<li><strong>融合收益评估</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>Memory_Reduction = 1 - (Fused_Memory_Access / Original_Memory_Access)
Speedup = Original_Time / Fused_Time
</code></pre></div>

<ol start="3">
<li><strong>自动调优收敛率</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>Convergence_Rate = (Best_Performance - Current_Performance) / Iterations
</code></pre></div>

<ol start="4">
<li><strong>JIT编译投资回报</strong>：</li>
</ol>
<div class="codehilite"><pre><span></span><code>ROI = (Specialized_Kernel_Speedup × Reuse_Count - Compilation_Time) / Compilation_Time
</code></pre></div>

<h3 id="_3">性能优化决策树</h3>
<div class="codehilite"><pre><span></span><code>输入特征分析
    ├─ Batch Size
    │   ├─ 小 (&lt; 4) → 最大化融合 + JIT特化
    │   └─ 大 (≥ 4) → Graph + Tensor Core
    ├─ Sequence Length
    │   ├─ 短 (&lt; 128) → 完全融合
    │   └─ 长 (≥ 128) → 分块 + 流水线
    └─ 计算密度
        ├─ Memory-bound → 融合优先
        └─ Compute-bound → 并行优先
</code></pre></div>

<h2 id="207">20.7 练习题</h2>
<h3 id="_4">基础题</h3>
<p><strong>练习1：Graph基本操作</strong>
构建一个CUDA Graph，包含3个串行的矩阵乘法操作，测量相比独立kernel调用的性能提升。</p>
<details>
<summary>提示</summary>
<p>使用流捕获机制记录操作序列，注意预分配所有需要的缓冲区。比较Graph执行和独立kernel调用的总时间。</p>
</details>
<details>
<summary>答案</summary>
<p>使用cudaStreamBeginCapture开始捕获，依次记录3个GEMM kernel，然后cudaStreamEndCapture结束捕获。实例化Graph后，通过cudaGraphLaunch执行。典型情况下，Graph执行能够减少50-70%的启动开销，特别是对于小矩阵。关键在于消除了kernel间的CPU-GPU同步。</p>
</details>
<p><strong>练习2：简单内核融合</strong>
将一个element-wise加法操作和ReLU激活函数融合成单个内核，分析内存带宽的节省。</p>
<details>
<summary>提示</summary>
<p>原始版本需要3次内存访问（读A、读B、写C用于加法，读C、写D用于ReLU）。融合版本只需要2次（读A和B，写最终结果）。</p>
</details>
<details>
<summary>答案</summary>
<p>融合内核直接计算<code>output[i] = max(0.0f, a[i] + b[i])</code>，减少了50%的内存访问。对于memory-bound的操作，这直接转化为约2x的性能提升。实际加速比取决于缓存命中率和内存带宽利用率。</p>
</details>
<p><strong>练习3：搜索空间定义</strong>
为一个矩阵转置kernel定义合理的调优搜索空间，包括block size和tile size的候选值。</p>
<details>
<summary>提示</summary>
<p>考虑硬件约束（最大线程数、共享内存大小）和性能因素（bank conflict、占用率）。</p>
</details>
<details>
<summary>答案</summary>
<p>Block size候选：{16×16, 32×8, 8×32}以保持256线程。Tile size候选：{16×16, 32×32}考虑共享内存限制。对于避免bank conflict，tile宽度应该是33而不是32。搜索空间大小：3×2=6种配置，可以快速遍历。</p>
</details>
<p><strong>练习4：JIT编译场景识别</strong>
列举3个适合使用JIT编译的场景，并说明原因。</p>
<details>
<summary>提示</summary>
<p>考虑哪些情况下运行时信息能够带来显著的优化机会。</p>
</details>
<details>
<summary>答案</summary>
<ol>
<li>动态形状的矩阵运算：可以将维度作为编译时常量，启用循环展开</li>
<li>稀疏模式已知的运算：根据稀疏结构生成特化代码</li>
<li>用户自定义的激活函数：避免函数指针调用开销
每种场景都能通过特化获得20-50%的性能提升。</li>
</ol>
</details>
<h3 id="_5">挑战题</h3>
<p><strong>练习5：复杂Graph优化</strong>
设计一个包含条件分支的CUDA Graph，实现动态选择不同精度（FP32/FP16）的计算路径。测量不同精度下的性能差异。</p>
<details>
<summary>提示</summary>
<p>使用CUDA 12.3的条件节点功能，或者预构建两个子图，运行时选择执行。需要考虑精度转换的开销。</p>
</details>
<details>
<summary>答案</summary>
<p>创建两个子图，一个用于FP32路径，一个用于FP16路径。使用条件节点根据输入数据的特征（如数值范围）动态选择。FP16路径通常能提供2x的内存带宽优势和Tensor Core加速，但需要处理溢出和精度损失。关键是在Graph构建时就确定好所有可能的执行路径，避免运行时重构。</p>
</details>
<p><strong>练习6：多层融合策略</strong>
设计一个融合策略，将LayerNorm、GEMM和激活函数三个操作融合，并处理好数值稳定性问题。</p>
<details>
<summary>提示</summary>
<p>LayerNorm需要两次遍历数据（计算统计量和归一化），考虑如何与GEMM的计算模式结合。注意在线算法的使用。</p>
</details>
<details>
<summary>答案</summary>
<p>使用分块策略：每个block负责输出的一部分行。先用一个warp计算LayerNorm统计量（使用Welford算法保证数值稳定），同步后所有线程并行执行归一化和GEMM。关键优化：1)使用共享内存缓存归一化后的数据；2)将GEMM的累加与激活函数融合；3)使用向量化load/store。这种融合能减少75%的全局内存访问。</p>
</details>
<p><strong>练习7：自适应调优系统</strong>
实现一个自适应的调优系统，能够根据硬件特性和问题规模自动选择最优的kernel配置。系统应该包含离线训练和在线预测两个阶段。</p>
<details>
<summary>提示</summary>
<p>使用机器学习方法建立性能模型。特征包括：问题规模、硬件规格、内存访问模式。可以使用简单的决策树或神经网络。</p>
</details>
<details>
<summary>答案</summary>
<p>离线阶段：收集不同配置下的性能数据，提取特征（矩阵维度、算术强度、硬件SM数等），训练一个轻量级MLP预测最优配置。在线阶段：对新问题提取特征，模型预测top-3配置，快速评估后选择最优。关键是特征工程：包括计算密度、内存访问模式、数据复用度等。这种方法相比暴力搜索能减少90%的调优时间。</p>
</details>
<p><strong>练习8：端到端优化实战</strong>
给定一个包含Conv-BN-ReLU-Pool的CNN层，设计并实现一个完整的优化方案，包括Graph构建、内核融合和自动调优，目标是达到cuDNN 80%的性能。</p>
<details>
<summary>提示</summary>
<p>分析数据流和复用机会。Conv和BN可以融合，ReLU可以与BN输出阶段融合，Pool可能需要单独处理。使用im2col或implicit GEMM方法。</p>
</details>
<details>
<summary>答案</summary>
<p>优化方案：</p>
<ol>
<li>使用implicit GEMM实现Conv，避免im2col的内存开销</li>
<li>将BN的统计量预计算并融入Conv的epilogue</li>
<li>ReLU直接在写出时应用</li>
<li>Pool使用单独的kernel但通过Graph串联，避免同步</li>
<li>使用自动调优找到最优的tile size和block配置</li>
<li>对于常见的层配置，使用JIT生成特化代码</li>
</ol>
<p>关键实现细节：</p>
<ul>
<li>Conv使用1x1、3x3、5x5等专门优化的模板</li>
<li>BN参数提前fuse成scale和bias</li>
<li>使用Tensor Core（如果适用）</li>
<li>内存布局优化（NCHW vs NHWC）</li>
</ul>
<p>通过这些优化，可以达到cuDNN 75-85%的性能，差距主要在于cuDNN使用了更多硬件特定的优化和汇编级调优。</p>
</details>
<h2 id="208">20.8 常见陷阱与错误</h2>
<h3 id="graph">Graph相关陷阱</h3>
<ol>
<li><strong>Graph更新失败</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：假设更新总是成功</span>
<span class="n">cudaGraphExecKernelNodeSetParams</span><span class="p">(</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">node</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">params</span><span class="p">);</span>

<span class="c1">// 正确：检查更新结果</span>
<span class="n">cudaGraphExecUpdateResult</span><span class="w"> </span><span class="n">updateResult</span><span class="p">;</span>
<span class="n">cudaGraphExecUpdate</span><span class="p">(</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">updateResult</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">updateResult</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">cudaGraphExecUpdateSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 需要重新实例化</span>
<span class="w">    </span><span class="n">cudaGraphExecDestroy</span><span class="p">(</span><span class="n">graphExec</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaGraphInstantiate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">graphExec</span><span class="p">,</span><span class="w"> </span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<ol start="2">
<li><strong>流捕获泄漏</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：捕获期间使用了默认流</span>
<span class="n">cudaStreamBeginCapture</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStreamCaptureModeGlobal</span><span class="p">);</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToDevice</span><span class="p">);</span><span class="w"> </span><span class="c1">// 使用默认流！</span>

<span class="c1">// 正确：所有操作都使用捕获流</span>
<span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToDevice</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
</code></pre></div>

<ol start="3">
<li><strong>Graph资源管理</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：没有释放Graph资源</span>
<span class="n">cudaGraph_t</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="n">cudaGraphCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">graph</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// 使用graph...</span>

<span class="c1">// 正确：正确释放</span>
<span class="n">cudaGraphDestroy</span><span class="p">(</span><span class="n">graph</span><span class="p">);</span>
<span class="n">cudaGraphExecDestroy</span><span class="p">(</span><span class="n">graphExec</span><span class="p">);</span>
</code></pre></div>

<h3 id="_6">融合相关陷阱</h3>
<ol start="4">
<li><strong>寄存器压力过大</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：过度融合导致寄存器溢出</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">overFusedKernel</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">regs</span><span class="p">[</span><span class="mi">128</span><span class="p">];</span><span class="w">  </span><span class="c1">// 太多寄存器变量</span>
<span class="w">    </span><span class="c1">// 导致寄存器溢出到本地内存</span>
<span class="p">}</span>

<span class="c1">// 正确：平衡融合程度</span>
<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">balancedFusion</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">regs</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span><span class="w">  </span><span class="c1">// 适度的寄存器使用</span>
<span class="p">}</span>
</code></pre></div>

<ol start="5">
<li><strong>共享内存bank conflict</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：融合后产生bank conflict</span>
<span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">32</span><span class="p">];</span><span class="w">  </span><span class="c1">// 32-way bank conflict</span>

<span class="c1">// 正确：padding避免conflict</span>
<span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">shared</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">33</span><span class="p">];</span><span class="w">  </span><span class="c1">// 避免bank conflict</span>
</code></pre></div>

<h3 id="jit">JIT相关陷阱</h3>
<ol start="6">
<li><strong>编译错误处理不当</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：不检查编译结果</span>
<span class="n">nvrtcCompileProgram</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span>

<span class="c1">// 正确：检查并获取错误信息</span>
<span class="n">nvrtcResult</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nvrtcCompileProgram</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">result</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">NVRTC_SUCCESS</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">logSize</span><span class="p">;</span>
<span class="w">    </span><span class="n">nvrtcGetProgramLogSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">logSize</span><span class="p">);</span>
<span class="w">    </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="kt">char</span><span class="p">[</span><span class="n">logSize</span><span class="p">];</span>
<span class="w">    </span><span class="n">nvrtcGetProgramLog</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="w"> </span><span class="n">log</span><span class="p">);</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Compilation error: %s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">log</span><span class="p">);</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[]</span><span class="w"> </span><span class="n">log</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<ol start="7">
<li><strong>缓存失效问题</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 错误：不考虑架构差异</span>
<span class="c1">// 在SM_70上编译的代码在SM_80上运行</span>

<span class="c1">// 正确：包含架构信息在缓存键中</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">cacheKey</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sourceHash</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;_sm&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">smVersion</span><span class="p">);</span>
</code></pre></div>

<h3 id="_7">调试技巧</h3>
<ol>
<li><strong>使用Nsight Compute分析Graph</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code>ncu<span class="w"> </span>--target-processes<span class="w"> </span>all<span class="w"> </span>--graph-profiling<span class="o">=</span>node<span class="w"> </span>./app
</code></pre></div>

<ol start="2">
<li><strong>融合效果验证</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1">// 添加验证代码</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">verifyFusion</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 运行未融合版本</span>
<span class="w">    </span><span class="n">runUnfused</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">expected_output</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 运行融合版本</span>
<span class="w">    </span><span class="n">runFused</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">actual_output</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 比较结果</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">maxError</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compareResults</span><span class="p">(</span><span class="n">expected_output</span><span class="p">,</span><span class="w"> </span><span class="n">actual_output</span><span class="p">);</span>
<span class="w">    </span><span class="n">assert</span><span class="p">(</span><span class="n">maxError</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">1e-5f</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<ol start="3">
<li><strong>JIT编译时间监控</strong></li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CompilationMonitor</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">compilationTimes</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">recordCompilation</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">time</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">compilationTimes</span><span class="p">[</span><span class="n">kernel</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">time</span><span class="p">;</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">time</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">1000.0f</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// 超过1秒</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Warning: %s compilation took %.2f ms</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                   </span><span class="n">kernel</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="n">time</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h2 id="209">20.9 最佳实践检查清单</h2>
<h3 id="graph_1">Graph优化检查清单</h3>
<ul>
<li>[ ] <strong>Graph构建</strong></li>
<li>□ 预分配所有需要的缓冲区</li>
<li>□ 使用流捕获vs手动构建的权衡</li>
<li>□ 正确处理多流同步</li>
<li>
<p>□ Graph更新策略明确</p>
</li>
<li>
<p>[ ] <strong>性能优化</strong></p>
</li>
<li>□ 最小化Graph中的同步点</li>
<li>□ 合并小kernel减少节点数</li>
<li>□ 使用异步操作</li>
<li>
<p>□ 考虑Graph分区for大规模图</p>
</li>
<li>
<p>[ ] <strong>资源管理</strong></p>
</li>
<li>□ 正确释放Graph和GraphExec</li>
<li>□ 避免Graph实例化的内存泄漏</li>
<li>□ 监控Graph执行时间</li>
</ul>
<h3 id="_8">内核融合检查清单</h3>
<ul>
<li>[ ] <strong>融合策略</strong></li>
<li>□ 识别producer-consumer关系</li>
<li>□ 评估内存带宽节省</li>
<li>□ 考虑寄存器压力</li>
<li>
<p>□ 平衡融合粒度</p>
</li>
<li>
<p>[ ] <strong>实现质量</strong></p>
</li>
<li>□ 避免bank conflict</li>
<li>□ 使用向量化访存</li>
<li>□ 正确处理边界条件</li>
<li>
<p>□ 数值稳定性验证</p>
</li>
<li>
<p>[ ] <strong>性能验证</strong></p>
</li>
<li>□ 对比融合前后的内存访问</li>
<li>□ 测量实际加速比</li>
<li>□ 分析占用率变化</li>
<li>□ 检查缓存利用率</li>
</ul>
<h3 id="jit_1">JIT优化检查清单</h3>
<ul>
<li>[ ] <strong>编译策略</strong></li>
<li>□ 识别适合JIT的场景</li>
<li>□ 设计合理的模板</li>
<li>□ 实现编译缓存</li>
<li>
<p>□ 处理编译失败</p>
</li>
<li>
<p>[ ] <strong>性能考虑</strong></p>
</li>
<li>□ 平衡编译时间和执行时间</li>
<li>□ 缓存命中率监控</li>
<li>□ 内存使用控制</li>
<li>
<p>□ 多架构支持</p>
</li>
<li>
<p>[ ] <strong>代码生成</strong></p>
</li>
<li>□ 生成高效的代码</li>
<li>□ 利用编译时常量</li>
<li>□ 适当的循环展开</li>
<li>□ 向量化优化</li>
</ul>
<h3 id="_9">自动调优检查清单</h3>
<ul>
<li>[ ] <strong>搜索空间</strong></li>
<li>□ 定义完整但精简的搜索空间</li>
<li>□ 有效的剪枝策略</li>
<li>□ 硬件约束考虑</li>
<li>
<p>□ 增量式搜索</p>
</li>
<li>
<p>[ ] <strong>调优算法</strong></p>
</li>
<li>□ 选择合适的优化算法</li>
<li>□ 性能模型准确性</li>
<li>□ 收敛速度监控</li>
<li>
<p>□ 过拟合预防</p>
</li>
<li>
<p>[ ] <strong>实用性</strong></p>
</li>
<li>□ 调优时间预算</li>
<li>□ 结果可重现性</li>
<li>□ 跨平台移植性</li>
<li>□ 调优结果持久化
```</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter19.html" class="nav-link prev">← 第19章：多GPU编程与扩展</a><a href="chapter21.html" class="nav-link next">第21章：嵌入式GPU开发（Jetson） →</a></nav>
        </main>
    </div>
</body>
</html>