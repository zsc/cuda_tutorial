<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第21章：嵌入式GPU开发（Jetson）</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">CUDA 高性能编程实战教程</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：CUDA硬件架构深度剖析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：CUDA编程模型与执行模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：全局内存优化策略</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：共享内存与Bank Conflict</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：寄存器优化与常量内存</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：Warp级编程与协作组</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：原子操作与同步原语</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：PTX内联与底层优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：张量核心与混合精度计算</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：CUTLASS深度解析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：激光雷达点云处理加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：多传感器融合的并行化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：实时语义分割与实例分割</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：路径规划与轨迹优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：视觉SLAM的GPU加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：机械臂运动规划</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第17章：强化学习推理加速</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第18章：大规模点云重建与网格化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第19章：多GPU编程与扩展</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第20章：CUDA Graph与内核融合</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第21章：嵌入式GPU开发（Jetson）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第22章：稀疏计算与动态稀疏</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第23章：量化与低精度计算</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第24章：新一代GPU特性展望</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第25章：性能分析与调优方法论</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第26章：CUDA调试技术与错误处理</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter27.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第27章：开发环境与工具链配置</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="21gpujetson">第21章：嵌入式GPU开发（Jetson）</h1>
<p>自动驾驶和具身智能需要在边缘设备上进行实时推理，NVIDIA Jetson平台提供了功耗优化的GPU计算能力。本章深入探讨Jetson架构特点、功耗管理、内存优化和TensorRT集成，帮助你将高性能AI应用部署到边缘设备。通过实际的自动驾驶感知系统案例，你将掌握从模型优化到系统集成的完整边缘部署流程。</p>
<h2 id="211-jetson">21.1 Jetson架构特点</h2>
<p>Jetson平台采用了与桌面级GPU显著不同的架构设计，针对嵌入式场景的功耗、体积和成本约束进行了深度优化。理解这些架构特点是高效开发边缘AI应用的基础。</p>
<h3 id="2111-jetson">21.1.1 Jetson产品线概览</h3>
<p>NVIDIA Jetson产品线覆盖了从入门级到高性能的完整谱系，每个型号都针对特定的应用场景优化：</p>
<div class="codehilite"><pre><span></span><code>产品型号        GPU架构    CUDA核心   内存      功耗    典型应用
----------------------------------------------------------------
Jetson Nano    Maxwell    128       4GB      5-10W   入门级AI推理
Jetson TX2     Pascal     256       8GB      7.5-15W 工业视觉
Jetson Xavier  Volta      512       16/32GB  10-30W  自动驾驶
Jetson Orin    Ampere     1024-2048 32/64GB  15-60W  机器人/AV
</code></pre></div>

<p>关键的架构演进包括：</p>
<ul>
<li><strong>Maxwell到Pascal</strong>：引入统一内存，提升内存带宽效率</li>
<li><strong>Pascal到Volta</strong>：添加Tensor Core，支持混合精度计算</li>
<li><strong>Volta到Ampere</strong>：优化稀疏计算，提升INT8性能</li>
<li><strong>Ampere到Ada</strong>：增强光线追踪和AI推理能力</li>
</ul>
<p>选型时需要考虑的关键因素：</p>
<ol>
<li><strong>算力需求</strong>：TOPS（Tera Operations Per Second）指标</li>
<li><strong>内存容量</strong>：模型大小和批处理需求</li>
<li><strong>功耗预算</strong>：电池供电还是外接电源</li>
<li><strong>接口需求</strong>：摄像头数量、PCIe扩展等</li>
<li><strong>软件兼容性</strong>：JetPack版本和CUDA计算能力</li>
</ol>
<h3 id="2112-uma">21.1.2 统一内存架构（UMA）</h3>
<p>Jetson采用统一内存架构，CPU和GPU共享同一物理内存，这带来了独特的优化机会：</p>
<div class="codehilite"><pre><span></span><code>传统桌面GPU架构：              Jetson UMA架构：
┌─────────┐  ┌─────────┐       ┌─────────────────┐
│   CPU   │  │   GPU   │       │  CPU + GPU SoC  │
└────┬────┘  └────┬────┘       └────────┬────────┘
     │            │                      │
┌────▼────┐  ┌────▼────┐              ┌─▼─┐
│ 系统内存 │  │ 显存    │              │统一│
└─────────┘  └─────────┘              │内存│
                                       └───┘
</code></pre></div>

<p>UMA的优势：</p>
<ul>
<li><strong>零拷贝访问</strong>：CPU和GPU可以直接访问相同的内存地址</li>
<li><strong>内存利用率高</strong>：动态分配，避免显存浪费</li>
<li><strong>简化编程模型</strong>：减少显式内存传输</li>
</ul>
<p>UMA的挑战：</p>
<ul>
<li><strong>带宽竞争</strong>：CPU和GPU共享内存带宽</li>
<li><strong>缓存一致性</strong>：需要正确管理缓存刷新</li>
<li><strong>页面迁移开销</strong>：操作系统可能在CPU/GPU间迁移页面</li>
</ul>
<p>编程时的关键API：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 分配统一内存</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="c1">// 设置内存访问提示</span>
<span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemAdviseSetPreferredLocation</span><span class="p">,</span><span class="w"> </span><span class="n">deviceId</span><span class="p">);</span>
<span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemAdviseSetAccessedBy</span><span class="p">,</span><span class="w"> </span><span class="n">deviceId</span><span class="p">);</span>

<span class="c1">// 预取数据到指定设备</span>
<span class="n">cudaMemPrefetchAsync</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">deviceId</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>

<span class="c1">// 零拷贝内存（固定内存映射）</span>
<span class="n">cudaHostAlloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaHostAllocMapped</span><span class="p">);</span>
<span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
</code></pre></div>

<h3 id="2113-gpu">21.1.3 GPU计算能力差异</h3>
<p>Jetson GPU的计算能力与同代桌面GPU相比有所调整，需要针对性优化：</p>
<p><strong>SM（流多处理器）数量差异</strong>：</p>
<div class="codehilite"><pre><span></span><code>桌面级 RTX 3090：  82个SM，10496 CUDA核心
Jetson AGX Orin：  16个SM，2048 CUDA核心
缩放比例：         ~1:5
</code></pre></div>

<p>这意味着：</p>
<ol>
<li><strong>Grid配置需要调整</strong>：减少block数量以匹配SM数量</li>
<li><strong>占用率策略不同</strong>：更容易达到100%占用率，但绝对性能受限</li>
<li><strong>内存带宽比例</strong>：相对于计算能力，内存带宽更充裕</li>
</ol>
<p><strong>特殊指令支持差异</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 检查计算能力并选择相应算法</span>
<span class="cp">#if __CUDA_ARCH__ &gt;= 700  </span><span class="c1">// Volta及以上</span>
<span class="w">    </span><span class="c1">// 使用Tensor Core加速</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">fragment</span><span class="o">&lt;</span><span class="p">...</span><span class="o">&gt;</span><span class="w"> </span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="n">b_frag</span><span class="p">,</span><span class="w"> </span><span class="n">c_frag</span><span class="p">;</span>
<span class="w">    </span><span class="n">wmma</span><span class="o">::</span><span class="n">load_matrix_sync</span><span class="p">(</span><span class="n">a_frag</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
<span class="cp">#elif __CUDA_ARCH__ &gt;= 600  </span><span class="c1">// Pascal</span>
<span class="w">    </span><span class="c1">// 使用半精度但无Tensor Core</span>
<span class="w">    </span><span class="n">__half2</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__hmul2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
<span class="cp">#else</span>
<span class="w">    </span><span class="c1">// Maxwell：仅支持单精度</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
<span class="cp">#endif</span>
</code></pre></div>

<p><strong>Warp调度差异</strong>：</p>
<ul>
<li>桌面GPU：4个warp调度器/SM（Ampere）</li>
<li>Jetson Orin：4个warp调度器/SM</li>
<li>Jetson Xavier：4个warp调度器/SM</li>
<li>Jetson Nano：2个warp调度器/SM</li>
</ul>
<p>这影响指令级并行度（ILP）的优化策略。</p>
<h3 id="2114">21.1.4 硬件加速器生态</h3>
<p>Jetson集成了多种专用加速器，充分利用这些加速器是达到最优性能的关键：</p>
<p><strong>DLA（Deep Learning Accelerator）</strong>：</p>
<div class="codehilite"><pre><span></span><code>特点：

- 专用于INT8推理
- 功耗极低（0.5-1W）
- 支持常见CNN层
- 可与GPU并行工作

使用场景：

- 背景分割等低精度任务
- 多模型并行推理
- 功耗敏感的持续运行任务
</code></pre></div>

<p><strong>VIC（Video Image Compositor）</strong>：</p>
<div class="codehilite"><pre><span></span><code>功能：

- 颜色空间转换（YUV↔RGB）
- 图像缩放和裁剪
- 多路视频合成
- 去噪和增强

编程接口：

- Multimedia API
- VPI（Vision Programming Interface）
</code></pre></div>

<p><strong>NVENC/NVDEC</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">编码支持</span><span class="err">：</span><span class="n">H</span><span class="mf">.264</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="mf">.265</span><span class="p">,</span><span class="w"> </span><span class="n">VP9</span>
<span class="n">性能</span><span class="err">：</span><span class="mi">4</span><span class="n">K</span><span class="mf">@30f</span><span class="n">ps</span><span class="err">（</span><span class="n">多路</span><span class="err">）</span>
<span class="n">应用</span><span class="err">：</span><span class="n">视频流处理</span><span class="err">、</span><span class="n">录制</span>
</code></pre></div>

<p><strong>ISP（Image Signal Processor）</strong>：</p>
<div class="codehilite"><pre><span></span><code>功能：

- RAW图像处理
- 自动曝光/白平衡
- HDR合成
- 镜头畸变校正

集成方式：

- Argus API
- GStreamer插件
</code></pre></div>

<p>协同使用示例流程：</p>
<div class="codehilite"><pre><span></span><code>摄像头 → ISP → VIC → GPU/DLA → NVENC → 网络传输
   ↓       ↓      ↓        ↓         ↓
  RAW   去噪  缩放   AI推理   压缩编码
</code></pre></div>

<p>性能对比（YOLOv5推理）：</p>
<div class="codehilite"><pre><span></span><code>执行单元    功耗    FPS    延迟
GPU only    15W     30     33ms
GPU + DLA   12W     45     22ms
DLA only    2W      15     67ms
</code></pre></div>

<h2 id="212">21.2 功耗优化策略</h2>
<p>在边缘设备上，功耗直接影响续航时间、散热需求和系统可靠性。Jetson提供了多层次的功耗管理机制，从系统级到指令级都有相应的优化手段。</p>
<h3 id="2121-dvfs">21.2.1 功耗模式与DVFS</h3>
<p>Jetson支持多种预定义的功耗模式，通过动态电压频率调节（DVFS）平衡性能与功耗：</p>
<p><strong>预设功耗模式</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 查看当前功耗模式</span>
sudo<span class="w"> </span>nvpmodel<span class="w"> </span>-q

<span class="c1"># 切换到不同模式（以Orin为例）</span>
sudo<span class="w"> </span>nvpmodel<span class="w"> </span>-m<span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c1"># MAXN模式：最高性能，60W</span>
sudo<span class="w"> </span>nvpmodel<span class="w"> </span>-m<span class="w"> </span><span class="m">1</span><span class="w">  </span><span class="c1"># 50W模式</span>
sudo<span class="w"> </span>nvpmodel<span class="w"> </span>-m<span class="w"> </span><span class="m">2</span><span class="w">  </span><span class="c1"># 30W模式</span>
sudo<span class="w"> </span>nvpmodel<span class="w"> </span>-m<span class="w"> </span><span class="m">3</span><span class="w">  </span><span class="c1"># 15W模式</span>

<span class="c1"># 自定义功耗配置</span>
sudo<span class="w"> </span>nano<span class="w"> </span>/etc/nvpmodel.conf
</code></pre></div>

<p><strong>Jetson Orin功耗模式详解</strong>：</p>
<div class="codehilite"><pre><span></span><code>模式  CPU核心  频率      GPU频率   内存频率  功耗   应用场景
------------------------------------------------------------------
MAXN  12      2.2GHz    1.3GHz    3.2GHz   60W    最高性能
50W   12      2.0GHz    1.1GHz    3.2GHz   50W    平衡模式
30W   8       1.8GHz    900MHz    2.1GHz   30W    功耗优先
15W   4       1.5GHz    625MHz    1.6GHz   15W    低功耗
</code></pre></div>

<p><strong>动态频率管理</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 手动设置GPU频率</span>
sudo<span class="w"> </span>jetson_clocks<span class="w"> </span>--show<span class="w">  </span><span class="c1"># 显示当前频率</span>
sudo<span class="w"> </span>jetson_clocks<span class="w">         </span><span class="c1"># 锁定最高频率</span>
sudo<span class="w"> </span>jetson_clocks<span class="w"> </span>--restore<span class="w">  </span><span class="c1"># 恢复动态调节</span>

<span class="c1"># 细粒度频率控制</span>
<span class="nb">echo</span><span class="w"> </span><span class="m">1300000000</span><span class="w"> </span>&gt;<span class="w"> </span>/sys/devices/17000000.gpu/devfreq/17000000.gpu/max_freq
<span class="nb">echo</span><span class="w"> </span><span class="m">625000000</span><span class="w"> </span>&gt;<span class="w"> </span>/sys/devices/17000000.gpu/devfreq/17000000.gpu/min_freq
</code></pre></div>

<p><strong>应用级功耗管理API</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// CUDA程序中设置GPU频率</span>
<span class="n">cudaDeviceSetLimit</span><span class="p">(</span><span class="n">cudaLimitDevRuntimeSyncDepth</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>

<span class="c1">// 使用NVML API进行功耗监控</span>
<span class="n">nvmlReturn_t</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="n">nvmlDevice_t</span><span class="w"> </span><span class="n">device</span><span class="p">;</span>
<span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">power</span><span class="p">;</span>

<span class="n">nvmlInit</span><span class="p">();</span>
<span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device</span><span class="p">);</span>
<span class="n">nvmlDeviceGetPowerUsage</span><span class="p">(</span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">power</span><span class="p">);</span><span class="w">  </span><span class="c1">// 获取实时功耗（毫瓦）</span>
</code></pre></div>

<h3 id="2122">21.2.2 内核级功耗优化</h3>
<p>CUDA内核的设计直接影响功耗，优化策略包括：</p>
<p><strong>1. 降低动态功耗</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 低功耗版本：减少活跃线程数</span>
<span class="kr">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">kernel_low_power</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">tid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">stride</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">gridDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 使用更大的stride，减少活跃SM数量</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tid</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 使用更低精度的运算</span>
<span class="w">        </span><span class="n">__half2</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__float2half2_rn</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__hmul2</span><span class="p">(</span><span class="n">val</span><span class="p">,</span><span class="w"> </span><span class="n">__float2half2_rn</span><span class="p">(</span><span class="mf">0.5f</span><span class="p">));</span>
<span class="w">        </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__half2float</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 插入空闲周期降低功耗</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">((</span><span class="n">i</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="mh">0xFF</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">__nanosleep</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span><span class="w">  </span><span class="c1">// PTX级别的休眠</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>2. 利用低功耗指令</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用FMA指令减少指令数</span>
<span class="kt">float</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fmaf</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">);</span><span class="w">  </span><span class="c1">// 比 a*b+c 功耗更低</span>

<span class="c1">// 使用位运算代替除法</span>
<span class="kt">int</span><span class="w"> </span><span class="n">div_by_32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="mi">5</span><span class="p">;</span><span class="w">  </span><span class="c1">// 代替 value / 32</span>

<span class="c1">// 使用查表代替复杂计算</span>
<span class="kt">__constant__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">lut</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
<span class="kt">float</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lut</span><span class="p">[</span><span class="n">index</span><span class="p">];</span><span class="w">  </span><span class="c1">// 代替 sinf/cosf等</span>
</code></pre></div>

<p><strong>3. 内存访问优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用纹理内存降低功耗（缓存友好）</span>
<span class="n">texture</span><span class="o">&lt;</span><span class="kt">float4</span><span class="p">,</span><span class="w"> </span><span class="n">cudaTextureType2D</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tex</span><span class="p">;</span>
<span class="kt">float4</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tex2D</span><span class="p">(</span><span class="n">tex</span><span class="p">,</span><span class="w"> </span><span class="n">u</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="p">);</span>

<span class="c1">// 合并访问减少内存事务</span>
<span class="kt">float4</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float4</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">ptr</span><span class="p">)[</span><span class="n">tid</span><span class="p">];</span>

<span class="c1">// 使用共享内存减少全局内存访问</span>
<span class="kt">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">cache</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
<span class="n">cache</span><span class="p">[</span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global_data</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="nf">__syncthreads</span><span class="p">();</span>
</code></pre></div>

<p><strong>4. Warp级优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 保持warp内线程同步，减少分支分歧</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">__all_sync</span><span class="p">(</span><span class="mh">0xFFFFFFFF</span><span class="p">,</span><span class="w"> </span><span class="n">condition</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 所有线程执行相同路径</span>
<span class="p">}</span>

<span class="c1">// 使用warp级原语减少同步开销</span>
<span class="kt">int</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">__reduce_add_sync</span><span class="p">(</span><span class="mh">0xFFFFFFFF</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
</code></pre></div>

<h3 id="2123">21.2.3 内存访问模式优化</h3>
<p>内存访问是功耗的主要来源，优化策略包括：</p>
<p><strong>1. 数据布局优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// AoS转SoA减少内存事务</span>
<span class="c1">// 低效（AoS）：</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Point</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">;</span><span class="w"> </span><span class="p">};</span>
<span class="n">Point</span><span class="w"> </span><span class="n">points</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>

<span class="c1">// 高效（SoA）：</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Points</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="n">z</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>2. 缓存优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// L2缓存持久化</span>
<span class="n">cudaFuncSetAttribute</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span>
<span class="w">    </span><span class="n">cudaFuncAttributePreferredSharedMemoryCarveout</span><span class="p">,</span><span class="w"> </span>
<span class="w">    </span><span class="n">cudaSharedmemCarveoutMaxL1</span><span class="p">);</span>

<span class="c1">// 设置L2缓存驻留</span>
<span class="n">cudaDeviceSetLimit</span><span class="p">(</span><span class="n">cudaLimitPersistingL2CacheSize</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="o">*</span><span class="mi">1024</span><span class="p">);</span>

<span class="c1">// 使用缓存提示</span>
<span class="n">__builtin_prefetch</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span><span class="w">  </span><span class="c1">// 预取到L1</span>
</code></pre></div>

<p><strong>3. 内存压缩</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用压缩数据格式</span>
<span class="c1">// 原始：float[1024] = 4KB</span>
<span class="c1">// 压缩：half[1024] = 2KB</span>
<span class="c1">// 或使用自定义量化</span>
<span class="kt">uint8_t</span><span class="w"> </span><span class="n">quantized</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">uint8_t</span><span class="p">)(</span><span class="n">value</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">255.0f</span><span class="p">);</span>
<span class="kt">float</span><span class="w"> </span><span class="n">restored</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">quantized</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">255.0f</span><span class="p">;</span>
</code></pre></div>

<p><strong>4. 批处理与流水线</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 双缓冲减少空闲等待</span>
<span class="kt">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">buffer</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">256</span><span class="p">];</span>
<span class="kt">int</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="c1">// 加载第一批数据</span>
<span class="n">buffer</span><span class="p">[</span><span class="n">current</span><span class="p">][</span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
<span class="nf">__syncthreads</span><span class="p">();</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batches</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 异步加载下一批</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="mi">1</span><span class="o">-</span><span class="n">current</span><span class="p">][</span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">tid</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="o">*</span><span class="mi">256</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 处理当前批</span>
<span class="w">    </span><span class="n">process</span><span class="p">(</span><span class="n">buffer</span><span class="p">[</span><span class="n">current</span><span class="p">]);</span>

<span class="w">    </span><span class="n">current</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">current</span><span class="p">;</span>
<span class="w">    </span><span class="nf">__syncthreads</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="2124">21.2.4 热管理与散热设计</h3>
<p>热管理对维持性能和系统稳定性至关重要：</p>
<p><strong>温度监控</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 实时温度监控</span>
tegrastats<span class="w">  </span><span class="c1"># 显示CPU/GPU温度、频率、功耗</span>

<span class="c1"># 读取温度传感器</span>
cat<span class="w"> </span>/sys/devices/virtual/thermal/thermal_zone*/temp

<span class="c1"># Python监控脚本</span>
import<span class="w"> </span>os
def<span class="w"> </span>get_temps<span class="o">()</span>:
<span class="w">    </span><span class="nv">zones</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[]</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span>range<span class="o">(</span><span class="m">10</span><span class="o">)</span>:
<span class="w">        </span>try:
<span class="w">            </span>with<span class="w"> </span>open<span class="o">(</span>f<span class="s1">&#39;/sys/devices/virtual/thermal/thermal_zone{i}/temp&#39;</span><span class="o">)</span><span class="w"> </span>as<span class="w"> </span>f:
<span class="w">                </span><span class="nv">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>int<span class="o">(</span>f.read<span class="o">())</span><span class="w"> </span>/<span class="w"> </span><span class="m">1000</span>.0
<span class="w">                </span>zones.append<span class="o">(</span>temp<span class="o">)</span>
<span class="w">        </span>except:
<span class="w">            </span><span class="k">break</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span>zones
</code></pre></div>

<p><strong>热节流策略</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 应用级热管理</span>
<span class="n">class</span><span class="w"> </span><span class="n">ThermalManager</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">temp_threshold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">75.0f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 摄氏度</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">current_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">;</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">adjust_workload</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">float</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_gpu_temperature</span><span class="p">();</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">temp</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">temp_threshold</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">current_scale</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="mf">0.9f</span><span class="p">;</span><span class="w">  </span><span class="c1">// 降低10%负载</span>
<span class="w">            </span><span class="n">usleep</span><span class="p">(</span><span class="mi">1000</span><span class="p">);</span><span class="w">  </span><span class="c1">// 增加延迟</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">temp</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">temp_threshold</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">5.0f</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">current_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">min</span><span class="p">(</span><span class="mf">1.0f</span><span class="p">,</span><span class="w"> </span><span class="n">current_scale</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1.1f</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 调整内核配置</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">blocks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_blocks</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">current_scale</span><span class="p">;</span>
<span class="w">        </span><span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>散热优化最佳实践</strong>：</p>
<ol>
<li><strong>任务调度</strong>：在温度低谷期执行高强度任务</li>
<li><strong>负载均衡</strong>：在CPU/GPU/DLA间分配任务</li>
<li><strong>间歇运行</strong>：插入空闲周期让芯片降温</li>
<li><strong>功耗上限</strong>：设置功耗预算避免过热</li>
</ol>
<p><strong>系统级散热配置</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 配置风扇曲线</span>
sudo<span class="w"> </span>nano<span class="w"> </span>/etc/nvfancontrol.conf

<span class="c1"># 示例配置</span>
FAN_PROFILE<span class="w"> </span>quiet<span class="w"> </span><span class="o">{</span>
<span class="w">    </span><span class="c1">#temp   fan_speed</span>
<span class="w">    </span><span class="m">20</span><span class="w">      </span><span class="m">0</span>
<span class="w">    </span><span class="m">50</span><span class="w">      </span><span class="m">30</span>
<span class="w">    </span><span class="m">70</span><span class="w">      </span><span class="m">60</span>
<span class="w">    </span><span class="m">85</span><span class="w">      </span><span class="m">100</span>
<span class="o">}</span>

<span class="c1"># 应用配置</span>
sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>nvfancontrol
</code></pre></div>

<p>功耗优化验证工具：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 使用tegrastats记录功耗数据</span>
tegrastats<span class="w"> </span>--logfile<span class="w"> </span>power_log.txt

<span class="c1"># 分析功耗模式</span>
python3<span class="w"> </span>analyze_power.py<span class="w"> </span>power_log.txt

<span class="c1"># 功耗与性能权衡分析</span>
<span class="c1"># FPS/Watt指标是关键评估标准</span>
</code></pre></div>

<h2 id="213">21.3 统一内存的最佳实践</h2>
<p>Jetson的统一内存架构是其独特优势，正确使用可以显著简化编程并提升性能。本节详细探讨各种统一内存技术的最佳实践。</p>
<h3 id="2131">21.3.1 零拷贝内存使用</h3>
<p>零拷贝内存允许CPU和GPU直接访问相同的物理内存，避免数据传输开销：</p>
<p><strong>零拷贝内存分配方式</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 方式1：固定内存映射</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">cpu_ptr</span><span class="p">;</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">gpu_ptr</span><span class="p">;</span>
<span class="n">cudaHostAlloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cpu_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaHostAllocMapped</span><span class="p">);</span>
<span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="c1">// 方式2：统一内存（推荐）</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="c1">// 方式3：系统分配内存注册</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
<span class="n">cudaHostRegister</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaHostRegisterMapped</span><span class="p">);</span>
<span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpu_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
</code></pre></div>

<p><strong>零拷贝访问模式对比</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 传统模式：需要显式拷贝</span>
<span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">h_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
<span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">d_data</span><span class="p">;</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_data</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_data</span><span class="p">,</span><span class="w"> </span><span class="n">h_data</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_data</span><span class="p">);</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_data</span><span class="p">,</span><span class="w"> </span><span class="n">d_data</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="c1">// 零拷贝模式：直接访问</span>
<span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">;</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="c1">// CPU初始化</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="c1">// GPU处理</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">);</span>
<span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="c1">// CPU读取结果</span>
<span class="kt">float</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</code></pre></div>

<p><strong>性能优化技巧</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 1. 使用访问提示优化页面放置</span>
<span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemAdviseSetPreferredLocation</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemAdviseSetAccessedBy</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>

<span class="c1">// 2. 批量处理减少页面故障</span>
<span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span><span class="w">  </span><span class="c1">// 1MB批次</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">total_size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaMemPrefetchAsync</span><span class="p">(</span><span class="n">ptr</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="w">    </span><span class="n">process_batch</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">ptr</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// 3. 使用流实现异步处理</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>
<span class="n">cudaMemPrefetchAsync</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">);</span>
</code></pre></div>

<h3 id="2132">21.3.2 页面迁移策略</h3>
<p>统一内存的页面迁移策略直接影响性能：</p>
<p><strong>页面迁移触发机制</strong>：</p>
<div class="codehilite"><pre><span></span><code>触发条件         CPU访问    GPU访问    迁移方向
-----------------------------------------------
首次访问         是         是         访问者
页面故障         是         是         故障位置
预取操作         手动       手动       指定位置
访问计数器       自动       自动       高频访问者
</code></pre></div>

<p><strong>优化页面迁移的策略</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">class</span><span class="w"> </span><span class="n">UnifiedMemoryManager</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">MemoryRegion</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">preferred_device</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MemoryRegion</span><span class="o">&gt;</span><span class="w"> </span><span class="n">regions</span><span class="p">;</span>

<span class="n">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">allocate</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// 设置首选位置</span>
<span class="w">            </span><span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span>
<span class="w">                </span><span class="n">cudaMemAdviseSetPreferredLocation</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">);</span>
<span class="w">            </span><span class="c1">// 允许所有设备访问</span>
<span class="w">            </span><span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span>
<span class="w">                </span><span class="n">cudaMemAdviseSetAccessedBy</span><span class="p">,</span><span class="w"> </span><span class="n">cudaCpuDeviceId</span><span class="p">);</span>
<span class="w">            </span><span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span>
<span class="w">                </span><span class="n">cudaMemAdviseSetAccessedBy</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">regions</span><span class="p">.</span><span class="n">push_back</span><span class="p">({</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">});</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">prefetch</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">find_region</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">it</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">regions</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">cudaMemPrefetchAsync</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">it</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="w">            </span><span class="n">it</span><span class="o">-&gt;</span><span class="n">preferred_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">optimize_placement</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 基于访问模式动态调整</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">region</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">regions</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">free_mem</span><span class="p">,</span><span class="w"> </span><span class="n">total_mem</span><span class="p">;</span>
<span class="w">            </span><span class="n">cudaMemGetInfo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">free_mem</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">total_mem</span><span class="p">);</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">free_mem</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">region</span><span class="p">.</span><span class="n">size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// 内存充足，预取到GPU</span>
<span class="w">                </span><span class="n">prefetch</span><span class="p">(</span><span class="n">region</span><span class="p">.</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">region</span><span class="p">.</span><span class="n">stream</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// 内存紧张，保留在CPU</span>
<span class="w">                </span><span class="n">prefetch</span><span class="p">(</span><span class="n">region</span><span class="p">.</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">cudaCpuDeviceId</span><span class="p">,</span><span class="w"> </span><span class="n">region</span><span class="p">.</span><span class="n">stream</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>避免页面抖动</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 问题：频繁的CPU/GPU交替访问导致页面抖动</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">bad_pattern</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// CPU写入</span>
<span class="w">        </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">        </span><span class="c1">// GPU处理（触发迁移到GPU）</span>
<span class="w">        </span><span class="n">process</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="c1">// CPU读取（触发迁移回CPU）</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// 解决：批量处理，减少迁移次数</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">good_pattern</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// CPU批量初始化</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// 预取到GPU</span>
<span class="w">    </span><span class="n">cudaMemPrefetchAsync</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// GPU批量处理</span>
<span class="w">    </span><span class="n">process</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
<span class="w">    </span><span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="w">    </span><span class="c1">// 预取回CPU</span>
<span class="w">    </span><span class="n">cudaMemPrefetchAsync</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">cudaCpuDeviceId</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// CPU批量读取</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<h3 id="2133">21.3.3 内存池管理</h3>
<p>在Jetson上实现高效的内存池管理可以减少分配开销和碎片化：</p>
<p><strong>自定义内存池实现</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">class</span><span class="w"> </span><span class="n">JetsonMemoryPool</span><span class="w"> </span><span class="p">{</span>
<span class="n">private</span><span class="o">:</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">Block</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">;</span>
<span class="w">        </span><span class="kt">bool</span><span class="w"> </span><span class="n">in_use</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">device_hint</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Block</span><span class="o">&gt;</span><span class="w"> </span><span class="n">blocks</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="w"> </span><span class="n">pool_mutex</span><span class="p">;</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">total_allocated</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">max_pool_size</span><span class="p">;</span>

<span class="n">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">JetsonMemoryPool</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">max_size</span><span class="p">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">max_pool_size</span><span class="p">(</span><span class="n">max_size</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>

<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">allocate</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lock</span><span class="p">(</span><span class="n">pool_mutex</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 查找可重用的块</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">blocks</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">block</span><span class="p">.</span><span class="n">in_use</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">size</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">&amp;&amp;</span>
<span class="w">                </span><span class="n">block</span><span class="p">.</span><span class="n">size</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1.5</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// 避免过度浪费</span>
<span class="w">                </span><span class="n">block</span><span class="p">.</span><span class="n">in_use</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>

<span class="w">                </span><span class="c1">// 根据新的设备提示调整</span>
<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">device_hint</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">block</span><span class="p">.</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">size</span><span class="p">,</span>
<span class="w">                        </span><span class="n">cudaMemAdviseSetPreferredLocation</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">);</span>
<span class="w">                    </span><span class="n">block</span><span class="p">.</span><span class="n">device_hint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device</span><span class="p">;</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">ptr</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 分配新块</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">total_allocated</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">max_pool_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="w">            </span><span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">cudaMemAdvise</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span>
<span class="w">                    </span><span class="n">cudaMemAdviseSetPreferredLocation</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="n">blocks</span><span class="p">.</span><span class="n">push_back</span><span class="p">({</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">});</span>
<span class="w">            </span><span class="n">total_allocated</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">size</span><span class="p">;</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">ptr</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">nullptr</span><span class="p">;</span><span class="w">  </span><span class="c1">// 池已满</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">deallocate</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">lock_guard</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lock</span><span class="p">(</span><span class="n">pool_mutex</span><span class="p">);</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">block</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">blocks</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">block</span><span class="p">.</span><span class="n">ptr</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">block</span><span class="p">.</span><span class="n">in_use</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">                </span><span class="c1">// 可选：清理内存内容</span>
<span class="w">                </span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">.</span><span class="n">size</span><span class="p">);</span>
<span class="w">                </span><span class="k">return</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">defragment</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 合并相邻的空闲块</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="n">blocks</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">blocks</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span>
<span class="w">            </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">Block</span><span class="o">&amp;</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Block</span><span class="o">&amp;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">ptr</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">ptr</span><span class="p">;</span>
<span class="w">            </span><span class="p">});</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">blocks</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">in_use</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">in_use</span><span class="w"> </span><span class="o">&amp;&amp;</span>
<span class="w">                </span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">ptr</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">size</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// 合并块</span>
<span class="w">                </span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">].</span><span class="n">size</span><span class="p">;</span>
<span class="w">                </span><span class="n">blocks</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">blocks</span><span class="p">.</span><span class="n">begin</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">i</span><span class="o">++</span><span class="p">;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2134-dma">21.3.4 DMA优化技术</h3>
<p>直接内存访问（DMA）优化可以释放CPU资源并提升传输效率：</p>
<p><strong>DMA传输优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用异步内存操作</span>
<span class="n">class</span><span class="w"> </span><span class="n">DMAOptimizer</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">dma_stream</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">queue</span><span class="o">&lt;</span><span class="n">cudaEvent_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pending_events</span><span class="p">;</span>

<span class="n">public</span><span class="o">:</span>
<span class="w">    </span><span class="n">DMAOptimizer</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 创建专用DMA流</span>
<span class="w">        </span><span class="n">cudaStreamCreateWithPriority</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dma_stream</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">cudaStreamNonBlocking</span><span class="p">,</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span><span class="w">  </span><span class="c1">// 高优先级</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">async_transfer</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 使用DMA引擎进行传输</span>
<span class="w">        </span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">cudaMemcpyDefault</span><span class="p">,</span><span class="w"> </span><span class="n">dma_stream</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 记录事件用于同步</span>
<span class="w">        </span><span class="n">cudaEvent_t</span><span class="w"> </span><span class="n">event</span><span class="p">;</span>
<span class="w">        </span><span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">event</span><span class="p">);</span>
<span class="w">        </span><span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event</span><span class="p">,</span><span class="w"> </span><span class="n">dma_stream</span><span class="p">);</span>
<span class="w">        </span><span class="n">pending_events</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">event</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">pipeline_transfer</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span>
<span class="w">                          </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">chunks</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">chunks</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">chunks</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">size_t</span><span class="w"> </span><span class="n">offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">chunk_size</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 异步传输当前块</span>
<span class="w">            </span><span class="n">cudaMemcpyAsync</span><span class="p">((</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span>
<span class="w">                          </span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">src</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span>
<span class="w">                          </span><span class="n">chunk_size</span><span class="p">,</span>
<span class="w">                          </span><span class="n">cudaMemcpyDefault</span><span class="p">,</span>
<span class="w">                          </span><span class="n">dma_stream</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 在传输的同时可以处理前一块</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="kt">size_t</span><span class="w"> </span><span class="n">prev_offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">chunk_size</span><span class="p">;</span>
<span class="w">                </span><span class="n">process</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">dma_stream</span><span class="o">&gt;&gt;&gt;</span>
<span class="w">                    </span><span class="p">((</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">prev_offset</span><span class="p">,</span><span class="w"> </span><span class="n">chunk_size</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>硬件DMA通道利用</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// Jetson特定的DMA优化</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">optimize_for_jetson_dma</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 1. 对齐到页面边界</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">page_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4096</span><span class="p">;</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">aligned_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">page_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="o">~</span><span class="p">(</span><span class="n">page_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 2. 使用大页面减少TLB压力</span>
<span class="w">    </span><span class="n">madvise</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aligned_size</span><span class="p">,</span><span class="w"> </span><span class="n">MADV_HUGEPAGE</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 3. 锁定内存防止交换</span>
<span class="w">    </span><span class="n">mlock</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aligned_size</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 4. 设置NUMA亲和性（如果适用）</span>
<span class="w">    </span><span class="n">numa_tonode_memory</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">aligned_size</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p><strong>VIC硬件加速的DMA</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 使用VIC进行图像DMA和处理</span>
<span class="n">class</span><span class="w"> </span><span class="n">VICDMAProcessor</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">NvBufferSession</span><span class="o">*</span><span class="w"> </span><span class="n">session</span><span class="p">;</span>

<span class="n">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">process_image_with_vic</span><span class="p">(</span><span class="kt">uint8_t</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">uint8_t</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span>
<span class="w">                                </span><span class="kt">int</span><span class="w"> </span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">height</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 创建VIC兼容的缓冲区</span>
<span class="w">        </span><span class="n">NvBufferCreateParams</span><span class="w"> </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="w">        </span><span class="n">params</span><span class="p">.</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">width</span><span class="p">;</span>
<span class="w">        </span><span class="n">params</span><span class="p">.</span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">height</span><span class="p">;</span>
<span class="w">        </span><span class="n">params</span><span class="p">.</span><span class="n">payloadType</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NvBufferPayload_SurfArray</span><span class="p">;</span>
<span class="w">        </span><span class="n">params</span><span class="p">.</span><span class="n">nvbuf_tag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NvBufferTag_VIC</span><span class="p">;</span>

<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">src_fd</span><span class="p">,</span><span class="w"> </span><span class="n">dst_fd</span><span class="p">;</span>
<span class="w">        </span><span class="n">NvBufferCreateEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">src_fd</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">params</span><span class="p">);</span>
<span class="w">        </span><span class="n">NvBufferCreateEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dst_fd</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">params</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 使用VIC进行缩放+颜色转换（硬件DMA）</span>
<span class="w">        </span><span class="n">NvBufferTransformParams</span><span class="w"> </span><span class="n">transform_params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="w">        </span><span class="n">transform_params</span><span class="p">.</span><span class="n">transform_flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>
<span class="w">            </span><span class="n">NVBUFFER_TRANSFORM_FILTER</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">NVBUFFER_TRANSFORM_FLIP</span><span class="p">;</span>
<span class="w">        </span><span class="n">transform_params</span><span class="p">.</span><span class="n">transform_filter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NvBufferTransform_Filter_Smart</span><span class="p">;</span>

<span class="w">        </span><span class="n">NvBufferTransform</span><span class="p">(</span><span class="n">src_fd</span><span class="p">,</span><span class="w"> </span><span class="n">dst_fd</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">transform_params</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 结果可直接用于CUDA处理，无需额外拷贝</span>
<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">gpu_ptr</span><span class="p">;</span>
<span class="w">        </span><span class="n">NvBufferMemMap</span><span class="p">(</span><span class="n">dst_fd</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">NvBufferMem_Read</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gpu_ptr</span><span class="p">);</span>
<span class="w">        </span><span class="n">process_cuda</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">gpu_ptr</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h2 id="214-tensorrt">21.4 TensorRT集成</h2>
<p>TensorRT是NVIDIA专为推理优化设计的高性能深度学习推理库，在Jetson平台上扮演着关键角色。它通过层融合、精度校准、内核自动调优等技术，可以将模型推理速度提升3-10倍，同时降低内存占用和功耗。对于自动驾驶和具身智能应用，TensorRT是实现实时推理的核心技术。</p>
<h3 id="2141">21.4.1 模型转换流程</h3>
<p>将训练好的模型转换为TensorRT引擎涉及多个步骤，每一步都有其优化空间。理解整个转换流程对于获得最佳性能至关重要。</p>
<p><strong>转换路径选择</strong>：</p>
<p>TensorRT支持多种模型格式的转换路径，每种路径有不同的优缺点：</p>
<div class="codehilite"><pre><span></span><code>PyTorch → ONNX → TensorRT：最通用，支持动态图
TensorFlow → TF-TRT → TensorRT：集成度高，保留TF生态
TensorFlow → ONNX → TensorRT：更好的算子支持
Caffe → TensorRT：直接支持，但功能有限
</code></pre></div>

<p>选择转换路径时需要考虑：</p>
<ul>
<li><strong>算子覆盖率</strong>：不是所有算子都被TensorRT原生支持</li>
<li><strong>动态维度需求</strong>：某些路径更好地支持动态输入</li>
<li><strong>精度要求</strong>：不同路径的数值精度可能略有差异</li>
<li><strong>转换复杂度</strong>：直接路径通常更简单但灵活性较低</li>
</ul>
<p><strong>ONNX转换最佳实践</strong>：</p>
<p>ONNX作为中间表示格式，是最灵活的转换路径。转换过程中的关键考虑：</p>
<ol>
<li>
<p><strong>导出配置优化</strong>：
   - 设置正确的opset版本以获得最佳算子支持
   - 使用动态轴处理可变批大小
   - 启用常量折叠减少图复杂度</p>
</li>
<li>
<p><strong>图优化技术</strong>：
   - 移除不必要的类型转换节点
   - 合并连续的transpose操作
   - 简化复杂的reshape序列</p>
</li>
<li>
<p><strong>算子兼容性处理</strong>：
   - 使用onnx-simplifier简化计算图
   - 替换不支持的算子为等效实现
   - 添加自定义算子插件支持</p>
</li>
</ol>
<p><strong>TensorRT引擎构建</strong>：</p>
<p>引擎构建是性能优化的核心阶段，TensorRT在此阶段执行多种优化：</p>
<ol>
<li><strong>层融合（Layer Fusion）</strong>：
   融合相邻的层减少内存访问和内核启动开销。常见的融合模式包括：</li>
</ol>
<ul>
<li>Conv + BN + ReLU → 单个融合层</li>
<li>Conv + Add + ReLU → 残差块融合</li>
<li>MatMul + Add → GEMM with bias</li>
</ul>
<ol start="2">
<li><strong>精度优化</strong>：
   TensorRT支持混合精度推理，自动选择每层的最优精度：</li>
</ol>
<ul>
<li>FP32：最高精度，作为基准</li>
<li>FP16：2倍加速，轻微精度损失</li>
<li>INT8：4倍加速，需要校准</li>
</ul>
<ol start="3">
<li><strong>内核自动调优</strong>：
   对每个层测试多个内核实现，选择最快的：</li>
</ol>
<ul>
<li>不同的tile大小</li>
<li>不同的内存访问模式</li>
<li>特定硬件的优化版本</li>
</ul>
<ol start="4">
<li><strong>内存优化</strong>：
   - 张量内存重用减少总体内存占用
   - 优化内存布局（NCHW vs NHWC）
   - 消除不必要的数据格式转换</li>
</ol>
<p><strong>构建配置优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// 关键构建参数配置</span>
<span class="n">class</span><span class="w"> </span><span class="n">TRTEngineBuilder</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">IBuilder</span><span class="o">*</span><span class="w"> </span><span class="n">builder</span><span class="p">;</span>
<span class="w">    </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">INetworkDefinition</span><span class="o">*</span><span class="w"> </span><span class="n">network</span><span class="p">;</span>
<span class="w">    </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">IBuilderConfig</span><span class="o">*</span><span class="w"> </span><span class="n">config</span><span class="p">;</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">configure_for_jetson</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 1. 工作空间大小：影响可用优化策略</span>
<span class="w">        </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setMaxWorkspaceSize</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="mi">30</span><span class="p">);</span><span class="w">  </span><span class="c1">// 1GB</span>

<span class="w">        </span><span class="c1">// 2. DLA支持：启用DLA加速</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">builder</span><span class="o">-&gt;</span><span class="n">getNbDLACores</span><span class="p">()</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setDefaultDeviceType</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">kDLA</span><span class="p">);</span>
<span class="w">            </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setDLACore</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">            </span><span class="c1">// 设置DLA回退策略</span>
<span class="w">            </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setFlag</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">BuilderFlag</span><span class="o">::</span><span class="n">kGPU_FALLBACK</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 3. 精度模式：根据需求选择</span>
<span class="w">        </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setFlag</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">BuilderFlag</span><span class="o">::</span><span class="n">kFP16</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">int8_calibration_available</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setFlag</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">BuilderFlag</span><span class="o">::</span><span class="n">kINT8</span><span class="p">);</span>
<span class="w">            </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setInt8Calibrator</span><span class="p">(</span><span class="n">calibrator</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 4. 优化配置文件：处理动态输入</span>
<span class="w">        </span><span class="k">auto</span><span class="w"> </span><span class="n">profile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">builder</span><span class="o">-&gt;</span><span class="n">createOptimizationProfile</span><span class="p">();</span>
<span class="w">        </span><span class="n">profile</span><span class="o">-&gt;</span><span class="n">setDimensions</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">OptProfileSelector</span><span class="o">::</span><span class="n">kMIN</span><span class="p">,</span><span class="w"> </span><span class="n">min_dims</span><span class="p">);</span>
<span class="w">        </span><span class="n">profile</span><span class="o">-&gt;</span><span class="n">setDimensions</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">OptProfileSelector</span><span class="o">::</span><span class="n">kOPT</span><span class="p">,</span><span class="w"> </span><span class="n">opt_dims</span><span class="p">);</span>
<span class="w">        </span><span class="n">profile</span><span class="o">-&gt;</span><span class="n">setDimensions</span><span class="p">(</span><span class="s">&quot;input&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">OptProfileSelector</span><span class="o">::</span><span class="n">kMAX</span><span class="p">,</span><span class="w"> </span><span class="n">max_dims</span><span class="p">);</span>
<span class="w">        </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">addOptimizationProfile</span><span class="p">(</span><span class="n">profile</span><span class="p">);</span>

<span class="w">        </span><span class="c1">// 5. 策略选择：平衡构建时间和运行性能</span>
<span class="w">        </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setProfilingVerbosity</span><span class="p">(</span>
<span class="w">            </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">ProfilingVerbosity</span><span class="o">::</span><span class="n">kDETAILED</span><span class="p">);</span>
<span class="w">        </span><span class="n">config</span><span class="o">-&gt;</span><span class="n">setTacticSources</span><span class="p">(</span>
<span class="w">            </span><span class="mi">1U</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">TacticSource</span><span class="o">::</span><span class="n">kCUBLAS</span><span class="p">)</span><span class="w"> </span><span class="o">|</span>
<span class="w">            </span><span class="mi">1U</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">TacticSource</span><span class="o">::</span><span class="n">kCUDNN</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<h3 id="2142-int8">21.4.2 INT8量化校准</h3>
<p>INT8量化是在Jetson上实现高性能推理的关键技术，可以提供4倍的理论加速比，同时将模型大小减少到原来的1/4。然而，从FP32到INT8的转换需要仔细的校准以保持模型精度。</p>
<p><strong>量化原理与挑战</strong>：</p>
<p>INT8量化将浮点数映射到8位整数范围[-128, 127]，这个过程涉及：</p>
<ol>
<li><strong>动态范围确定</strong>：找到每个张量的最优量化范围</li>
<li><strong>量化粒度选择</strong>：逐层、逐通道或逐张量量化</li>
<li><strong>异常值处理</strong>：处理超出正常范围的激活值</li>
<li><strong>精度保持</strong>：确保量化后的模型精度损失在可接受范围</li>
</ol>
<p><strong>校准数据集准备</strong>：</p>
<p>校准数据集的质量直接影响量化模型的精度：</p>
<ol>
<li><strong>代表性</strong>：数据应覆盖实际部署场景的分布</li>
<li><strong>多样性</strong>：包含各种边缘情况和困难样本</li>
<li><strong>规模</strong>：通常500-1000个样本足够</li>
<li><strong>预处理一致性</strong>：使用与推理时完全相同的预处理</li>
</ol>
<p><strong>熵校准与百分位校准</strong>：</p>
<p>TensorRT提供两种主要的校准算法：</p>
<ol>
<li>
<p><strong>熵校准（Entropy Calibration）</strong>：
   - 最小化量化前后的KL散度
   - 适合大多数CNN模型
   - 倾向于保留分布的主要特征</p>
</li>
<li>
<p><strong>百分位校准（Percentile Calibration）</strong>：
   - 基于激活值的百分位数确定范围
   - 对异常值更鲁棒
   - 适合存在长尾分布的模型</p>
</li>
</ol>
<p><strong>自定义校准器实现</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">class</span><span class="w"> </span><span class="n">Int8EntropyCalibrator</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">public</span><span class="w"> </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">IInt8EntropyCalibrator2</span><span class="w"> </span><span class="p">{</span>
<span class="n">private</span><span class="o">:</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">calibration_files</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">current_batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">device_input</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="w"> </span><span class="n">calibration_cache</span><span class="p">;</span>

<span class="n">public</span><span class="o">:</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">getBatch</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">bindings</span><span class="p">[],</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">names</span><span class="p">[],</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">nbBindings</span><span class="p">)</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">current_batch</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">calibration_files</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 加载校准数据批次</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">batch_data</span><span class="p">;</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">current_batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">calibration_files</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">load_and_preprocess</span><span class="p">(</span><span class="n">calibration_files</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="w"> </span><span class="n">batch_data</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 传输到GPU</span>
<span class="w">        </span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">device_input</span><span class="p">,</span><span class="w"> </span><span class="n">batch_data</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span>
<span class="w">                  </span><span class="n">batch_data</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span>
<span class="w">                  </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
<span class="w">        </span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_input</span><span class="p">;</span>
<span class="w">        </span><span class="n">current_batch</span><span class="o">++</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">readCalibrationCache</span><span class="p">(</span><span class="kt">size_t</span><span class="o">&amp;</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 读取缓存的校准表，避免重复校准</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">calibration_cache</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="w"> </span><span class="n">cache_file</span><span class="p">(</span><span class="s">&quot;calibration.cache&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cache_file</span><span class="p">.</span><span class="n">good</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">cache_file</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">end</span><span class="p">);</span>
<span class="w">                </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cache_file</span><span class="p">.</span><span class="n">tellg</span><span class="p">();</span>
<span class="w">                </span><span class="n">cache_file</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">beg</span><span class="p">);</span>
<span class="w">                </span><span class="n">calibration_cache</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">length</span><span class="p">);</span>
<span class="w">                </span><span class="n">cache_file</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">calibration_cache</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">calibration_cache</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">calibration_cache</span><span class="p">.</span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="n">writeCalibrationCache</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">cache</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 保存校准表供后续使用</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">ofstream</span><span class="w"> </span><span class="n">cache_file</span><span class="p">(</span><span class="s">&quot;calibration.cache&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
<span class="w">        </span><span class="n">cache_file</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">cache</span><span class="p">),</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="w">        </span><span class="n">calibration_cache</span><span class="p">.</span><span class="n">assign</span><span class="p">((</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">cache</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">cache</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>量化感知训练（QAT）</strong>：</p>
<p>对于精度要求极高的应用，量化感知训练可以获得更好的结果：</p>
<ol>
<li><strong>训练时模拟量化</strong>：在前向传播中插入量化/反量化操作</li>
<li><strong>学习量化参数</strong>：将scale和zero point作为可学习参数</li>
<li><strong>渐进式量化</strong>：从高精度逐步过渡到低精度</li>
<li><strong>混合精度策略</strong>：对敏感层保持高精度</li>
</ol>
<h3 id="2143">21.4.3 动态批处理</h3>
<p>动态批处理是提高GPU利用率和系统吞吐量的关键技术，特别是在处理来自多个源的异步请求时。</p>
<p><strong>动态形状支持</strong>：</p>
<p>TensorRT 7.0+引入了对动态形状的全面支持，这对于实际部署至关重要：</p>
<ol>
<li>
<p><strong>优化配置文件（Optimization Profiles）</strong>：
   - 为不同的输入形状范围创建多个配置
   - 每个配置指定最小、最优和最大维度
   - 运行时根据实际输入选择最佳配置</p>
</li>
<li>
<p><strong>显式批处理维度</strong>：
   - 批处理维度成为网络输入的一部分
   - 支持不同层有不同的批大小
   - 实现真正的动态批处理</p>
</li>
<li>
<p><strong>形状张量操作</strong>：
   - 支持依赖于输入形状的操作
   - 动态reshape、slice等操作
   - 条件执行和循环结构</p>
</li>
</ol>
<p><strong>批处理策略优化</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">class</span><span class="w"> </span><span class="n">DynamicBatchManager</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">Request</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">;</span>
<span class="w">        </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">;</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">input_size</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">promise</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">&gt;</span><span class="w"> </span><span class="n">promise</span><span class="p">;</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">time_point</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">steady_clock</span><span class="o">&gt;</span><span class="w"> </span><span class="n">arrival_time</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">queue</span><span class="o">&lt;</span><span class="n">Request</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pending_requests</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="w"> </span><span class="n">queue_mutex</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">max_batch_size</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">max_latency_ms</span><span class="p">;</span>

<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">batch_formation_strategy</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">running</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">unique_lock</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lock</span><span class="p">(</span><span class="n">queue_mutex</span><span class="p">);</span>

<span class="w">            </span><span class="c1">// 等待请求或超时</span>
<span class="w">            </span><span class="n">cv</span><span class="p">.</span><span class="n">wait_for</span><span class="p">(</span><span class="n">lock</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">milliseconds</span><span class="p">(</span><span class="n">max_latency_ms</span><span class="p">),</span>
<span class="w">                </span><span class="p">[</span><span class="n">this</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="o">!</span><span class="n">pending_requests</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">running</span><span class="p">;</span><span class="w"> </span><span class="p">});</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">running</span><span class="p">)</span><span class="w"> </span><span class="k">break</span><span class="p">;</span>

<span class="w">            </span><span class="c1">// 形成批次</span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Request</span><span class="o">&gt;</span><span class="w"> </span><span class="n">batch</span><span class="p">;</span>
<span class="w">            </span><span class="k">auto</span><span class="w"> </span><span class="n">now</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">steady_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

<span class="w">            </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">pending_requests</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>
<span class="w">                   </span><span class="n">batch</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">max_batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">req</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pending_requests</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>

<span class="w">                </span><span class="c1">// 延迟约束检查</span>
<span class="w">                </span><span class="k">auto</span><span class="w"> </span><span class="n">latency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">milliseconds</span><span class="o">&gt;</span>
<span class="w">                              </span><span class="p">(</span><span class="n">now</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">req</span><span class="p">.</span><span class="n">arrival_time</span><span class="p">).</span><span class="n">count</span><span class="p">();</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">latency</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">max_latency_ms</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">0.8</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="n">batch</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">req</span><span class="p">));</span>
<span class="w">                    </span><span class="n">pending_requests</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
<span class="w">                </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">max_batch_size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="c1">// 等待更多请求以提高效率</span>
<span class="w">                    </span><span class="n">batch</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">req</span><span class="p">));</span>
<span class="w">                    </span><span class="n">pending_requests</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
<span class="w">                </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="k">break</span><span class="p">;</span><span class="w">  </span><span class="c1">// 保留请求给下一批</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">}</span>

<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">batch</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">process_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">);</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>内存管理优化</strong>：</p>
<p>动态批处理需要高效的内存管理策略：</p>
<ol>
<li><strong>内存池预分配</strong>：为不同批大小预分配缓冲区</li>
<li><strong>零拷贝批处理</strong>：使用统一内存避免数据复制</li>
<li><strong>环形缓冲区</strong>：实现高效的生产者-消费者模式</li>
<li><strong>CUDA Graph优化</strong>：对固定模式使用Graph加速</li>
</ol>
<h3 id="2144">21.4.4 插件开发与优化</h3>
<p>TensorRT插件机制允许添加自定义算子，这对于支持新架构或优化特定操作至关重要。</p>
<p><strong>插件开发流程</strong>：</p>
<p>开发高性能TensorRT插件需要理解其生命周期和接口要求：</p>
<ol>
<li>
<p><strong>插件接口实现</strong>：
   - IPluginV2DynamicExt：支持动态形状
   - 实现推理、序列化、资源管理接口
   - 正确处理数据格式和精度</p>
</li>
<li>
<p><strong>性能优化要点</strong>：
   - 选择最优的CUDA配置
   - 实现多精度支持（FP32/FP16/INT8）
   - 利用共享内存和寄存器优化
   - 考虑tensor core加速</p>
</li>
<li>
<p><strong>兼容性考虑</strong>：
   - 支持不同的数据布局（NCHW/NHWC）
   - 处理广播和stride
   - 实现高效的形状推导</p>
</li>
</ol>
<p><strong>自定义算子示例</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="n">class</span><span class="w"> </span><span class="n">CustomPoolingPlugin</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">public</span><span class="w"> </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">IPluginV2DynamicExt</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 高效的自定义池化实现</span>
<span class="w">    </span><span class="n">template</span><span class="o">&lt;</span><span class="n">typename</span><span class="w"> </span><span class="n">T</span><span class="o">&gt;</span>
<span class="w">    </span><span class="kr">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">custom_pooling_kernel</span><span class="p">(</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="o">*</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="o">*</span><span class="w"> </span><span class="n">output</span><span class="p">,</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">channels</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">height</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">width</span><span class="p">,</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">pool_size</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">blockDim</span><span class="p">.</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nb">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">total_elements</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">channels</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="p">;</span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">total_elements</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 计算位置</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">channels</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="p">);</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="p">))</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">channels</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">idx</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">width</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">height</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">width</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 自定义池化逻辑：阈值加权池化</span>
<span class="w">        </span><span class="n">T</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ph</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ph</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">pool_size</span><span class="p">;</span><span class="w"> </span><span class="n">ph</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">pw</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">pw</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">pool_size</span><span class="p">;</span><span class="w"> </span><span class="n">pw</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">h_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pool_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ph</span><span class="p">;</span>
<span class="w">                </span><span class="kt">int</span><span class="w"> </span><span class="n">w_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pool_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">pw</span><span class="p">;</span>

<span class="w">                </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">h_idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">w_idx</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">width</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="n">T</span><span class="w"> </span><span class="n">val</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">[</span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">channels</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">+</span>
<span class="w">                                 </span><span class="n">c</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">+</span>
<span class="w">                                 </span><span class="n">h_idx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">w_idx</span><span class="p">];</span>

<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">val</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="n">sum</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">val</span><span class="p">;</span>
<span class="w">                        </span><span class="n">count</span><span class="o">++</span><span class="p">;</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="n">output</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">count</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">count</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">enqueue</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">PluginTensorDesc</span><span class="o">*</span><span class="w"> </span><span class="n">inputDesc</span><span class="p">,</span>
<span class="w">                </span><span class="k">const</span><span class="w"> </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">PluginTensorDesc</span><span class="o">*</span><span class="w"> </span><span class="n">outputDesc</span><span class="p">,</span>
<span class="w">                </span><span class="k">const</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="k">const</span><span class="o">*</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="k">const</span><span class="o">*</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">workspace</span><span class="p">,</span><span class="w"> </span><span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="p">)</span><span class="w"> </span><span class="n">override</span><span class="w"> </span><span class="p">{</span>

<span class="w">        </span><span class="c1">// 获取维度信息</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">channels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>

<span class="w">        </span><span class="c1">// 选择合适的块大小</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">elements</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">channels</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="p">;</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">blocks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">elements</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads</span><span class="p">;</span>

<span class="w">        </span><span class="c1">// 根据数据类型分发</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kFLOAT</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">custom_pooling_kernel</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">                </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="w">                </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">channels</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="p">,</span>
<span class="w">                </span><span class="n">pool_size_</span><span class="p">,</span><span class="w"> </span><span class="n">threshold_</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kHALF</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">custom_pooling_kernel</span><span class="o">&lt;</span><span class="n">__half</span><span class="o">&gt;&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
<span class="w">                </span><span class="p">(</span><span class="n">__half</span><span class="o">*</span><span class="p">)</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="p">(</span><span class="n">__half</span><span class="o">*</span><span class="p">)</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="w">                </span><span class="n">batch</span><span class="p">,</span><span class="w"> </span><span class="n">channels</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="p">,</span>
<span class="w">                </span><span class="n">pool_size_</span><span class="p">,</span><span class="w"> </span><span class="n">threshold_</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</code></pre></div>

<p><strong>插件优化技巧</strong>：</p>
<ol>
<li>
<p><strong>内存访问优化</strong>：
   - 使用向量化加载提高带宽利用率
   - 实现coalesced访问模式
   - 利用纹理内存或常量内存</p>
</li>
<li>
<p><strong>计算优化</strong>：
   - 使用tensor core进行矩阵运算
   - 实现warp级别的协作
   - 避免分支分歧</p>
</li>
<li>
<p><strong>多版本实现</strong>：
   - 为不同输入大小提供特化版本
   - 根据硬件能力选择实现
   - 支持不同精度的优化路径</p>
</li>
</ol>
<h2 id="215-ai">21.5 案例：边缘AI部署</h2>
<h3 id="2151">21.5.1 系统架构设计</h3>
<h3 id="2152">21.5.2 多模型协同推理</h3>
<h3 id="2153">21.5.3 实时性能优化</h3>
<h3 id="2154">21.5.4 资源调度策略</h3>
<h2 id="_1">本章小结</h2>
<h2 id="_2">练习题</h2>
<h2 id="_3">常见陷阱与错误</h2>
<h2 id="_4">最佳实践检查清单</h2>
            </article>
            
            <nav class="page-nav"><a href="chapter20.html" class="nav-link prev">← 第20章：CUDA Graph与内核融合</a><a href="chapter22.html" class="nav-link next">第22章：稀疏计算与动态稀疏 →</a></nav>
        </main>
    </div>
</body>
</html>